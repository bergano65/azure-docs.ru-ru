---
title: Что такое ответственное машинное обучение (Предварительная версия)
titleSuffix: Azure Machine Learning
description: Узнайте, что такое машинное обучение и как его использовать в Машинное обучение Azure
services: machine-learning
ms.service: machine-learning
ms.subservice: core
ms.topic: conceptual
ms.author: luquinta
author: luisquintanilla
ms.date: 08/05/2020
ms.openlocfilehash: 689b90fc1f45faad72640f47e5eebe936d2dc8b7
ms.sourcegitcommit: 2ff0d073607bc746ffc638a84bb026d1705e543e
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 08/06/2020
ms.locfileid: "87829396"
---
# <a name="what-is-responsible-machine-learning-preview"></a>Что такое машинное обучение? (предварительная версия)

В этой статье вы узнаете, что такое машинное обучение (ML) и каким образом можно разместить его на практике с Машинное обучение Azure.

## <a name="responsible-machine-learning-principles"></a>Ответственные принципы машинного обучения

На всех этапах разработки и использования систем искусственного интеллекта обязательным условием является доверие. Доверие к платформе, ко всем процессам и моделям. В корпорации Майкрософт ответственное машинное обучение включает следующие значения и принципы.

- понимание моделей машинного обучения;
  - анализ и объяснение поведения модели;
  - оценка и устранение необъективности модели;
- защита пользователей и их данных;
  - предотвращение раскрытия данных с применением дифференциальной конфиденциальности;
  - Работа с зашифрованными данными с помощью шифрования Гомоморфные
- полный контроль процесса машинного обучения;
  - документирование жизненного цикла машинного обучения в таблицах данных.

:::image type="content" source="media/concept-responsible-ml/responsible-ml-pillars.png" alt-text="Ответственные основные принципы машинного обучения — интерпретируемость, разностная конфиденциальность, шифрование Гомоморфные, журнал аудита — Машинное обучение Azure":::

Так как искусственный интеллект и автономные системы все глубже проникают в структуру общества, важно своевременно принять меры по прогнозированию и предотвращению нежелательных последствий применения этих технологий.

## <a name="interpret-and-explain-model-behavior"></a>Анализ и объяснение поведения модели

Сложные для объяснения или непрозрачные системы могут быть проблематичными, поскольку это затрудняет для таких заинтересованных лиц, как разработчиков систем, стабилизаторов, пользователей и руководителей, ответственных за принятие бизнес-решений. Некоторые системы искусственного интеллекта более понятны, чем другие, а иногда приходится выбирать между системой с более высокой точностью или большей понятностью.

Чтобы создавать хорошо объяснимые системы искусственного интеллекта, применяйте пакет с открытым кодом [InterpretML](https://github.com/interpretml/interpret), созданный корпорацией Майкрософт. [InterpretML можно использовать на платформе "Машинное обучение Azure"](how-to-machine-learning-interpretability.md) для [интерпретации и объяснения моделей машинного обучения](how-to-machine-learning-interpretability-aml.md), в том числе [моделей автоматизированного машинного обучения](how-to-machine-learning-interpretability-automl.md).

## <a name="mitigate-fairness-in-machine-learning-models"></a>Снижение равноправия в моделях машинного обучения

По мере того, как системы искусственного интеллекта все больше участвуют в принятии повседневных решений, становится крайне важным непредвзятое поведение таких систем со справедливым распределением результатов.

Необъективность в системах искусственного интеллекта может привести к целому ряду нежелательных последствий:

- ограничение доступа некоторых лиц к возможностям, ресурсам или сведениям;
- усиление смещения и стереотипов.

Многие аспекты объективности невозможно оценить или выразить в метриках. Существуют средства и методики, которые повышают объективность систем искусственного интеллекта при их проектировании и разработке.

Два основных этапа для снижения необъективности в системах ИИ — это оценка и устранение рисков. Мы рекомендуем использовать пакет [FairLearn](https://github.com/fairlearn/fairlearn) с открытым кодом, который умеет оценивать и устранять потенциальную необъективность в системах искусственного интеллекта. Дополнительные сведения о объективности в машинном обучении и пакете FairLearn см. в [этой статье](./concept-fairness-ml.md).

## <a name="prevent-data-exposure-with-differential-privacy"></a>Предотвращение раскрытия данных с применением дифференциальной конфиденциальности

Если для анализа используются некоторые данные, очень важно сохранять их конфиденциальность и приватность на всем протяжении использования. Дифференциальная конфиденциальность — это набор систем и рекомендаций, которые помогают обеспечить безопасность и конфиденциальность данных частных лиц.

В традиционных сценариях необработанные данные хранятся в файлах и базах данных. При анализе данных пользователи обычно используют необработанные данные. Это является проблемой из-за возможного нарушения конфиденциальности личности. Благодаря дифференциальной конфиденциальности можно решить эту проблему, добавив в данные "шум" или случайность, чтобы пользователи не могли определить отдельные точки данных.

Реализация систем с дифференциальной конфиденциальностью — это сложный процесс. [WhiteNoise](https://github.com/opendifferentialprivacy/whitenoise-core) — это проект с открытым кодом, который содержит различные компоненты для создания глобальных систем с дифференциальной конфиденциальностью. Дополнительные сведения о дифференциальной конфиденциальности и проекте WhiteNoise см. в статье [о сохранении конфиденциальности данных благодаря дифференциальной конфиденциальности и WhiteNoise](./concept-differential-privacy.md).

> [!NOTE]
> Обратите внимание, что мы переименованы набор средств, и в ближайшие недели будет введено новое имя. 

## <a name="work-on-encrypted-data-with-homomorphic-encryption"></a>Работа с зашифрованными данными с помощью шифрования Гомоморфные

В традиционных облачных решениях для хранения и вычислений облако должно иметь незашифрованный доступ к данным клиента для их вычисления. Этот доступ предоставляет данные для операторов облака. Конфиденциальность данных зависит от политик управления доступом, реализованных в облаке и доверенных для клиента.

Гомоморфное шифрование позволяет выполнять вычисления с зашифрованными данными без необходимости доступа к секретному ключу (ключу расшифровки). Результаты вычислений шифруются и могут быть прочитаны только владельцем секретного ключа. Используя шифрование Гомоморфные, операторы облака никогда не будут иметь незашифрованный доступ к данным, на которых они хранятся и на которых осуществляется расчет. Вычисления выполняются непосредственно в зашифрованных данных. Конфиденциальность данных зависит от характера криптографических служб, а владелец данных контролирует все информационные выпуски. Дополнительные сведения о шифровании Гомоморфные в корпорации Майкрософт см. в разделе [Microsoft Research](https://www.microsoft.com/research/project/homomorphic-encryption/).

Чтобы приступить к работе с шифрованием Гомоморфные в Машинное обучение Azure, используйте привязки Python [с шифрованием](https://pypi.org/project/encrypted-inference/) для [Microsoft запечатывания](https://github.com/microsoft/SEAL). Microsoft ЗАПЕЧАТЫВАНИе — это библиотека шифрования Гомоморфные с открытым исходным кодом, которая позволяет выполнять дополнения и умножение в зашифрованных целых числах или реальных числах. Дополнительные сведения о Microsoft SEAL см. на странице [центр архитектуры Azure](https://docs.microsoft.com/azure/architecture/solution-ideas/articles/homomorphic-encryption-seal) или [проекте Microsoft Research](https://www.microsoft.com/research/project/microsoft-seal/).

См. Следующий пример, чтобы узнать, [как развернуть зашифрованную веб-службу в машинное обучение Azure](how-to-homomorphic-encryption-seal.md).

## <a name="document-the-machine-learning-lifecycle-with-datasheets"></a>Документирование жизненного цикла машинного обучения в таблицах данных

Документирование важной информации о процессе машинного обучения является важнейшим условием для принятия взвешенных решений на каждом этапе. Таблицы данных предоставляют способ для документирования ресурсов машинного обучения, которые используются и создаются в рамках жизненного цикла машинного обучения.

Модели, как правило, рассматриваются как "непрозрачные рамки", и часто бывает немало информации о них. Но системы машинного обучения становятся все более распространенными и применяются для принятия многих решений, поэтому использование таблиц данных станет важным шагом по разработке более ответственных систем машинного обучения.

Вот некоторые сведения о модели, которые вы можете документировать в таблицах данных:

- предполагаемое использование;
- архитектура модели;
- использованные для обучения данные;
- использованные для оценки данные;
- метрики производительности модели обучения;
- сведения об объективности.

Следующий пример демонстрирует, как можно применить пакет средств разработки Машинного обучения Azure для реализации [таблиц данных для моделей](https://github.com/microsoft/MLOps/blob/master/pytorch_with_datasheet/model_with_datasheet.ipynb).

## <a name="additional-resources"></a>Дополнительные ресурсы

- Дополнительные сведения см. в разделе [ответственное по инновациям](https://docs.microsoft.com/azure/architecture/guide/responsible-innovation/) , чтобы узнать о рекомендациях.
- Изучите набор рекомендаций [ABOUT ML](https://www.partnershiponai.org/about-ml/) по документированию систем машинного обучения.
