---
title: 'Преобразовать слово в вектор: ссылка на модуль'
titleSuffix: Azure Machine Learning
description: Узнайте, как использовать три предоставленных модели преобразования слов в векторы для извлечения словаря и его соответствующих внедрений слов из корпуса текста.
services: machine-learning
ms.service: machine-learning
ms.subservice: core
ms.topic: reference
author: likebupt
ms.author: keli19
ms.date: 05/19/2020
ms.openlocfilehash: 5fad3e4862b0c40c9edd00a5b9d47b245e529396
ms.sourcegitcommit: f5580dd1d1799de15646e195f0120b9f9255617b
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 09/29/2020
ms.locfileid: "91536738"
---
# <a name="convert-word-to-vector-module"></a>Преобразование слова в векторный модуль

В этой статье описывается, как использовать модуль Convert Word to Vector в Машинное обучение Azure Designer для следующих задач:

- Примените различные модели Word2Vec (Word2Vec, Фасттекст, предварительно обученную модель специализированный) к совокупностиму тексту, указанному в качестве входных данных.
- Создание словаря с внедрением слов.

В этом модуле используется библиотека Gensim. Дополнительные сведения о Женсим см. на [официальном веб-сайте](https://radimrehurek.com/gensim/apiref.html), который содержит учебники и описание алгоритмов.

### <a name="more-about-converting-words-to-vectors"></a>Дополнительные сведения о преобразовании слов в векторы

Преобразование слов в векторы или векторы слов — это процесс обработки естественного языка (NLP). В процессе используются языковые модели для преобразования слов в пространство вектора. Векторное пространство представляет каждое слово с помощью вектора вещественных чисел. Это также позволяет словам с аналогичными значениями иметь похожие представления.

Используйте внедрение слов в качестве начального ввода для NLP нисходящих задач, таких как классификация текста и анализ тональности.

В разных технологиях внедрения слов в этом модуле реализовано три широко используемых метода. Две, Word2Vec и Фасттекст — это модели оперативного обучения. Вторая — это предварительно обученная модель специализированный-wiki-гигаворд-100. 

Интерактивные обучающие модели обучены на основе входных данных. Предварительно обученные модели обрабатываются автономно на более крупном текстовом совокупности (например, в Википедии, Google News), который обычно содержит около 100 000 000 000 слов. Встраивание Word остается постоянным во время векторной вставки слов. Предварительно обученные модели Word предоставляют такие преимущества, как сокращение времени обучения, улучшенные векторы слов, а также повышение общей производительности.

Ниже приведены некоторые сведения о методах.

+ Word2Vec — одна из самых популярных методик для изучения внедрения слов с помощью неполной нейронной сети. Теория обсуждается в этом документе, который можно скачать в виде PDF-файла: [эффективная оценка представлений Word в пространстве векторов](https://arxiv.org/pdf/1301.3781.pdf). Реализация в этом модуле основана на [библиотеке женсим для Word2Vec](https://radimrehurek.com/gensim/models/word2vec.html).

+ Теория Фасттекст объясняется в этом документе, который можно скачать в формате PDF: расширяя [векторы слов с информацией о подсловах](https://arxiv.org/pdf/1607.04606.pdf). Реализация в этом модуле основана на [библиотеке женсим для фасттекст](https://radimrehurek.com/gensim/models/fasttext.html).

+ Предварительно обученная модель специализированный — специализированный-wiki-гигаворд-100. Это коллекция предварительно обученных векторов, основанных на тексте Википедии совокупности, который содержит маркеры 5 600 000 000 и 400 000 нерегистровых слов. Доступна загрузка PDF-файла: [специализированный: глобальные векторы для представления Word](https://nlp.stanford.edu/pubs/glove.pdf).

## <a name="how-to-configure-convert-word-to-vector"></a>Настройка преобразования слов в векторы

Для этого модуля требуется набор данных, содержащий столбец текста. Предварительно обработанный текст лучше.

1. Добавьте модуль **Преобразование слов в векторы** в конвейер.

2. В качестве входных данных для модуля укажите набор данных, содержащий один или несколько текстовых столбцов.

3. В поле **целевой столбец**выберите только один столбец, содержащий текст для обработки.

    Так как этот модуль создает словарь из текста, содержимое столбцов отличается, что ведет к другому содержимому словаря. Поэтому модуль принимает только один целевой столбец.

4. Для  **стратегии Word2Vec**выберите из специализированный предварительно **обученную модель на английском языке**, **женсим Word2Vec**и **женсим фасттекст**.

5. Если **стратегия Word2Vec** — **Женсим Word2Vec** или **женсим фасттекст**:

    + Для **алгоритма обучения Word2Vec**выберите один из **Skip_gram** и **кбов**. Разница введена в [исходном документе (PDF)](https://arxiv.org/pdf/1301.3781.pdf).

        Метод по умолчанию — **Skip_gram**.

    + Для параметра **Длина внедрения слова**укажите размерность векторов слов. Этот параметр соответствует `size` параметру в женсим.

        Размер внедрения по умолчанию — 100.

    + В поле **Размер контекстного окна**укажите максимальное расстояние между прогнозируемым словом и текущим словом. Этот параметр соответствует `window` параметру в женсим.

        Размер окна по умолчанию — 5.

    + Для параметра **число эпох**укажите количество эпох (итераций) в совокупности. Соответствует `iter` параметру в женсим.

        Номер эпохи по умолчанию — 5.

6. Для параметра **максимальный размер словаря**укажите максимальное число слов в созданном словаре.

    Если количество уникальных слов превышает максимальный, обрезать редко используемые слова.

    Размер словаря по умолчанию — 10 000.

7. Для **минимального числа слов**укажите минимальное число слов. Модуль будет игнорировать все слова с частотой ниже этого значения.

    Значение по умолчанию — 5.

8. Отправьте конвейер.

## <a name="examples"></a>Примеры

Выходные данные модуля:

+ **Словарь с внедрением**: содержит созданный словарь, а также внедрение каждого слова. Одно измерение занимает один столбец.

В следующем примере показано, как работает модуль преобразование слова в вектор Vector. Он использует преобразование слова в вектор с параметрами по умолчанию в предварительно обработанный набор данных Википедии SP 500.

### <a name="source-dataset"></a>Исходный набор данных

Набор данных содержит столбец Category, а также полный текст, полученный из Википедии. В следующей таблице приведено несколько репрезентативных примеров.

|Текст|
|----------|
|nasdaq 100 component s p 500 component foundation founder location city apple campus 1 infinite loop street infinite loop cupertino california cupertino california location country united states...|
|br nasdaq 100 nasdaq 100 component br s p 500 s p 500 component industry computer software foundation br founder charles geschke br john warnock location adobe systems...|
|s p 500 s p 500 component industry automotive industry automotive predecessor general motors corporation 1908 2009 successor...|
|s p 500 s p 500 component industry conglomerate company conglomerate foundation founder location city fairfield connecticut fairfield connecticut location country usa area...|
|br s p 500 s p 500 component foundation 1903 founder william s harley br arthur davidson harley davidson founder arthur davidson br walter davidson br william a davidson location...|

### <a name="output-vocabulary-with-embeddings"></a>Выходной словарь с внедрениями

В приведенной ниже таблице содержатся выходные данные этого модуля, в качестве входных данных — пакет обновления Википедии SP 500. В крайнем левом столбце указывается словарь. Его вектор внедрения представляется значениями оставшихся столбцов в той же строке.

|Словарь|Размерность внедрения 0|Размерность внедрения 1|Размерность внедрения 2|Размерность внедрения 3|Размерность внедрения 4|Размерность внедрения 5|...|Размерность внедрения 99|
|-------------|-------------|-------------|-------------|-------------|-------------|-------------|-------------|-------------|
|nasdaq|–0,375865|0,609234|0,812797|–0,002236|0,319071|0,591986|...|0,364276
|Компонент|0,081302|0,40001|0,121803|0,108181|0,043651|–0,091452|...|0,636587
|s|–0,34355|–0,037092|–0,012167|0,151542|0,601019|0,084501|...|0,149419
|p|–0,133407|0,073244|0,170396|0,326706|0,213463|–0,700355|...|0,530901
foundation|–0,166819|0,10883|–0,07933|–0,073753|0,262137|0,045725|...|0,27487
founder|–0,297408|0,493067|0,316709|–0,031651|0,455416|–0,284208|...|0,22798
location|–0,375213|0,461229|0,310698|0,213465|0,200092|0,314288|...|0,14228
city|–0,460828|0,505516|–0,074294|–0,00639|0,116545|0,494368|...|–0,2403
apple|0,05779|0,672657|0,597267|–0,898889|0,099901|0,11833|...|0,4636
campus|–0,281835|0,29312|0,106966|–0,031385|0,100777|–0,061452|...|0,05978
infinite|–0,263074|0,245753|0,07058|–0,164666|0,162857|–0,027345|...|–0,0525
loop|–0,391421|0,52366|0,141503|–0,105423|0,084503|–0,018424|...|–0,0521

В этом примере мы использовали **Женсим Word2Vec** по умолчанию для **стратегии Word2Vec**, а **алгоритм обучения** — **пропускать-граммы**. **Длина внедрения слов** составляет 100, поэтому у нас есть 100 столбцов.

## <a name="technical-notes"></a>Технические примечания

В этом разделе содержатся советы и ответы на часто задаваемые вопросы.

+ Разница между интерактивным обучением и предварительно обученной моделью:

    В этом модуле преобразования слова в векторный модуль мы предоставили три различные стратегии: две модели оперативного обучения и одну предварительно обученную модель. Модели интерактивного обучения используют входной набор данных в качестве обучающих данных, а также создают словари и векторы слов во время обучения. Предварительно заданная модель уже обучена гораздо большим количеством текстовых совокупности, например Википедии или Twitter Text. Предварительно обученная модель на самом деле представляет собой набор пар слов и внедрения.  

    Предварительно обученная модель специализированный суммирует словарь из входного набора данных и создает вектор внедрения для каждого слова из предварительно обученной модели. Без интерактивного обучения использование предварительно обученной модели может сэкономить время обучения. Он обеспечивает лучшую производительность, особенно если размер входного набора данных относительно мал.

+ Размер внедрения:

    Как правило, длина внедрения слова равна более сотни. Например, 100, 200, 300. Небольшой размер внедрения означает небольшое векторное пространство, которое может привести к конфликтам внедрения в Word.  

    Длина внедрения слов исправлена для предварительно обученных моделей. В этом примере размер внедрения специализированный-wiki-гигаворд-100 равен 100.


## <a name="next-steps"></a>Дальнейшие действия

Ознакомьтесь с [набором доступных модулей](module-reference.md) в службе Машинного обучения Azure. 

Список ошибок, относящихся к модулям конструктора, см. в разделе [машинное обучение коды ошибок](designer-error-codes.md).
