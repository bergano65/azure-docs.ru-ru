---
title: Высокая производительность, перекрестное определение платформы с помощью ONNX
titleSuffix: Azure Machine Learning
description: Дополнительные сведения о ONNX и среде выполнения ONNX для ускорения моделей
services: machine-learning
ms.service: machine-learning
ms.subservice: core
ms.topic: conceptual
ms.reviewer: jmartens
ms.author: prasantp
author: prasanthpul
ms.date: 08/15/2019
ms.custom: seodec18
ms.openlocfilehash: 4f6e9e6b44e4a8fcc52f6d8ae19af60d64972b3a
ms.sourcegitcommit: 0fab4c4f2940e4c7b2ac5a93fcc52d2d5f7ff367
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 09/17/2019
ms.locfileid: "71035406"
---
# <a name="onnx-and-azure-machine-learning-create-and-accelerate-ml-models"></a>ONNX и Машинное обучение Azure: Создание и ускорение моделей машинного обучения

Узнайте, как использовать [открытый обмен нейронными сетями](https://onnx.ai) (ONNX) для оптимизации вывода модели машинного обучения. Вывод или оценка модели — это этап, в котором развернутая модель используется для прогнозирования, чаще всего в рабочих данных. 

Оптимизация моделей машинного обучения для определения (или оценки модели) усложняется, поскольку необходимо настроить модель и библиотеку вывода, чтобы максимально эффективно использовать возможности оборудования. Проблема станет чрезвычайно трудной, если требуется обеспечить оптимальную производительность различных типов платформ (Cloud/ребр, ЦП, GPU и т. д.), так как каждый из них имеет различные возможности и характеристики. Сложность возрастает при наличии моделей из различных платформ, которые должны выполняться на различных платформах. Оптимизация всех различных сочетаний платформ и оборудования занимает очень много времени. Решение для обучения в вашей предпочитаемой платформе и выполнения в любом месте в облаке или на границе. Именно здесь ONNX входит в.

Корпорация Майкрософт и сообщество партнеров, созданных ONNX как открытый стандарт для представления моделей машинного обучения. Модели из [многих платформ](https://onnx.ai/supported-tools) , включая TensorFlow, PyTorch, SciKit-учиться, keras, Chain, MXNET и MATLAB, можно экспортировать или преобразовать в стандартный формат ONNX. После того как модели находятся в формате ONNX, их можно запускать на различных платформах и устройствах.

[Среда выполнения ONNX](https://github.com/Microsoft/onnxruntime) — это высокопроизводительный механизм вывода для развертывания моделей ONNX в рабочей среде. Она оптимизирована как для облака, так и для пограничной работы в Linux, Windows и Mac. Написанный C++на, у него также есть C, Python C# и API. Среда выполнения ONNX обеспечивает поддержку всех спецификаций ONNX-ML, а также интегрируется с ускорителями на различных устройствах, например Тенсоррт на видеопроцессорах NVidia.

Среда выполнения ONNX используется в высокомасштабируемых службах Майкрософт, таких как Bing, Office и Cognitive Services. Выигрыш в производительности зависит от ряда факторов, но эти службы Майкрософт показали __среднее увеличение производительности ЦП на 2 раза__. Среда выполнения ONNX также используется как часть Windows ML на сотнях миллионов устройств. Среду выполнения можно использовать с Машинное обучение Azure. С помощью среды выполнения ONNX вы можете воспользоваться обширными оптимизациями рабочего уровня, тестированием и текущими улучшениями.

[![Схема потока ONNX, показывающая обучение, конвертеры и развертывание](media/concept-onnx/onnx.png)](./media/concept-onnx/onnx.png#lightbox)

## <a name="get-onnx-models"></a>Получение моделей ONNX

Модели ONNX можно получить различными способами:
+ Обучение новой модели ONNX в Машинное обучение Azure (см. примеры в нижней части этой статьи)
+ Преобразование существующей модели из другого формата в ONNX (см. [учебники](https://github.com/onnx/tutorials)) 
+ Получите предварительно обученную модель ONNX из [модели ONNX Zoo](https://github.com/onnx/models) (см. примеры в нижней части этой статьи).
+ Создание настраиваемых моделей ONNX в [пользовательской службе визуального распознавания Azure](https://docs.microsoft.com/azure/cognitive-services/Custom-Vision-Service/). 

Многие модели, включая классификацию изображений, обнаружение объектов и обработку текста, могут быть представлены в виде ONNX моделей. Однако некоторые модели, возможно, не удастся преобразовать. Если вы запустили эту ситуацию, запросите проблему в GitHub соответствующего конвертера, который вы использовали. Вы можете продолжать использовать существующую модель форматирования, пока не будет устранена данная ошибка.

## <a name="deploy-onnx-models-in-azure"></a>Развертывание моделей ONNX в Azure

С помощью Машинное обучение Azure можно развертывать и отслеживать модели ONNX, а также управлять ими. Используя стандартный [рабочий процесс развертывания](concept-model-management-and-deployment.md) и среду выполнения ONNX, можно создать конечную точку REST, размещенную в облаке. См. пример записных книжек Jupyter в конце этой статьи, чтобы испытать ее самостоятельно. 

### <a name="install-and-use-onnx-runtime-with-python"></a>Установка и использование среды выполнения ONNX с Python

Пакеты Python для среды выполнения ONNX доступны в [PyPi.org](https://pypi.org) ([ЦП](https://pypi.org/project/onnxruntime), [GPU](https://pypi.org/project/onnxruntime-gpu)). Перед установкой ознакомьтесь с [требованиями к системе](https://github.com/Microsoft/onnxruntime#system-requirements) . 

 Чтобы установить среду выполнения ONNX для Python, используйте одну из следующих команд: 
```python   
pip install onnxruntime       # CPU build
pip install onnxruntime-gpu   # GPU build
```

Для вызова среды выполнения ONNX в сценарии Python используйте следующую команду.    
```python
import onnxruntime
session = onnxruntime.InferenceSession("path to model")
```

Входные и выходные данные для использования модели см. в документации к соответствующей модели. Для просмотра модели можно также использовать такое средство визуализации, как [Netron](https://github.com/lutzroeder/Netron). Среда выполнения ONNX также позволяет запрашивать метаданные модели и ее входные и выходные данные:    
```python
session.get_modelmeta()
first_input_name = session.get_inputs()[0].name
first_output_name = session.get_outputs()[0].name
```

Для вывода модели добавьте параметр `run` и передайте список выходных данных, которые хотите получить (не заполняйте, если вам нужны все), а также карту входных значений. Результатом будет список выходных данных.  
```python
results = session.run(["output1", "output2"], {
                      "input1": indata1, "input2": indata2})
results = session.run([], {"input1": indata1, "input2": indata2})
```

Полный справочник по API Python см. в [документации по среде выполнения ONNX](https://aka.ms/onnxruntime-python).    

## <a name="examples"></a>Примеры

[how-to-use-azureml/deployment/onnx](https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/deployment/onnx): примеры записных книжек, создающих и развертывающих модели ONNX.

[!INCLUDE [aml-clone-in-azure-notebook](../../../includes/aml-clone-for-examples.md)]

## <a name="more-info"></a>Дополнительная информация

Узнайте больше об ONNX или поддержите проект:
+ [Веб-сайт проекта ONNX](https://onnx.ai)
+ [Код ONNX в GitHub](https://github.com/onnx/onnx)

Узнайте больше о среде выполнения ONNX или поддержите проект:
+ [Репозиторий GitHub для среды выполнения ONNX](https://github.com/Microsoft/onnxruntime)


