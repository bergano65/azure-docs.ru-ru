---
title: Интерпретируемость модели в Машинное обучение Azure (Предварительная версия)
titleSuffix: Azure Machine Learning
description: Узнайте, как объяснить, почему модель выполняет прогнозы с помощью пакета SDK для Машинное обучение Azure. Его можно использовать во время обучения и вывода, чтобы понять, как модель выполняет прогнозы.
services: machine-learning
ms.service: machine-learning
ms.subservice: core
ms.topic: how-to
ms.author: mesameki
author: mesameki
ms.reviewer: Luis.Quintanilla
ms.date: 07/09/2020
ms.openlocfilehash: 83fc13362a373686ee027fd642f03003b411cd63
ms.sourcegitcommit: 3541c9cae8a12bdf457f1383e3557eb85a9b3187
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 07/09/2020
ms.locfileid: "86201443"
---
# <a name="model-interpretability-in-azure-machine-learning-preview"></a>Интерпретируемость модели в Машинное обучение Azure (Предварительная версия)
[!INCLUDE [applies-to-skus](../../includes/aml-applies-to-basic-enterprise-sku.md)]

## <a name="overview-of-model-interpretability"></a>Общие сведения о интерпретируемости модели

Интерпретируемость очень важна для специалистов по обработке и анализу данных, аудиторов и руководителей бизнес-решений, чтобы обеспечить соответствие политикам компании, отраслевым стандартам и государственным нормам.

+ Специалистам по обработке и анализу данных необходима возможность объяснить свои модели руководителям и заинтересованным лицам, чтобы они могли понять ценность и точность их результатов. Они также нуждаются в интерпретации для отладки своих моделей и принятия взвешенных решений о том, как их улучшить. 

+ Для юридических аудиторий требуются средства для проверки моделей с учетом соответствия нормативным требованиям и отслеживания того, как решения моделей влияют на человека. 

+ Руководителям, ответственным за принятие бизнес-решений, требуется возможность обеспечить прозрачность для конечных пользователей. Это позволяет им получать и поддерживать отношения доверия.


Включение возможности объяснения модели машинного обучения важно в ходе двух основных этапов разработки модели:
+ На этапе обучения конструкторы моделей и оценивающие могут использовать результаты интерпретации модели для проверки на наличие этих данных и создания отношений доверия с заинтересованными лицами. Они также используют аналитические данные модели для отладки, проверки поведения модели в соответствии с целями, а также для проверки недостоверности и незначительных функций модели.

+ На этапе возникновения проблемы, так как наличие прозрачности в развернутых моделях позволяет руководителям понять, как работает модель, и как ее решения будут рассматриваться и повлиять на людей в реальном времени. 

## <a name="interpretability-with-azure-machine-learning"></a>Интерпретируемость с помощью Машинное обучение Azure

Классы интерпретации доступны через несколько пакетов SDK: (Узнайте, как [установить пакеты SDK для машинное обучение Azure](https://docs.microsoft.com/python/api/overview/azure/ml/install?view=azure-ml-py)).

* `azureml.interpret`— основной пакет, содержащий функциональные возможности, поддерживаемые корпорацией Майкрософт.

* `azureml.contrib.interpret`, предварительная версия и экспериментальные возможности, которые можно попробовать.

Используйте `pip install azureml-interpret` и `pip install azureml-interpret-contrib` для общего использования, а `pip install azureml-contrib-interpret` для аутомл используйте для получения пакетов интерпретации.


> [!IMPORTANT]
> Содержимое в `contrib` пространстве имен поддерживается не полностью. По мере того, как экспериментальные функциональные возможности становятся более зрелыми, они постепенно перемещаются в основное пространство имен.
.



## <a name="how-to-interpret-your-model"></a>Как интерпретировать модель

С помощью классов и методов в пакете SDK можно:
+ Объясните Прогноз модели, создав значения важности функций для всей модели и (или) отдельных точек. 
+ Обеспечить интерпретируемость модели в реальных наборах данных в масштабе, во время обучения и вывода.
+ Использование интерактивной панели мониторинга визуализации для обнаружения закономерностей в данных и объяснениях во время обучения


В машинном обучении **функции** — это поля данных, используемые для прогнозирования целевой точки данных. Например, для прогнозирования кредитного риска можно использовать поля данных для возраста, размера учетной записи и возраста учетной записи. В этом случае срок хранения, размер учетной записи и возраст учетной записи являются **функциями**. Важность функции сообщает, как каждое поле данных затронуло прогнозы модели. Например, возраст может сильно использоваться в прогнозе, тогда как размер и возраст учетной записи не влияют на прогнозируемые значения. Этот процесс позволяет специалистам по обработке и анализу данных объяснить результирующие прогнозы, чтобы заинтересованные лица могли видеть, какие функции наиболее важны в модели.

Узнайте о поддерживаемых методиках интерпретации, поддерживаемых моделях машинного обучения и поддерживаемых средах выполнения.


## <a name="supported-interpretability-techniques"></a>Поддерживаемые методы интерпретации

 `azureml-interpret`использует методики интерпретации, разработанные в [интерпретированном сообществе](https://github.com/interpretml/interpret-community/), пакет Python с открытым исходным кодом для обучения интерпретируемых моделей и помогая объяснить блаккбокс AI-системы. [Интерпретатор-сообщество](https://github.com/interpretml/interpret-community/) выступает в качестве узла для поддерживаемых объяснений SDK и в настоящее время поддерживает следующие методы интерпретации:

|Методика интерпретации|Описание|Тип|
|--|--|--------------------|
|Пояснение к дереву ШАП| Пояснение к дереву [ШАП](https://github.com/slundberg/shap), в котором основное внимание уделяется алгоритму оценки скорости быстрого ШАП значений времени, характерному для **деревьев и это совокупности деревьев**.|Зависящие от модели|
|Глубокое пояснение ШАП| Основываясь на пояснениях от ШАП, глубокая пояснения — алгоритм аппроксимации с высокой скоростью для ШАП значений в моделях глубокого обучения, который строится на связи с Диплифт, описанным в [документе ШАП НИПС](https://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions). Модели **TensorFlow** и модели **keras** с использованием серверной части TensorFlow поддерживаются (также доступна предварительная поддержка PyTorch).|Зависящие от модели|
|Шап линейное пояснение| Линейное пояснение ШАП рассчитывает значения ШАП для **линейной модели**, при необходимости учитывая взаимные корреляции функций.|Зависящие от модели|
|Пояснение ядра ШАП| В объяснении ядра ШАП используется специально взвешенная локальная линейная регрессия для оценки значений ШАП для **любой модели**.|Не зависит от модели|
|Пояснение к процедуре "имитировать" (Глобальный суррогат)| Концепция имитируется на основе представления [глобальных суррогатных моделей](https://christophm.github.io/interpretable-ml-book/global.html) для имитации блаккбокс моделей. Глобальная суррогатная модель — это внутренняя интерпретируемая модель, которая обучена для приблизительных прогнозов **любой модели с черными ящиками** как можно точнее. Специалисты по обработке и анализу данных могут интерпретировать суррогатную модель, чтобы рисовать выводы о модели черного ящика. В качестве суррогатной модели можно использовать одну из следующих интерпретируемых моделей: LightGBM (Лгбмексплаинаблемодел), линейная регрессия (Линеарексплаинаблемодел), модель с метод стохастического градиента (Сгдексплаинаблемодел) и дерево принятия решений (ДеЦисионтриексплаинаблемодел).|Не зависит от модели|
|Пояснение по важности функции перестановки (ПФИ)| Важность функции перестановки — это методика, используемая для объяснения моделей классификации и регрессии, которые представляют собой [статью о случайных лесах бреиман](https://www.stat.berkeley.edu/~breiman/randomforest2001.pdf) (см. раздел 10). На высоком уровне, как это работает, случайным образом перетасовывание данные по одному компоненту для всего набора данных и вычислению объема изменений в показателях производительности. Чем больше изменение, тем важнее компонент. ПФИ может объяснить общее поведение **любой базовой модели** , но не объясняет отдельные прогнозы. |Не зависит от модели|




Помимо методов интерпретации, описанных выше, мы поддерживаем более ШАПное объяснение, именуемое `TabularExplainer` . В зависимости от модели `TabularExplainer` использует один из поддерживаемых пояснений ШАП:

* Триексплаинер для всех моделей на основе дерева
* Дипексплаинер для моделей DNN
* Линеарексплаинер для линейных моделей
* Кернелексплаинер для всех других моделей

`TabularExplainer`также обладает значительными усовершенствованиями функций и производительности по прямым ШАП объяснениям:

* **Формирование сводных данных инициализации**. В случаях, когда скорость объяснения наиболее важна, мы суммируем набор данных инициализации и создадим небольшой набор репрезентативных образцов, который ускоряет создание общих и индивидуальных значений важности функций.
* **Выборка набора оценочных данных**. Если пользователь передает большой набор образцов оценки, но на самом деле они не требуют оценки, для параметра выборки можно задать значение true, чтобы ускорить вычисление общих объяснений модели.

На следующей схеме показана текущая структура поддерживаемых объяснений.

[![Архитектура интерпретации Машинное обучение](./media/how-to-machine-learning-interpretability/interpretability-architecture.png)](./media/how-to-machine-learning-interpretability/interpretability-architecture.png#lightbox)


## <a name="supported-machine-learning-models"></a>Поддерживаемые модели машинного обучения

`azureml.interpret`Пакет SDK поддерживает модели, обученные со следующими форматами наборов данных:
- `numpy.array`
- `pandas.DataFrame`
- `iml.datatypes.DenseData`
- `scipy.sparse.csr_matrix`

Функции пояснения принимают в качестве входных данных модели и конвейеры. Если модель предоставлена, то модель должна реализовать функцию прогнозирования `predict` или `predict_proba` соответствует соглашению Scikit. Если модель не поддерживает эту функцию, можно создать оболочку для модели в функции, которая создает тот же результат, что `predict` и или `predict_proba` в Scikit, и использовать эту функцию-оболочку с выбранным объяснением. Если указан конвейер, функция пояснения предполагает, что выполняющийся скрипт конвейера возвращает прогноз. Использование этой методики упаковки `azureml.interpret` может поддерживать модели, обученные с помощью платформ PyTorch, TensorFlow и keras Deep Train, а также классических моделей машинного обучения.

## <a name="local-and-remote-compute-target"></a>Локальный и удаленный целевые объекты вычислений

`azureml.interpret`Пакет предназначен для работы с локальными и удаленными целевыми объектами вычислений. При локальном запуске функции пакета SDK не будут обращаться к службам Azure. 

Вы можете запустить объяснение удаленно на Машинное обучение Azure вычислить и записать пояснения в службу журнала выполнения Машинное обучение Azure. После записи этих сведений отчеты и визуализации из описания можно легко найти в Машинное обучение Azure Studio для анализа пользователей.


## <a name="next-steps"></a>Дальнейшие действия

- См. [инструкции](how-to-machine-learning-interpretability-aml.md) по включению интерпретации для моделей, как локально, так и на машинное обучение Azure удаленных ресурсов вычислений. 
- Дополнительные сценарии см. в [примерах записных книжек](https://github.com/Azure/MachineLearningNotebooks/tree/master/how-to-use-azureml/explain-model) . 
- Если вы заинтересованы в работе с текстовыми сценариями, см. раздел [интерпретируемый текст](https://github.com/interpretml/interpret-text), связанный репозиторий с открытым кодом для [интерпретации сообщества](https://github.com/interpretml/interpret-community/), для методов интерпретации для NLP. `azureml.interpret`в настоящее время пакет не поддерживает эти методы, но вы можете приступить к работе с [примером записной книжки на классификации текста](https://github.com/interpretml/interpret-text/blob/master/notebooks/text_classification/text_classification_classical_text_explainer.ipynb).
