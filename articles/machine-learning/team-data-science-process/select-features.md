---
title: Выбор компонентов в командном процессе обработки и анализа данных
description: Описывает цели реконструирования характеристик и содержит примеры, поясняющие его роль в совершенствовании данных в процессе машинного обучения.
services: machine-learning
author: marktab
manager: cgronlun
editor: cgronlun
ms.service: machine-learning
ms.subservice: team-data-science-process
ms.topic: article
ms.date: 11/21/2017
ms.author: tdsp
ms.custom: seodec18, previous-author=deguhath, previous-ms.author=deguhath
ms.openlocfilehash: da5da64538ceaf906388c49963c0d5115e1b5ab9
ms.sourcegitcommit: c22327552d62f88aeaa321189f9b9a631525027c
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 11/04/2019
ms.locfileid: "73480223"
---
# <a name="feature-selection-in-the-team-data-science-process-tdsp"></a>Выбор характеристик в процессе обработки и анализа данных группы (TDSP)
В этой статье описаны цели выбора характеристик и приводятся примеры, поясняющие его роль в совершенствовании данных в процессе машинного обучения. Эти примеры взяты из Студии машинного обучения Azure.

Проектирование и выбор признаков являются частью процесса TDSP, описанного в статье [Жизненный цикл процесса обработки и анализа данных группы](overview.md). Проектирование и выбор характеристик входят в этап **разработки характеристик** процесса TDSP.

* **Реконструирование признаков**: этот процесс направлен на создание дополнительных признаков на основе соответствующих существующих необработанных признаков и повышение эффективности прогнозирования алгоритма обучения.
* **Выбор признаков**: в этом процессе выбирается ключевое подмножество исходных признаков с целью сокращения размерности задачи обучения.

Как правило, **проектирование признаков** сначала применяется для создания дополнительных признаков, а затем выполняется на этапе **выбора признаков**, чтобы исключить несоответствующие, избыточные или сильно коррелирующие признаки.

## <a name="filter-features-from-your-data---feature-selection"></a>Фильтрация признаков в данных. Выбор признаков
Выбор признаков — это процесс, который часто применяется для создания обучающих наборов данных для задач прогнозирующего моделирования, например задач классификации или регрессии. Целью этого является выбор подмножества признаков из исходного набора данных, уменьшение его размеров с помощью минимального набора признаков для представления максимального отклонения в данных. Это подмножество признаков используется для обучения модели. Выбор признаков служит двум основным целям.

* Во-первых, выбор признаков часто повышает точность классификации, исключая несоответствующие, избыточные или сильно коррелирующие признаки.
* Во-вторых, он сокращает число признаков, что повышает эффективность процесса обучения модели. Эффективность особенно важна для обучаемых систем, требующих больших затрат на обучение, таких как вспомогательные векторные компьютеры.

Несмотря на то, что выбор признаков нацелен на сокращение числа признаков в наборе данных, используемых для обучения модели, он не связан с термином "сокращение размерности". Методы выбора признаков извлекают поднабор из исходных признаков в данных без каких-либо изменений.  Методы сокращения размерности используют реконструированные признаки, которые могут преобразовывать исходные признаки и соответственно изменять их. Примеры методов сокращения размерности: анализ главных компонентов, анализ канонических корреляций и сингулярная декомпозиция.

В частности одна из распространенных категорий методов выбора признаков в контролируемом контексте называется "выбором признаков на основе фильтра". Эти методы применяют статистическую меру для назначения рейтинга каждому признаку путем вычисления корреляции между каждым признаком и целевым атрибутом. Затем признаки ранжируются по рейтингу, который может использоваться, чтобы задать пороговое значение для сохранения или исключения конкретных признаков. Примеры статистических показателей, используемых в этих методах: корреляция Пирсона, взаимная информация и критерий хи-квадрат.

В Студии машинного обучения Azure предусмотрены модули для выбора признаков. Как показано на следующем рисунке, эти модули включают [Выбор компонентов на основе фильтров][filter-based-feature-selection] и [Анализ линейного Discriminantного анализа Фишера][fisher-linear-discriminant-analysis].

![Модули выбора признаков](./media/select-features/feature-Selection.png)

Рассмотрим, к примеру, использование модуля [выбора компонентов на основе фильтров][filter-based-feature-selection] . Для удобства и дальше используйте пример интеллектуального анализа текста. Предположим, что требуется построить регрессионную модель после создания набора функций 256 с помощью модуля [хэширования функций][feature-hashing] , а переменная ответа — это "col1", содержащий рейтинги обзора книги в диапазоне от 1 до 5. Установив для параметра «Метод количественной оценки» значение «Корреляция Пирсона», параметра «Целевой столбец» — «Col1» и для параметра «Число необходимых признаков» — значение 50. После этого [Выбор компонентов на основе фильтра][filter-based-feature-selection] модуля создает набор данных, содержащий функции 50 вместе с целевым атрибутом col1. На следующем рисунке показан процесс этого эксперимента и входные параметры.

![Свойства модуля выбора признаков с помощью фильтра](./media/select-features/feature-Selection1.png)

На следующем рисунке показаны результирующие наборы данных.

![Результирующий набор данных для модуля выбора признаков с помощью фильтра](./media/select-features/feature-Selection2.png)

Каждый признак оценивается на основе корреляции Пирсона между ним и целевым атрибутом «Col1». Сохраняются признаки с наибольшей оценкой.

На следующем рисунке показаны соответствующие оценки для выбранных признаков.

![Оценки для модуля выбора признаков с помощью фильтра](./media/select-features/feature-Selection3.png)

Применяя этот модуль [выбора компонентов на основе фильтра][filter-based-feature-selection] , выбираются функции 50 из 256, так как они имеют наиболее коррелированные функции с целевой переменной col1, основанные на методе оценки "корреляция Пирсона".

## <a name="conclusion"></a>Заключение
Реконструирование характеристик и выбор характеристик — два часто выполняемых действия. Реконструированные и выбранные характеристики повышают эффективность процесса обучения, который пытается извлечь ключевые сведения, содержащиеся в данных. Они также дают возможность повысить возможности этих моделей, чтобы точнее классифицировать входные данные и более надежно предсказывать нужные результаты. Реконструирование и выбор признаков можно также объединять, чтобы сделать процесс обучения более алгоритмизируемым. Этого можно достичь путем расширения и сокращения числа признаков, необходимых для калибровки или обучения модели. С точки зрения математики выбранные для обучения модели признаки являются минимальным набором независимых переменных, которые определяют структуры в данных и затем успешно прогнозируют результаты.

Не всегда обязательно выполнять реконструирование или выбор признаков. Его необходимость зависит от собранных данных, выбранного алгоритма и цели эксперимента.

<!-- Module References -->
[feature-hashing]: https://msdn.microsoft.com/library/azure/c9a82660-2d9c-411d-8122-4d9e0b3ce92a/
[filter-based-feature-selection]: https://msdn.microsoft.com/library/azure/918b356b-045c-412b-aa12-94a1d2dad90f/
[fisher-linear-discriminant-analysis]: https://msdn.microsoft.com/library/azure/dcaab0b2-59ca-4bec-bb66-79fd23540080/

