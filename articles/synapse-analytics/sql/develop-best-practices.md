---
title: Рекомендации по разработке для Synapse SQL
description: Рекомендации, которые следует учитывать при разработке для Synapse SQL.
services: synapse-analytics
author: XiaoyuMSFT
manager: craigg
ms.service: synapse-analytics
ms.topic: conceptual
ms.subservice: sql
ms.date: 04/15/2020
ms.author: xiaoyul
ms.reviewer: igorstan
ms.openlocfilehash: a5e514602668c96d63562e45fb114cf9770a54a9
ms.sourcegitcommit: 96918333d87f4029d4d6af7ac44635c833abb3da
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 11/04/2020
ms.locfileid: "93321484"
---
# <a name="development-best-practices-for-synapse-sql"></a>Рекомендации по разработке для Synapse SQL

В этой статье приводятся руководство и рекомендации по разработке решения хранилища данных. 

## <a name="dedicated-sql-pool-development-best-practices"></a>Рекомендации по разработке выделенного пула SQL

### <a name="reduce-cost-with-pause-and-scale"></a>Снижение расходов за счет приостановки и масштабирования ресурсов

Дополнительные сведения о сокращении затрат при помощи приостановки и масштабирования см. в статье об [управлении вычислительными ресурсами](../sql-data-warehouse/sql-data-warehouse-manage-compute-overview.md?toc=/azure/synapse-analytics/toc.json&bc=/azure/synapse-analytics/breadcrumb/toc.json).

### <a name="maintain-statistics"></a>Обеспечение статистики

Убедитесь, что вы обновляете статистику ежедневно или после каждой нагрузки.  Однако всегда есть компромиссы между производительностью и затратами на создание и обновление статистики. Если вы обнаружите слишком много времени для поддержания всей статистики, более избирательно выбирать, какие столбцы имеют статистику или какие столбцы нуждаются в частом обновлении.  

Например, может потребоваться обновить столбцы даты, где новые значения могут добавляться ежедневно. 

> [!NOTE]
> Статистику рекомендуется вести в столбцах, которые являются частью объединения, используются в предложении WHERE или GROUP BY.

Дополнительные сведения см. в статьях [Управление статистикой таблиц в хранилище данных SQL](develop-tables-statistics.md), [CREATE STATISTICS](/sql/t-sql/statements/create-statistics-transact-sql?toc=/azure/synapse-analytics/toc.json&bc=/azure/synapse-analytics/breadcrumb/toc.json&view=azure-sqldw-latest&preserve-view=true) и [UPDATE STATISTICS](/sql/t-sql/statements/update-statistics-transact-sql?toc=/azure/synapse-analytics/toc.json&bc=/azure/synapse-analytics/breadcrumb/toc.json&view=azure-sqldw-latest&preserve-view=true).

### <a name="hash-distribute-large-tables"></a>Хэш-распределение больших таблиц

По умолчанию таблицы распределяются по методу циклического перебора. Эта возможность позволяет пользователям легко начать создавать таблицы, не принимая решение о том, как следует распределять их таблицы.  Таблицы с циклическим перебором могут быть достаточно эффективными для некоторых рабочих нагрузок. Но в большинстве случаев выбор столбца распределения будет выполняться гораздо лучше.  

Наглядно это превосходство можно увидеть при объединении больших таблиц фактов.  

Например, если имеется таблица Orders, распределенная по order_id, и таблица Transactions, также распространяемая order_id, при присоединении таблицы Orders к таблице Transactions на order_id этот запрос превращается в сквозной запрос. 

Это исключает выполнение операций перемещения данных.  Чем меньше в запросе действий, тем быстрее он выполняется.  Скорость выполнения запроса также зависит от объема перемещаемых данных.

> [!TIP]
> При загрузке распределенной таблицы входящие данные не должны быть отсортированы по ключу распределения, так как это замедлит процесс загрузки.  

Ниже приведены ссылки на дополнительные разделы, содержащие сведения о том, как с помощью столбца распределения можно улучшить производительность и как определить распределенную таблицу в предложение WITH инструкции CREATE TABLES.

Дополнительные сведения см. в статьях [Общие сведения о таблицах](develop-tables-overview.md), [Распределение таблиц](../sql-data-warehouse/sql-data-warehouse-tables-distribute.md?toc=/azure/synapse-analytics/toc.json&bc=/azure/synapse-analytics/breadcrumb/toc.json), [Выбор распределения таблиц](https://blogs.msdn.microsoft.com/sqlcat/20../../choosing-hash-distributed-table-vs-round-robin-distributed-table-in-azure-sql-dw-service/), [CREATE TABLE](/sql/t-sql/statements/create-table-azure-sql-data-warehouse?toc=/azure/synapse-analytics/toc.json&bc=/azure/synapse-analytics/breadcrumb/toc.json&view=azure-sqldw-latest&preserve-view=true) и [CREATE TABLE AS SELECT](/sql/t-sql/statements/create-table-as-select-azure-sql-data-warehouse?toc=/azure/synapse-analytics/toc.json&bc=/azure/synapse-analytics/breadcrumb/toc.json&view=azure-sqldw-latest&preserve-view=true).

### <a name="do-not-over-partition"></a>Недопущение избыточного секционирования
Несмотря на то, что секционирование данных — это эффективный способ управления данными, который реализуется благодаря переключению секций или оптимизации сканирования путем исключения секций, наличие большого количества секций может повлиять на производительность запросов.  Зачастую стратегия секционирования с высокой степенью детализации, которая хорошо работает на SQL Server может не работать в выделенном пуле SQL.  

> [!NOTE]
> Зачастую стратегия секционирования с высокой степенью детализации, которая хорошо работает на SQL Server может не работать в выделенном пуле SQL.  

Слишком большое количество секций снижает эффективность кластеризованных индексов Columnstore, если в секции содержится менее миллиона строк. Выделенный пул SQL разделяет данные на базы данных 60. 

Таким образом, если создается таблица со 100 разделами, получится 6000 разделов.  Все рабочие нагрузки отличаются, поэтому рекомендуется поэкспериментировать с секционированием, чтобы выбрать наиболее подходящее количество секций для вашей рабочей нагрузки.  

Одним из вариантов является использование степени детализации ниже той, которая подошла бы для SQL Server.  Например, попробуйте использовать еженедельные или ежемесячные секции вместо ежедневных.

Дополнительные сведения см. в статье [Секционирование таблиц](../sql-data-warehouse/sql-data-warehouse-tables-partition.md?toc=/azure/synapse-analytics/toc.json&bc=/azure/synapse-analytics/breadcrumb/toc.json).

### <a name="minimize-transaction-sizes"></a>Уменьшение размера транзакций

Инструкции INSERT, UPDATE и DELETE выполняются в транзакциях. В случае сбоя их необходимо откатить.  Чтобы сократить время выполнения отката, необходимо по возможности уменьшить размеры транзакций.  Это можно сделать, разделив инструкции INSERT, UPDATE и DELETE на части.  

Например, если вы ожидаете, что операция INSERT будет выполняться 1 час, ее можно разделить на четыре части, сократив время выполнения до 15 минут.

> [!TIP]
> К пустым таблицам можно применять специальные операции, которые сопровождаются записью в журнал минимальных сведений, (такие как CTAS, TRUNCATE, DROP TABLE или INSERT), чтобы снизить риск отката.  

Устранить откаты также можно, используя для управления данными только операции с метаданными (например, переключение секций).  

Например, вместо выполнения инструкции DELETE для удаления всех строк в таблице, упорядоченной по идентификатору order_date (октябрь 2001 г.), данные можно секционировать ежемесячно, а потом переключить секцию с данными на пустую секцию из другой таблицы (см. примеры использования инструкции ALTER TABLE).  

Используя инструкцию CTAS вместо DELETE, можно записать данные, которые необходимо сохранить в несекционированной таблице.  Выполнение CTAS займет столько же времени, и эта инструкция намного безопаснее, так как она выполняет мало записей в журнал о транзакциях и при необходимости ее можно быстро отменить.

Дополнительные сведения см. в статьях [Сведения о транзакциях](develop-transactions.md), [Оптимизация транзакций](../sql-data-warehouse/sql-data-warehouse-develop-best-practices-transactions.md?toc=/azure/synapse-analytics/toc.json&bc=/azure/synapse-analytics/breadcrumb/toc.json), [Секционирование таблиц](../sql-data-warehouse/sql-data-warehouse-tables-partition.md?toc=/azure/synapse-analytics/toc.json&bc=/azure/synapse-analytics/breadcrumb/toc.json), [TRUNCATE TABLE](/sql/t-sql/statements/truncate-table-transact-sql?toc=/azure/synapse-analytics/toc.json&bc=/azure/synapse-analytics/breadcrumb/toc.json&view=azure-sqldw-latest&preserve-view=true), [ALTER TABLE](/sql/t-sql/statements/alter-table-transact-sql?toc=/azure/synapse-analytics/toc.json&bc=/azure/synapse-analytics/breadcrumb/toc.json&view=azure-sqldw-latest&preserve-view=true) и [Функция Create Table As Select (CTAS)](../sql-data-warehouse/sql-data-warehouse-develop-ctas.md?toc=/azure/synapse-analytics/toc.json&bc=/azure/synapse-analytics/breadcrumb/toc.json).

### <a name="use-the-smallest-possible-column-size"></a>Использование минимального размера столбца

При определении DDL рекомендуется использовать поддерживаемый тип данных с наименьшим размером. Это позволит повысить производительность запросов. Это действие особенно важно для столбцов CHAR и VARCHAR.  

Если самое длинное значение в столбце состоит из 25 знаков, столбец необходимо определить как VARCHAR(25).  Не рекомендуется использовать по умолчанию длинные значения столбцов.  Кроме того, определяйте столбцы как VARCHAR, если это все, что необходимо, а не использовать NVARCHAR.

Дополнительные сведения см. в статьях, посвященных [общим сведениям о таблицах](develop-tables-overview.md), [типам данных таблиц](develop-tables-data-types.md) и [инструкции CREATE TABLE](/sql/t-sql/statements/create-table-azure-sql-data-warehouse?toc=/azure/synapse-analytics/toc.json&bc=/azure/synapse-analytics/breadcrumb/toc.json&view=azure-sqldw-latest&preserve-view=true).

### <a name="optimize-clustered-columnstore-tables"></a>Оптимизация таблиц с кластеризованными индексами columnstore

Кластеризованные индексы columnstore — это один из наиболее эффективных способов хранения данных в выделенном пуле SQL.  По умолчанию таблицы в выделенном пуле SQL создаются как кластеризованный ColumnStore.  

Качество кластеризованного сегмента Columnstore существенно влияет на эффективность выполнения запросов в таблицах с кластеризованными индексами Columnstore.  Если во время записи строк в таблицы Columnstore возникает нехватка памяти, качество сегмента Columnstore может ухудшиться.  

Качество сегмента можно изменить по числу строк в сжатой группе строк.  Пошаговые инструкции по обнаружению и улучшению качества сегментов для кластеризованных таблиц columnstore см. в статьях [причины низкого качества индекса columnstore](../sql-data-warehouse/sql-data-warehouse-tables-index.md?toc=/azure/synapse-analytics/toc.json&bc=/azure/synapse-analytics/breadcrumb/toc.json#causes-of-poor-columnstore-index-quality) и [табличных индексов](../sql-data-warehouse/sql-data-warehouse-tables-index.md?toc=/azure/synapse-analytics/toc.json&bc=/azure/synapse-analytics/breadcrumb/toc.json) .  

Так как высококачественные сегменты columnstore важны, рекомендуется использовать для загрузки данных идентификаторы пользователей, которым назначен класс ресурсов среднего или большого размера. Использование [единиц использования хранилища данных](resource-consumption-models.md) меньшего размера требует присвоения пользователю загрузки класса ресурсов большего размера.

Так как таблицы columnstore обычно не отправляют данные в сжатый сегмент columnstore до тех пор, пока в таблице не будет более 1 000 000 строк, а каждая Выделенная таблица пула SQL секционирована в 60 таблиц, то таблицы columnstore не смогут получить запрос, если только таблица не содержит более 60 000 000 строк.  

> [!TIP]
> Для таблиц, в которых менее 60 миллионов строк, индекс сolumstore, как правило, лучше не использовать.  

Кроме того, при секционировании данных необходимо учитывать, что каждая секция должна иметь 1 000 000 строк, чтобы получить преимущество от кластеризованного индекса columnstore.  Если таблица состоит из 100 секций, то, чтобы использовать кластеризованный индекс columnstore, она должна состоять как минимум из 6 миллиардов строк (60 распределений *100 секций* 1 миллион строк).  

Если таблица не содержит такого количества строк, рекомендуется уменьшить количество секций или использовать таблицу без кластеризованных индексов (кучу).  Чтобы получить более высокую производительность, возможно, вместо кластеризованной таблицы стоит использовать таблицу без кластеризованных индексов, содержащую вторичные индексы.

Если выбрать только необходимые столбцы, запросы к таблице ColumnStore будут выполняться быстрее.  

Ознакомьтесь также со статьями [Индексирование таблиц](../sql-data-warehouse/sql-data-warehouse-tables-index.md?toc=/azure/synapse-analytics/toc.json&bc=/azure/synapse-analytics/breadcrumb/toc.json), [Руководство по индексам Columnstore](/sql/relational-databases/indexes/columnstore-indexes-overview?toc=/azure/synapse-analytics/toc.json&bc=/azure/synapse-analytics/breadcrumb/toc.json&view=azure-sqldw-latest&preserve-view=true) и [Перестроение индексов Columnstore](../sql-data-warehouse/sql-data-warehouse-tables-index.md?toc=/azure/synapse-analytics/toc.json&bc=/azure/synapse-analytics/breadcrumb/toc.json#rebuilding-indexes-to-improve-segment-quality).

## <a name="serverless-sql-pool-development-best-practices"></a>Рекомендации по разработке для бессерверного пула SQL

### <a name="general-considerations"></a>Общие рекомендации

Бессерверный пул SQL позволяет запрашивать файлы в учетных записях хранения Azure. У него нет возможностей локального хранилища или приема, что означает, что все файлы, предназначенные для запроса, являются внешними для несерверного пула SQL. Таким образом, все, что связано с чтением файлов из хранилища, может оказать влияние на производительность запросов.

### <a name="colocate-azure-storage-account-and-serverless-sql-pool"></a>Совместное размещение учетной записи хранения Azure и несерверного пула SQL

Чтобы максимально сокращать задержку, совместно находите учетную запись хранения Azure и конечную точку пула SQL, бессерверной. Учетные записи хранения и конечные точки, подготовленные во время создания рабочей области, расположены в одном регионе.

Для обеспечения оптимальной производительности при доступе к другим учетным записям хранения с помощью бессерверного пула SQL убедитесь, что они находятся в одном регионе. В противном случае будет увеличена задержка сетевой передачи данных между удаленным регионом и регионом конечной точки.

### <a name="azure-storage-throttling"></a>Регулирование службы хранилища Azure

Несколько приложений и служб могут получить доступ к вашей учетной записи хранения. При объединении операций ввода-вывода в секунду или пропускной способности, формируемой приложениями, службами и рабочей нагрузкой пула SQL, не превышающей пределы учетной записи хранения, происходит регулирование хранилища. Регулирование хранилища существенно снижает производительность запросов.

После обнаружения регулирования серверный пул SQL имеет встроенную обработку этого сценария. Бессерверный пул SQL делает запросы к хранилищу в более низком темпе, пока регулирование не будет разрешено. 

Однако для оптимального выполнения запросов рекомендуется не перегрузить учетную запись хранения другими рабочими нагрузками во время выполнения запроса.

### <a name="prepare-files-for-querying"></a>Подготовка файлов к запросам

По возможности вы можете подготовить файлы для повышения производительности:

- Преобразуйте CSV-файл в Parquet-файл (Parquet — это формат столбцов). Так как он сжимается, размер файлов меньше, чем у CSV-файлов с теми же данными, а для бессерверного пула SQL потребуется меньше времени и запросов на хранение.
- Если запрос предназначен для одного большого файла, рекомендуется разделить его на несколько файлов меньшего размера.
- Попробуйте использовать CSV-файл размером менее 10 ГБ.
- Лучше иметь одинаковый размер файлов для одного пути OPENROWSET или для РАСПОЛОЖЕНИЯ внешней таблицы.
- Разделите данные, сохраняя разделы в разных папках или именах файлов (см. раздел об [использовании функций filename и filepath для конкретных разделов](#use-fileinfo-and-filepath-functions-to-target-specific-partitions)).

### <a name="use-fileinfo-and-filepath-functions-to-target-specific-partitions"></a>Использование функций fileinfo и filepath для назначения конкретных разделов

Данные часто организованы в разделы. Можно указать бессерверному пулу SQL запросы к определенным папкам и файлам. Это позволит сократить количество файлов и объем данных, необходимых для чтения и обработки запроса. 

Следовательно, производительность будет выше. Дополнительные сведения см. в разделах о функциях [filename](query-data-storage.md#filename-function) и [filepath](query-data-storage.md#filepath-function), а также в примерах [запрашивания конкретных файлов](query-specific-files.md).

Если данные в хранилище не секционированы, рекомендуется их секционировать, чтобы можно было использовать эти функции для оптимизации запросов, предназначенных для этих файлов.

При [запросе секционированных Apache Spark для внешних таблиц Azure синапсе](develop-storage-files-spark-tables.md) из несерверного пула SQL запрос автоматически наследует только необходимые файлы.

### <a name="use-cetas-to-enhance-query-performance-and-joins"></a>Использование CETAS для повышения производительности и улучшения соединений запросов

[CETAS](develop-tables-cetas.md) — одна из наиболее важных функций, доступных в бессерверном пуле SQL. CETAS — это параллельная операция, которая создает метаданные внешней таблицы и экспортирует результаты запроса SELECT в набор файлов в учетной записи хранения.

CETAS можно использовать для сохранения часто используемых частей запросов (например, соединенных ссылочных таблиц) в новый набор файлов. Позже можно выполнить соединение с этой отдельной внешней таблицей вместо повторного выполнения операций соединений в нескольких запросах. 

Поскольку CETAS создает файлы Parquet, статистика будет создана автоматически, когда первый запрос обращается к этой внешней таблице, и вы получите повышенную производительность.

### <a name="next-steps"></a>Дальнейшие действия

Если вам нужна информация, не указанная в этой статье, используйте функцию **поиска doc** в левой части этой страницы, чтобы найти все документы пула SQL.  На [странице Microsoft Q&A вопрос для Azure синапсе Analytics](https://docs.microsoft.com/answers/topics/azure-synapse-analytics.html) можно задать вопросы для других пользователей и для группы разработчиков Azure синапсе Analytics. Мы регулярно просматриваем этот форум и следим за тем, чтобы другие пользователи или наши специалисты ответили на интересующие вас вопросы.  

Если вы предпочитаете задавать вопросы на Stack Overflow, у нас также есть [форум Stack overflow Azure синапсе Analytics](https://stackoverflow.com/questions/tagged/azure-sqldw).
 
