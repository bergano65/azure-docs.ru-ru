---
title: Настройка производительности TCP/IP для виртуальных машин Azure | Документация Майкрософт
description: Ознакомьтесь с различными распространенными методами настройки производительности TCP/IP и их связью с виртуальными машинами Azure.
services: virtual-network
documentationcenter: na
author: rimayber
manager: paragk
editor: ''
ms.assetid: ''
ms.service: virtual-network
ms.devlang: na
ms.topic: article
ms.tgt_pltfrm: na
ms.workload: infrastructure-services
ms.date: 04/02/2019
ms.author: rimayber
ms.reviewer: dgoddard, stegag, steveesp, minale, btalb, prachank
ms.openlocfilehash: bb23484903ac3ce129c6e7a7a27e0765c227fb1d
ms.sourcegitcommit: a8b638322d494739f7463db4f0ea465496c689c6
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 07/17/2019
ms.locfileid: "68297782"
---
# <a name="tcpip-performance-tuning-for-azure-vms"></a>Настройка производительности TCP/IP для виртуальных машин Azure

В этой статье рассматриваются распространенные методы настройки производительности TCP/IP и некоторые моменты, которые следует учитывать при их использовании для виртуальных машин, работающих в Azure. В нем приводятся основные сведения о методах и их настройке.

## <a name="common-tcpip-tuning-techniques"></a>Распространенные методы настройки TCP/IP

### <a name="mtu-fragmentation-and-large-send-offload"></a>MTU, фрагментация и разгрузка большой отправки

#### <a name="mtu"></a>ОПРЕДЕЛЯЮЩ

Максимальный размер блока передаваемых данных (MTU) — это самый крупный кадр (пакет), заданный в байтах, который может быть отправлен через сетевой интерфейс. Параметр MTU является настраиваемым. Значение MTU по умолчанию, используемое на виртуальных машинах Azure, и параметр по умолчанию для большинства сетевых устройств, составляет 1 500 байт.

#### <a name="fragmentation"></a>Фрагментации

Фрагментация происходит при отправке пакета, превышающего MTU сетевого интерфейса. Стек TCP/IP прервет пакет на более мелкие части (фрагменты), соответствующие MTU интерфейса. Фрагментация происходит на уровне IP-адреса и не зависит от базового протокола (например, TCP). Если 2 000-байтовый пакет передается через сетевой интерфейс с MTU 1 500, пакет будет разбит на пакеты 1 1 500-Byte и 1 500-Byte.

Сетевые устройства в пути между источником и назначением могут либо сбрасывать пакеты, превышающие MTU, либо фрагментировать пакет на меньшие части.

#### <a name="the-dont-fragment-bit-in-an-ip-packet"></a>Бит «не фрагментировать» в пакете IP

Бит «не фрагментировать» (DF) является флагом в заголовке IP-протокола. Бит DF указывает, что сетевые устройства в пути между отправителем и получателем не должны фрагментировать пакет. Этот бит можно задать по многим причинам. (Один из примеров см. в подразделе «обнаружение MTU пути» этой статьи.) Когда сетевое устройство получает пакет с установленным битом «не фрагментировать», а этот пакет превышает MTU интерфейса, стандартное поведение устройства — удаление пакета. Устройство отправляет сообщение о фрагментации ICMP обратно в исходный источник пакета.

#### <a name="performance-implications-of-fragmentation"></a>Влияние фрагментации на производительность

Фрагментация может негативно сказаться на производительности. Одной из основных причин влияния на производительность является влияние фрагментации ЦП на память и пересборкой пакетов. Когда сетевому устройству необходимо выполнить фрагментацию пакета, ему потребуется выделить ресурсы ЦП и памяти для выполнения фрагментации.

То же самое происходит при сборке пакета. Сетевое устройство должно хранить все фрагменты, пока они не будут получены, чтобы их можно было собрать в исходный пакет. Этот процесс фрагментации и повторной сборки может также вызвать задержку.

Другая возможная отрицательная производительность фрагментации заключается в том, что фрагментированные пакеты могут поступать не по порядку. Если пакеты получены не по порядку, некоторые типы сетевых устройств могут быть удалены. В этом случае необходимо повторно передать весь пакет.

Фрагменты обычно удаляются с помощью таких устройств безопасности, как сетевые брандмауэры или когда буферы приема для сетевого устройства исчерпаны. Когда буферы приема для сетевого устройства исчерпаны, сетевое устройство пытается повторно собрать фрагментированный пакет, но не имеет ресурсов для хранения и повторного предположения пакета.

Фрагментация может рассматриваться как отрицательная операция, но поддержка фрагментации необходима при подключении различных сетей через Интернет.

#### <a name="benefits-and-consequences-of-modifying-the-mtu"></a>Преимущества и последствия изменения MTU

В целом, можно создать более эффективную сеть, увеличив значение MTU. Каждый передаваемый пакет содержит сведения о заголовке, которые добавляются к исходному пакету. Когда фрагментация создает больше пакетов, накладные расходы на заголовки становятся более эффективными, что делает сеть менее эффективной.

Ниже приведен пример. Размер заголовка Ethernet составляет 14 байт плюс последовательность с 4 байтами для проверки согласованности кадров. Если отправляется 1 2 000-байтовый пакет, в сеть добавляется 18 байт ресурсов Ethernet. Если пакет фрагментирован в 1 500-байтовый пакет и 500-байтовый пакет, каждый пакет будет содержать 18 байт заголовка Ethernet, а всего 36 байт.

Помните, что увеличение MTU не обязательно приводит к повышению эффективности сети. Если приложение отправляет только 500-байтовые пакеты, накладные расходы заголовков будут существовать, если значение MTU равно 1 500 байт или 9 000 байт. Сеть станет более эффективной, только если она использует большие размеры пакетов, которые зависят от MTU.

#### <a name="azure-and-vm-mtu"></a>Azure и MTU ВМ

Значение MTU по умолчанию для виртуальных машин Azure составляет 1 500 байт. Стек виртуальной сети Azure попытается фрагментировать пакет на 1 400 байт.

Обратите внимание, что стек виртуальной сети не является неэффективным, так как он фрагменты передаются в 1 400 байт, хотя виртуальные машины имеют MTU 1 500. Большой процент сетевых пакетов намного меньше 1 400 или 1 500 байт.

#### <a name="azure-and-fragmentation"></a>Azure и фрагментация

Виртуальный сетевой стек настроен на удаление фрагментов неупорядоченного кода, то есть фрагментированных пакетов, которые не поступают в исходный фрагментированный порядок. Эти пакеты удаляются в основном из-за уязвимости сетевой безопасности, объявленной в ноябре 2018 с именем Фрагментсмакк.

Фрагментсмакк — это ошибка в том, как ядро Linux обрабатывает пересборку фрагментированных пакетов IPv4 и IPv6. Удаленный злоумышленник может использовать этот изъян для активации ресурсоемких операций пересборки фрагментов, что может привести к увеличению времени ЦП и отказу в обслуживании в целевой системе.

#### <a name="tune-the-mtu"></a>Настройка MTU

Вы можете настроить значение MTU виртуальной машины Azure, как и в любой другой операционной системе. Но при настройке MTU следует учитывать фрагментацию, которая происходит в Azure, описанную выше.

Мы не рекомендуем клиентам увеличивать MTU виртуальных машин. В этом обсуждении рассказывается о том, как Azure реализует MTU и выполняет фрагментацию.

> [!IMPORTANT]
>Увеличение MTU не известно для повышения производительности и может негативно сказаться на производительности приложения.
>
>

#### <a name="large-send-offload"></a>Разгрузка большой отправки

Разгрузка большой отправки (LSO) может повысить производительность сети за счет разгрузки сегментации пакетов на адаптер Ethernet. Если параметр LSO включен, стек TCP/IP создает большой TCP-пакет и отправляет его на адаптер Ethernet для сегментации перед его пересылкой. Преимущество «LSO» заключается в том, что он может освободить ЦП от сегментирования пакетов до размеров, соответствующих MTU и разгрузку, которая обрабатывает интерфейс Ethernet, где он выполняется на оборудовании. Дополнительные сведения о преимуществах LSO см. в разделе [поддержка разгрузки больших](https://docs.microsoft.com/windows-hardware/drivers/network/performance-in-network-adapters#supporting-large-send-offload-lso)отправок.

Если включено LSO, клиенты Azure могут видеть крупные размеры кадров при выполнении записи пакетов. Эти крупные размеры кадров могут привести к тому, что некоторые клиенты могли подумать о фрагментации или что в противном случае используется большой MTU. В случае с параметром LSO адаптер Ethernet может объявить в стеке TCP/IP больший максимальный размер сегмента (MSS) для создания большего пакета TCP. Затем весь несегментированный кадр перенаправляется на адаптер Ethernet и отображается в записи пакетов, выполняемой на виртуальной машине. Но этот пакет будет разбит на несколько меньших кадров через адаптер Ethernet в соответствии с MTU адаптера Ethernet.

### <a name="tcp-mss-window-scaling-and-pmtud"></a>Масштабирование и ПМТУД окна MSS TCP

#### <a name="tcp-maximum-segment-size"></a>Максимальный размер сегмента TCP

Максимальный размер сегмента TCP (MSS) — это параметр, ограничивающий размер TCP-сегментов, что позволяет избежать фрагментации пакетов TCP. В операционных системах эта формула обычно используется для установки MSS:

`MSS = MTU - (IP header size + TCP header size)`

Заголовок IP и заголовок TCP имеют размер 20 байт или 40 байт. Поэтому интерфейс с MTU 1 500 будет иметь размер MSS 1 460. Однако размер MSS можно настроить.

Этот параметр придается в третьем случае по протоколу TCP, если сеанс TCP настроен между источником и назначением. Обе стороны отправляют значение MSS, а нижняя из двух используется для TCP-соединения.

Помните, что значения MTU источника и назначения не являются единственными факторами, определяющими значение MSS. На промежуточных сетевых устройствах, таких как VPN-шлюзы Azure, можно настроить MTU независимо от источника и назначения, чтобы обеспечить оптимальную производительность сети.

#### <a name="path-mtu-discovery"></a>Обнаружение MTU пути

Параметр MSS согласовывается, но он может не указывать на реальный размер MSS, который можно использовать. Это связано с тем, что другие сетевые устройства в пути между источником и назначением могут иметь более низкую величину MTU, чем у источника и назначения. В этом случае устройство, значение MTU которого меньше, чем пакет, удалит пакет. Устройство будет передавать сообщение о необходимости фрагментации ICMP (тип 3, код 4), который содержит MTU. Это сообщение ICMP позволяет исходному узлу уменьшить MTU соответствующего пути. Процесс называется «обнаружение MTU пути» (ПМТУД).

Процесс ПМТУД неэффективен и влияет на производительность сети. При отправке пакетов, превышающих MTU сетевого пути, пакеты необходимо передавать с более низким значением MSS. Если отправитель не получит сообщение о необходимости фрагментации ICMP, возможно, из-за сетевого брандмауэра в пути (который обычно называется *пмтуд блаккхоле*), отправителю не известно, что необходимо уменьшить MSS и будет постоянно передаваться пакет. Поэтому мы не рекомендуем увеличивать значение MTU виртуальной машины Azure.

#### <a name="vpn-and-mtu"></a>VPN и MTU

Если вы используете виртуальные машины, которые выполняют инкапсуляцию (например, VPN-виртуальные сети), существуют некоторые дополнительные соображения, касающиеся размера пакета и MTU. Виртуальные частные сети добавляют дополнительные заголовки к пакетам, что увеличивает размер пакета и требует меньшего размера MSS.

Для Azure рекомендуется установить для фиксации TCP-порта MSS значение 1 350 байт и интерфейс туннеля MTU в 1 400. Дополнительные сведения см. на [странице VPN-устройства и параметры IPsec/IKE](https://docs.microsoft.com/azure/vpn-gateway/vpn-gateway-about-vpn-devices).

### <a name="latency-round-trip-time-and-tcp-window-scaling"></a>Задержка, время приема-передачи и масштабирование окна TCP

#### <a name="latency-and-round-trip-time"></a>Задержка и время приема-передачи

Задержка в сети регулируется скоростью света в оптоволоконной сети. Пропускная способность сети TCP также эффективно регулируется по времени приема-передачи (RTT) между двумя сетевыми устройствами.

| | | | |
|-|-|-|-|
|**Route**|**Друг**|**Одностороннее время**|**RTT**|
|Нью Йорк — Сан, Санкт|4 148 км|21 мс|42 мс|
|Нью Йорк — Лондон|5 585 км|28 МС|56 МС|
|Нью Йорк — Сидней|15 993 км|80 мс|160 мс|

В этой таблице показано расстояние между двумя расположениями в прямой строке. В сетях расстояние обычно превышает значение прямой линии. Ниже приведена простая формула для вычисления минимального RTT, которое регулируется скоростью освещения:

`minimum RTT = 2 * (Distance in kilometers / Speed of propagation)`

Для скорости распространения можно использовать 200. Это расстояние в метрах, которое перемещается в 1 миллисекунду.

Возьмем в качестве примера новый Москва в Сан Франциско. Расстояние прямой линии составляет 4 148 км. Поключив это значение в уравнение, мы получаем следующее:

`Minimum RTT = 2 * (4,148 / 200)`

Результат уравнения задается в миллисекундах.

Если вы хотите получить лучшую производительность сети, логично выбрать назначения с кратчайшим расстоянием между ними. Кроме того, необходимо спроектировать виртуальную сеть, чтобы оптимизировать путь к трафику и сократить задержку. Дополнительные сведения см. в подразделе «вопросы проектирования сети» этой статьи.

#### <a name="latency-and-round-trip-time-effects-on-tcp"></a>Влияние задержки и времени кругового пути на TCP

Время приема-передачи напрямую влияет на максимальную пропускную способность TCP. В протоколе TCP *Размер окна* — это максимальный объем трафика, который может быть отправлен через TCP-соединение, прежде чем отправителю нужно получить подтверждение от получателя. Если для TCP MSS задано значение 1 460, а для параметра размер окна TCP задано значение 65 535, отправитель может 45 отправлять пакеты, прежде чем получать подтверждение от получателя. Если отправитель не получает подтверждение, данные будут передаваться повторно. Вот формула:

`TCP window size / TCP MSS = packets sent`

В этом примере 65 535/1 460 округляется до 45.

Это состояние "ожидание подтверждения" — механизм обеспечения надежной доставки данных, который заставляет RTT влиять на пропускную способность TCP. Чем дольше отправитель ждет подтверждения, тем дольше будет необходимо подождать перед отправкой дополнительных данных.

Ниже приведена формула для вычисления максимальной пропускной способности отдельного TCP-подключения:

`Window size / (RTT latency in milliseconds / 1,000) = maximum bytes/second`

В этой таблице показана максимальная пропускная способность (в МБ/с) для одного TCP-подключения. (Для удобства чтения используется единица измерения в мегабайтах.)

| | | | |
|-|-|-|-|
|**Размер окна TCP (байт)**|**Задержка приема-передачи (МС)**|**Максимальная пропускная способность в мегабайтах и секундах**|**Максимальная пропускная способность мегабит/сек**|
|65 535|1|65,54|524,29|
|65 535|30|2,18|17,48|
|65 535|60|1,09|8,74|
|65 535|90|.73|5,83|
|65 535|120|.55|4.37|

Если пакеты теряются, максимальная пропускная способность подключения TCP будет снижена, пока отправитель повторно передает уже отправленные данные.

#### <a name="tcp-window-scaling"></a>Масштабирование окна TCP

Масштабирование окна TCP — это метод, который динамически увеличивает размер окна TCP, позволяя отправлять больше данных, прежде чем требуется подтверждение. В предыдущем примере пакеты 45 были бы отправлены раньше, чем требовалось подтверждение. При увеличении числа пакетов, которые могут быть отправлены до появления подтверждения, уменьшается количество случаев, когда отправитель ожидает подтверждения, что увеличивает максимальную пропускную способность TCP.

В следующей таблице показаны эти связи:

| | | | |
|-|-|-|-|
|**Размер окна TCP (байт)**|**Задержка приема-передачи (МС)**|**Максимальная пропускная способность в мегабайтах и секундах**|**Максимальная пропускная способность мегабит/сек**|
|65 535|30|2,18|17,48|
|131 070|30|4.37|34,95|
|262 140|30|8,74|69,91|
|524 280|30|17,48|139,81|

Но значение заголовка TCP для окна TCP имеет длину всего 2 байта, что означает, что максимальное значение для окна приема — 65 535. Для увеличения максимального размера окна был введен коэффициент масштабирования окна TCP.

Коэффициент масштабирования является также параметром, который можно настроить в операционной системе. Ниже приведена формула для вычисления размера окна TCP с использованием коэффициентов масштабирования:

`TCP window size = TCP window size in bytes \* (2^scale factor)`

Ниже приведено Вычисление коэффициента масштабирования окна 3 и размера окна 65 535:

`65,535 \* (2^3) = 262,140 bytes`

Коэффициент масштабирования, равный 14, приводит к увеличению размера окна TCP, равному 14 (максимально допустимое смещение). Размер окна TCP будет 1 073 725 440 байт (8,5 гигабит).

#### <a name="support-for-tcp-window-scaling"></a>Поддержка масштабирования окна TCP

Windows может задавать различные коэффициенты масштабирования для разных типов подключений. (Классы подключений включают в себя центр обработки данных, Интернет и т. д.) Чтобы просмотреть тип `Get-NetTCPConnection` подключения масштабирования окна, используйте команду PowerShell:

```powershell
Get-NetTCPConnection
```

Для просмотра значений каждого `Get-NetTCPSetting` класса можно использовать команду PowerShell:

```powershell
Get-NetTCPSetting
```

Вы можете задать начальный размер окна TCP и коэффициент масштабирования TCP в Windows с помощью `Set-NetTCPSetting` команды PowerShell. Дополнительные сведения см. в разделе [Set-нетткпсеттинг](https://docs.microsoft.com/powershell/module/nettcpip/set-nettcpsetting?view=win10-ps).

```powershell
Set-NetTCPSetting
```

Это действующие параметры TCP для `AutoTuningLevel`:

| | | | |
|-|-|-|-|
|**аутотунинглевел**|**Коэффициент масштабирования**|**Коэффициент масштабирования**|**<br/>Формула вычисления максимального размера окна**|
|Отключено|Нет|Нет|Размер окна|
|С ограниченным доступом|4|2 ^ 4|Размер окна * (2 ^ 4)|
|С высоким уровнем ограничений|2|2 ^ 2|Размер окна * (2 ^ 2)|
|В обычном режиме|8|2 ^ 8|Размер окна * (2 ^ 8)|
|Экспериментальная возможность|14|2 ^ 14|Размер окна * (2 ^ 14)|

Эти параметры, скорее всего, влияют на производительность TCP, но следует помнить, что многие другие факторы в Интернете, за пределами управления Azure, также могут влиять на производительность TCP.

#### <a name="increase-mtu-size"></a>Увеличить размер MTU

Так как больший размер MTU означает больший размер MSS, может возникнуть вопрос, может ли увеличение значения MTU увеличить производительность TCP. Скорее всего, нет. Размер пакета по сравнению с TCP-трафиком имеет свои достоинства и недостатки. Как обсуждалось ранее, наиболее важными факторами, влияющими на производительность пропускной способности TCP, являются размер окна TCP, потеря пакетов и RTT.

> [!IMPORTANT]
> Мы не рекомендуем, чтобы клиенты Azure изменили значение MTU по умолчанию на виртуальных машинах.
>
>

### <a name="accelerated-networking-and-receive-side-scaling"></a>Ускоренная работа в сети и масштабирование на стороне приема

#### <a name="accelerated-networking"></a>Ускорение работы в сети

Сетевые функции виртуальных машин исторически использовали ЦП как на гостевой виртуальной машине, так и на гипервизоре или узле. Каждый пакет, транзитный через узел, обрабатывается программным обеспечением ЦП узла, включая все инкапсуляцию и декапсуляция виртуальной сети. Так что чем больше трафик проходит через узел, тем выше нагрузка на ЦП. И если ЦП узла занят другими операциями, это также повлияет на пропускную способность и задержку сети. Azure решает эту задачу с помощью ускорения работы в сети.

Ускоренная сеть обеспечивает постоянную ултраловную задержку сети с помощью встроенного программируемого оборудования Azure и технологий, таких как SR-IOV. Ускоренная сеть перемещает значительную часть программно-определяемой сети Azure из стека на процессоры и в Смартникс на основе FPGA. Это изменение позволяет приложениям конечных пользователей освобождать циклы вычислений, что снижает нагрузку на виртуальную машину, уменьшая устойчивость и несогласованность в задержках. Иными словами, производительность может быть более детерминированной.

Ускоренная сеть повышает производительность, позволяя гостевой виртуальной машине обходить узел и устанавливать путь к данным непосредственно с Смартник узла. Ниже приведены некоторые преимущества ускорения работы в сети.

- **Меньше задержек и более высоких пакетов в секунду (PPS)** : Удаление виртуального коммутатора из пути к данных позволяет устранить время обработки политики на узле и увеличить количество пакетов, которые могут быть обработаны на виртуальной машине.

- **Снижение колебаний**: Обработка виртуального коммутатора зависит от объема политики, которую необходимо применить, и рабочей нагрузки процессора, выполняющего обработку. Разгрузка применения политики к оборудованию устраняет эту вариативность за счет доставки пакетов непосредственно на виртуальную машину, устраняя связь между узлами и виртуальными машинами и все программные прерывания и переключения контекста.

- **Снижение загрузки ЦП**: Обход виртуального коммутатора на узле приводит к меньшему использованию ЦП для обработки сетевого трафика.

Чтобы использовать функцию ускорения сети, необходимо явно включить ее на каждой применимой виртуальной машине. Инструкции см. в статье [Создание виртуальной машины Linux с ускорением](https://docs.microsoft.com/azure/virtual-network/create-vm-accelerated-networking-cli) работы в сети.

#### <a name="receive-side-scaling"></a>Масштабирование на стороне приема

Масштабирование на стороне приема (RSS) — это технология сетевого драйвера, которая более эффективно распределяет получение сетевого трафика за счет распределения обработки приема между несколькими процессорами в многопроцессорной системе. В простых терминах RSS позволяет системе обрабатывать более принятый трафик, так как использует все доступные процессоры, а не только один. Более техническое обсуждение RSS см. в статье [Общие сведения о масштабировании на стороне приема](https://docs.microsoft.com/windows-hardware/drivers/network/introduction-to-receive-side-scaling).

Чтобы добиться максимальной производительности при включении ускорения работы в сети на виртуальной машине, необходимо включить RSS. Кроме того, RSS может предоставить преимущества виртуальным машинам, которые не используют ускоренную сеть. Общие сведения о том, как определить, включена ли поддержка RSS и как ее включить, см. в статье [Оптимизация пропускной способности сети для виртуальных машин Azure](https://aka.ms/FastVM).

### <a name="tcp-timewait-and-timewait-assassination"></a>TCP TIME_WAIT и TIME_WAIT ассассинатион

TCP TIME_WAIT — это еще одна Общая Настройка, влияющая на производительность сети и приложения. На занятых виртуальных машинах, открывающих и закрываемых многими сокетами, как клиентами, так и серверами (исходный IP-адрес: исходный порт + конечный IP-адрес: порт назначения) во время нормальной работы протокола TCP, данный сокет может находиться в состоянии TIME_WAIT в течение длительного времени. Состояние TIME_WAIT предназначено для того, чтобы разрешить доставку дополнительных данных на сокет перед его закрытием. Таким образом, стеки TCP/IP обычно запрещают повторное использование сокета путем автоматического удаления пакета TCP SYN клиента.

Время, в течение которого сокет находится в списке TIME_WAIT, можно настроить. Может варьироваться от 30 секунд до 240 секунд. Сокеты являются ограниченным ресурсом, а количество сокетов, которые могут использоваться в любое заданное время, можно настроить. (Как правило, число доступных сокетов равно 30 000.) Если доступные сокеты используются или если клиенты и серверы имеют несовпадающие параметры TIME_WAIT, а виртуальная машина пытается повторно использовать сокет в состоянии TIME_WAIT, новые соединения не будут работать, так как пакеты TCP SYN будут удалены без вмешательства пользователя.

Значение диапазона портов для исходящих сокетов обычно настраивается в стеке TCP/IP операционной системы. То же самое справедливо и для параметров TCP TIME_WAIT и повторного использования сокета. Изменение этих чисел может привести к повышению масштабируемости. Но в зависимости от ситуации эти изменения могут вызвать проблемы взаимодействия. При изменении этих значений следует соблюдать осторожность.

Для устранения этого ограничения масштабирования можно использовать параметр TIME_WAIT ассассинатион. TIME_WAIT ассассинатион позволяет повторно использовать сокет в определенных ситуациях, например, если порядковый номер в IP-пакете нового соединения превышает порядковый номер последнего пакета из предыдущего соединения. В этом случае операционная система разрешит установить новое подключение (будет принимать новое значение SYN/ACK) и принудительно закрыть предыдущее подключение, которое было в состоянии TIME_WAIT. Эта возможность поддерживается на виртуальных машинах Windows в Azure. Чтобы узнать о поддержке других виртуальных машин, обратитесь к поставщику ОС.

Дополнительные сведения о настройке параметров TCP TIME_WAIT и диапазона портов источника см. в разделе [Параметры, которые можно изменить для повышения производительности сети](https://docs.microsoft.com/biztalk/technical-guides/settings-that-can-be-modified-to-improve-network-performance).

## <a name="virtual-network-factors-that-can-affect-performance"></a>Факторы виртуальной сети, которые могут повлиять на производительность

### <a name="vm-maximum-outbound-throughput"></a>Максимальная исходящая пропускная способность виртуальной машины

Azure предоставляет различные размеры и типы виртуальных машин, каждый из которых имеет разную комбинацию возможностей производительности. Одной из этих возможностей является пропускная способность сети (или пропускная способность), измеряемая в мегабит в секунду (Мбит/с). Так как виртуальные машины размещаются на общем оборудовании, сетевая емкость должна быть достаточно общей для виртуальных машин, использующих одно и то же оборудование. Более крупные виртуальные машины выделяют больше пропускной способности, чем небольшие виртуальные машины.

Пропускная способность сети, выделяемая каждой виртуальной машине, определяет скорость передачи данных от виртуальной машины (исходящий трафик). Ограничение распространяется на весь сетевой трафик, покидающий виртуальную машину, независимо от его назначения. Например, если виртуальная машина имеет ограничение в 1 000 Мбит/с, это ограничение применяется, если исходящий трафик предназначен для другой виртуальной машины в той же виртуальной сети или один за пределами Azure.

Входящий трафик не измеряется и не ограничивается напрямую. Но существуют и другие факторы, такие как ограничения ресурсов ЦП и хранилища, которые могут повлиять на способность виртуальной машины обрабатывать входящие данные.

Ускоренная сеть разработана для повышения производительности сети, включая задержку, пропускную способность и загрузку ЦП. Ускоренная сеть может повысить пропускную способность виртуальной машины, но она может сделать это только до выделенной пропускной способности виртуальной машины.

К виртуальным машинам Azure подключен по крайней мере один сетевой интерфейс. У них может быть несколько. Пропускная способность, выделенная виртуальной машине, представляет собой сумму всего исходящего трафика по всем сетевым интерфейсам, подключенным к компьютеру. Иными словами, пропускная способность выделяется отдельно для каждой виртуальной машины независимо от того, сколько сетевых интерфейсов подключено к компьютеру.

Ожидаемая исходящая пропускная способность и число сетевых интерфейсов, поддерживаемых каждым размером виртуальной машины, подробно описаны в подразделении [размеров виртуальных машин Windows в Azure](https://docs.microsoft.com/azure/virtual-machines/windows/sizes?toc=%2fazure%2fvirtual-network%2ftoc.json). Чтобы увидеть максимальную пропускную способность, выберите тип, например **Общее назначение**, а затем найдите раздел, посвященный размерам на полученной странице (например, "Dv2-Series"). Для каждого ряда существует таблица, которая предоставляет сетевые спецификации в последнем столбце, озаглавленном "максимальное число сетевых адаптеров/ожидаемая пропускная способность сети (Мбит/с)".

Ограничение пропускной способности применяется ко всей виртуальной машине. На пропускную способность не влияют следующие факторы.

- **Число сетевых интерфейсов**: Ограничение пропускной способности применяется к сумме всего исходящего трафика от виртуальной машины.

- **Ускоренная сеть**: Хотя эта функция может быть полезна при достижении опубликованного ограничения, она не меняет это ограничение.

- **Назначение трафика**: Все назначения подсчитывается в сторону исходящего ограничения.

- **Протокол**: Весь исходящий трафик по всем протоколам подсчитывается в сторону ограничения.

Дополнительные сведения см. в статье пропускная [способность сети виртуальной машины](https://aka.ms/AzureBandwidth).

### <a name="internet-performance-considerations"></a>Вопросы производительности в Интернете

Как обсуждалось в этой статье, факторы в Интернете и вне контроля Azure могут повлиять на производительность сети. Ниже приведены некоторые из этих факторов.

- **Задержка**. На время приема-передачи между двумя назначениями могут возникать проблемы в промежуточных сетях, по трафику, который не принимает "кратчайший" путь расстояния и по неоптимальным путям пиринга.

- **Потеря пакетов**: Потеря пакетов может быть вызвана перегрузкой сети, проблемами физического пути и недостаточной производительностью сетевых устройств.

- **Размер MTU/фрагментация**: Фрагментация в пути может привести к задержкам при поступлении данных или в пакетах, которые поступают в неопределенном порядке, что может повлиять на доставку пакетов.

Traceroute — это хороший инструмент для измерения характеристик производительности сети (таких как потеря пакетов и задержка) по каждому сетевому пути между исходным устройством и конечным устройством.

### <a name="network-design-considerations"></a>Вопросы проектирования сети

Наряду с рассмотренными выше соображениями топология виртуальной сети может повлиять на производительность сети. Например, конструкция типа "звезда", обеспечивающая передачу трафика глобально в виртуальную сеть с одним концентратором, приведет к задержке в сети, что повлияет на общую производительность сети.

Количество сетевых устройств, через которые проходит сетевой трафик, также может повлиять на общую задержку. Например, в структуре «звезда», если трафик проходит через виртуальный сетевой модуль периферийного сервера и виртуальный модуль концентратора перед передачей в Интернет, сетевые виртуальные устройства могут ввести задержку.

### <a name="azure-regions-virtual-networks-and-latency"></a>Регионы Azure, виртуальные сети и задержка

Регионы Azure состоят из нескольких центров обработки данных, которые существуют в общей географической области. Эти центры обработки данных могут быть физически не рядом друг с другом. В некоторых случаях они разделяются примерно на 10 километров. Виртуальная сеть является логическим наложением поверх сети физического центра обработки данных Azure. Виртуальная сеть не подразумевает какую-либо конкретную топологию сети в центре обработки данных.

Например, две виртуальные машины в одной виртуальной сети и подсети могут находиться в разных стойках, строках или даже в центрах обработки данных. Они могут быть разделены с помощью волоконно оптических кабелей или километрами волоконно оптического кабеля. Этот вариант может привести к задержке переменных (разница в нескольких миллисекундах) между разными виртуальными машинами.

Географическое размещение виртуальных машин и потенциальная задержка между двумя виртуальными машинами могут зависеть от конфигурации групп доступности и Зоны доступности. Но расстояние между центрами обработки данных в регионе зависит от региона и в основном зависит от топологии центров обработки данных в регионе.

### <a name="source-nat-port-exhaustion"></a>Нехватка исходного порта NAT

Развертывание в Azure может взаимодействовать с конечными точками за пределами Azure в общедоступном Интернете и/или в пространстве общедоступных IP-адресов. Когда экземпляр инициирует исходящее подключение, Azure динамически сопоставляет частный IP-адрес с общедоступным IP-адресом. После того как Azure создаст это сопоставление, возвращаемый трафик исходящего потока может также получить доступ к частному IP-адресу, в котором исходит поток.

Для каждого исходящего подключения Azure Load Balancer необходимо поддерживать это сопоставление в течение некоторого периода времени. С учетом многоклиентской природы Azure поддержание этого сопоставления для каждого исходящего потока для каждой виртуальной машины может быть ресурсоемким. Поэтому существуют ограничения, которые задаются и основываются на конфигурации виртуальной сети Azure. Или, чтобы точнее сказать, виртуальная машина Azure может одновременно устанавливать определенное количество исходящих подключений. При достижении этих ограничений виртуальная машина не сможет создавать больше исходящих подключений.

Но это поведение можно настроить. Дополнительные сведения о нехватке портов SNAT и SNAT см. в [этой статье](https://docs.microsoft.com/azure/load-balancer/load-balancer-outbound-connections).

## <a name="measure-network-performance-on-azure"></a>Измерение производительности сети в Azure

В этой статье приводится ряд максимальных показателей производительности, связанных с задержкой сети и временем приема-передачи (RTT) между двумя виртуальными машинами. В этом разделе приводятся некоторые рекомендации по тестированию задержки и RTT, а также проверке производительности TCP и производительности сети виртуальных машин. Вы можете настроить и протестировать производительность по TCP/IP и сетевым значениям, описанным выше, с помощью методик, описанных в этом разделе. Вы можете подключать значения задержки, MTU, MSS и размера окна к представленным выше вычислениям и сравнить теоретическое максимальное значение с фактическими значениями, наблюдаемыми во время тестирования.

### <a name="measure-round-trip-time-and-packet-loss"></a>Измерение времени кругового пути и потери пакетов

Производительность TCP в значительной степени зависит от RTT и потери пакетов. Служебная программа PING, доступная в Windows и Linux, предоставляет самый простой способ измерения времени приема на работу и потери пакетов. В выходных данных проверки связи будет показана минимальная/максимальная/средняя задержка между источником и назначением. Она также будет показывать потери пакетов. Проверка связи по умолчанию использует протокол ICMP. Для проверки RTT TCP можно использовать PsPing. Дополнительные сведения см. в разделе [PsPing](https://docs.microsoft.com/sysinternals/downloads/psping).

### <a name="measure-actual-throughput-of-a-tcp-connection"></a>Измерение фактической пропускной способности TCP-соединения

NTttcp — это средство для тестирования производительности TCP виртуальной машины Linux или Windows. Можно изменить различные параметры TCP, а затем проверить преимущества с помощью NTttcp. Для получения дополнительных сведений см. следующие ресурсы.

- [Тестирование пропускной способности и пропускной способности (NTttcp)](https://aka.ms/TestNetworkThroughput)

- [Служебная программа NTttcp](https://gallery.technet.microsoft.com/NTttcp-Version-528-Now-f8b12769)

### <a name="measure-actual-bandwidth-of-a-virtual-machine"></a>Измерение фактической пропускной способности виртуальной машины

Можно проверить производительность различных типов виртуальных машин, ускоренной сети и т. д. с помощью средства под названием iPerf. iPerf также доступен в Linux и Windows. iPerf может использовать TCP или UDP для проверки общей пропускной способности сети. на тесты пропускной способности TCP iPerf влияют факторы, описанные в этой статье (например, задержка и RTT). Поэтому UDP может дать лучшие результаты, если нужно просто протестировать максимальную пропускную способность.

Дополнительные сведения вы найдете в следующих статьях:

- [Устранение неполадок производительности сети Expressroute](https://docs.microsoft.com/azure/expressroute/expressroute-troubleshooting-network-performance)

- [Порядок проверки пропускной способности VPN для виртуальной сети](https://docs.microsoft.com/azure/vpn-gateway/vpn-gateway-validate-throughput-to-vnet)

### <a name="detect-inefficient-tcp-behaviors"></a>Обнаружение неэффективных поведений TCP

При записи пакетов клиенты Azure могут видеть TCP-пакеты с флагами TCP (SACK, DUP ACK, повторной ПЕРЕДАЧЕй и быстрой повторной ПЕРЕДАЧЕй), которые могут указывать на проблемы с производительностью сети. Эти пакеты специально указывают на неэффективность работы сети в результате потери пакетов. Но потеря пакетов не обязательно связана с проблемами производительности Azure. Проблемы с производительностью могут быть результатом проблем с приложениями, проблем с операционной системой или других проблем, которые могут быть не связаны напрямую с платформой Azure.

Кроме того, помните, что некоторые пересылки и дублирование подтверждений в сети являются нормальными. Были созданы надежные протоколы TCP. Свидетельство этих пакетов TCP в записи пакетов не обязательно указывает на системную сетевую проблему, если они не являются избыточными.

Тем не менее, эти типы пакетов указывают на то, что пропускная способность TCP не достигает максимальной производительности по причинам, изложенным в других разделах этой статьи.

## <a name="next-steps"></a>Следующие шаги

Теперь, когда вы узнали о настройке производительности TCP/IP для виртуальных машин Azure, вам может потребоваться ознакомиться с другими соображениями по [планированию виртуальных сетей](https://docs.microsoft.com/azure/virtual-network/virtual-network-vnet-plan-design-arm) или дополнительными сведениями [о подключении и настройке виртуальных сетей](https://docs.microsoft.com/azure/virtual-network/).
