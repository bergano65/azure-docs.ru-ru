---
title: Интеграция Azure Data Explorer с фабрикой данных Azure
description: В этой теме интегрируйте Explorer данных Azure с фабрикой данных Azure, чтобы использовать действия копирования, поиска и команд
services: data-explorer
author: orspod
ms.author: orspodek
ms.reviewer: tomersh26
ms.service: data-explorer
ms.topic: conceptual
ms.date: 01/20/2020
ms.openlocfilehash: bb08cf4db45a378b35a8245eadd56a2ab3e48bab
ms.sourcegitcommit: 2ec4b3d0bad7dc0071400c2a2264399e4fe34897
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 03/27/2020
ms.locfileid: "76293629"
---
# <a name="integrate-azure-data-explorer-with-azure-data-factory"></a>Интеграция исследователя данных Azure с фабрикой данных Azure

[Azure Data Factory](/azure/data-factory/) (ADF) — это облачный сервис интеграции данных, который позволяет интегрировать различные хранилища данных и выполнять действия на данных. ADF позволяет создавать рабочие процессы на основе данных для организации и автоматизации движения данных и преобразования данных. Azure Data Explorer — один из [поддерживаемых хранилищ данных](/azure/data-factory/copy-activity-overview#supported-data-stores-and-formats) на фабрике данных Azure. 

## <a name="azure-data-factory-activities-for-azure-data-explorer"></a>Действия Azure Data Factory для исследователя данных Azure

Различные интеграции с фабрикой данных Azure доступны для пользователей Azure Data Explorer:

### <a name="copy-activity"></a>Действие копирования
 
Для передачи данных между хранилищами данных используется деятельность Azure Data Factory Copy. Azure Data Explorer поддерживается как источник, где данные копируется из Azure Data Explorer в любой поддерживаемый хранилище данных, и раковины, где данные копируется из любого поддерживаемого хранилища данных в Azure Data Explorer. Для получения дополнительной информации смотрите [данные копирования в Azure Data Explorer с помощью Azure Data Factory.](/azure/data-factory/connector-azure-data-explorer) а также для детального просмотра [данных о загрузке с фабрики данных Azure data Factory в Azure Data Explorer.](data-factory-load-data.md)
Azure Data Explorer поддерживается Azure IR (Интеграция Runtime), используется при копировании данных в Azure и самостоятельно хозяйничающей ИК, используемой при копировании данных из/в хранилищах данных, расположенных в помещениях или в сети с контролем доступа, например, виртуальной сети Azure. Для получения дополнительной [which IR to use](/azure/data-factory/concepts-integration-runtime#determining-which-ir-to-use) информации, см.
 
> [!TIP]
> При использовании действия копирования и создании **службы связи** или **набора данных**выберите хранилище данных **Azure Data Explorer (Kusto),** а не старый хранилище данных **Kusto.**  

### <a name="lookup-activity"></a>Активность поиска
 
Активность поиска используется для выполнения запросов в Azure Data Explorer. Результат запроса будет возвращен в виде вывода действия Lookup и может быть использован в следующем действии в конвейере, как описано в [документации ADF Lookup.](/azure/data-factory/control-flow-lookup-activity#use-the-lookup-activity-result-in-a-subsequent-activity)  
В дополнение к лимиту размера ответа в 5000 строк и 2 МБ, действие также имеет ограничение времени запроса 1 час.

### <a name="command-activity"></a>Командная деятельность

Действия командования позволяют выполнять команды [управления](/azure/kusto/concepts/#control-commands)Azure Data Explorer. В отличие от запросов, команды управления потенциально могут изменять данные или метаданные. Некоторые команды управления предназначены для поимки данных в Azure `.ingest`Data `.set-or-append`Explorer, используя такие команды, как или ) или `.export`копируя данные из Azure Data Explorer в внешние хранилища данных с помощью таких команд, как .
Для детального прохождения командной деятельности [см.](data-factory-command-activity.md)  Использование команды управления для копирования данных иногда может быть более быстрым и дешевым вариантом, чем действие Copy. Чтобы определить, когда использовать действие «Командование» по сравнению с действием Copy, при [копировании данных выберите между действиями Copy и Command.](#select-between-copy-and-azure-data-explorer-command-activities-when-copy-data)

### <a name="copy-in-bulk-from-a-database-template"></a>Копирование оптом из шаблона базы данных

[Копия оптом из базы данных в Azure Data Explorer с помощью шаблона Azure Data Factory](data-factory-template.md) является предопределенным конвейером Azure Data Factory. Шаблон используется для создания большого количества конвейеров в базе данных или в таблице для более быстрого копирования данных. 

### <a name="mapping-data-flows"></a>Сопоставление потоков данных 

[Картирование потоков данных Azure Data Factory](/azure/data-factory/concepts-data-flow-overview) – это визуально разработанные преобразования данных, которые позволяют инженерам по обработке графических преобразований данных без написания кода. Для создания потока данных и вхобищных данных в Azure Data Explorer используйте следующий метод:

1. Создайте [поток картографических данных.](/azure/data-factory/data-flow-create)
1. [Экспорт данных в Azure Blob](/azure/data-factory/data-flow-sink). 
1. Определите активность копирования [Event Grid](/azure/data-explorer/ingest-data-event-grid) или [ADF](/azure/data-explorer/data-factory-load-data) для поимки данных в Azure Data Explorer.

## <a name="select-between-copy-and-azure-data-explorer-command-activities-when-copy-data"></a>Выберите между действиями команды copy и Azure Data Explorer 

Этот раздел поможет вам выбрать правильную деятельность для ваших потребностей в копировании данных.

При копировании данных из Azure Data Explorer в Azure Data Factory есть два варианта:
* Копирование деятельности.
* Команда Команды Azure Data Explorer, выполняющая одну из команд управления, передающая данные в Azure Data Explorer. 

### <a name="copy-data-from-azure-data-explorer"></a>Копирование данных из Azure Data Explorer
  
Вы можете скопировать данные из Azure [`.export`](/azure/kusto/management/data-export/) Data Explorer с помощью действия копирования или команды. Команда `.export` выполняет запрос, а затем экспортирует результаты запроса. 

Ниже приведена таблица для сравнения `.export` действия Copy и команды для копирования данных из Azure Data Explorer.

| | Действие копирования | .экспортная команда |
|---|---|---|
| **Описание потока** | ADF выполняет запрос на Kusto, обрабатывает результат и отправляет его в хранилище целевых данных. <br>(**ADX > ADF > хранилище данных раковины)** | ADF отправляет `.export` команду управления в Azure Data Explorer, который выполняет команду, и отправляет данные непосредственно в хранилище целевых данных. <br>(**ADX > хранилище данных раковины**) |
| **Поддерживаемые целевые хранилища данных** | Широкий спектр [поддерживаемых хранилив данных](/azure/data-factory/copy-activity-overview#supported-data-stores-and-formats) | ADLSv2, Azure Blob, База данных S'L |
| **Производительность** | Централизованное | <ul><li>Распределенные (по умолчанию), экспорт данных из нескольких узлов одновременно</li><li>Быстрее и COGS эффективным.</li></ul> |
| **Пределы серверов** | Ограничения запросов могут быть [расширены/отключены.](/azure/kusto/concepts/querylimits) По умолчанию запросы ADF содержат: <ul><li>Предельный размер 500 000 записей или 64 МБ.</li><li>Ограничение по времени 10 минут.</li><li>`noTruncation`установлен на ложные.</li></ul> | По умолчанию расширяет или отсвагает лимиты запроса: <ul><li>Ограничения по размеру отключены.</li><li>Время тайм-аута сервера продлевается до 1 часа.</li><li>`MaxMemoryConsumptionPerIterator`и `MaxMemoryConsumptionPerQueryPerNode` распространяются до макс (5 ГБ, TotalPhysicalMemory/2).</li></ul>

> [!TIP]
> Если пункт назначения копий является одним `.export` из хранилив данных, поддерживаемых командой, и `.export` если ни одна из функций активности Copy не имеет решающего значение для ваших потребностей, выберите команду.

### <a name="copying-data-to-azure-data-explorer"></a>Копирование данных в Azure Data Explorer

Вы можете скопировать данные в Azure Data Explorer с помощью действий копирования `.set` `.replace)`или приема команд, таких как [глотание от запроса](/azure/kusto/management/data-ingestion/ingest-from-query) (,`.set-or-append` `.set-or-replace`, , и [глотать из хранилища](/azure/kusto/management/data-ingestion/ingest-from-storage) (`.ingest`). 

Ниже приведена таблица для сравнения действия Copy и команд по проглатыванию данных для копирования данных в Azure Data Explorer.

| | Действие копирования | Прием из запроса<br> `.set-or-append` / `.set-or-replace` / `.set` / `.replace` | Прием из хранилища <br> `.ingest` |
|---|---|---|---|
| **Описание потока** | ADF получает данные из хранилища исходных данных, преобразует их в табликовый формат и выполняет необходимые изменения в схеме. Затем ADF загружает данные в капли Azure, разбивает их на куски, а затем загружает капли, чтобы глотать их в таблицу ADX. <br> (**Хранилище исходных данных > ADF > Azure капли > ADX**) | Эти команды могут выполнять запрос `.show` или команду, а также глотать результаты запроса в таблицу **(ADX > ADX).** | Эта команда попадает в таблицу, "вытягивая" данные из одного или нескольких артефактов облачного хранилища. |
| **Поддерживаемые хранилища исходных данных** |  [разнообразие опций](/azure/data-factory/copy-activity-overview#supported-data-stores-and-formats) | ADLS Gen 2, Azure Blob, S'L (с использованием sql_request плагина), Cosmos (с использованием cosmosdb_sql_request плагина) и любой другой хранилище данных, предоставляющий APIs HTTP или Python. | Filesystem, Хранилище Azure Blob, ADLS Gen 1, ADLS Gen 2 |
| **Производительность** | Ingestions стоят в очереди и управляются, что обеспечивает малогабаритные проемы и обеспечивает высокую доступность, обеспечивая балансировку нагрузки, повторы и обработку ошибок. | <ul><li>Эти команды не были предназначены для импорта данных с большим объемом.</li><li>Работает как ожидалось и дешевле. Но для сценариев производства и когда скорость трафика и размеры данных велики, используйте активность Copy.</li></ul> |
| **Пределы серверов** | <ul><li>Нет ограничения по размеру.</li><li>Максимальный тайм-аут: 1 час на просаженный капля. |<ul><li>В части запроса есть только ограничение по размеру, которое `noTruncation=true`можно пропустить, указав.</li><li>Максимальный тайм-аут: 1 час.</li></ul> | <ul><li>Нет ограничения по размеру.</li><li>Максимальный тайм-аут: 1 час.</li></ul>|

> [!TIP]
> * При копировании данных от ADF до `ingest from query` Azure Data Explorer используйте команды.  
> * Для больших наборов данных (>1 ГБ) используйте действие Copy.  

## <a name="required-permissions"></a>Необходимые разрешения

В следующей таблице перечислены необходимые разрешения для различных этапов интеграции с Фабрикой данных Azure.

| Шаг | Операция | Минимальный уровень разрешений | Примечания |
|---|---|---|---|
| **Создание службы связи** | Навигация по базам данных | *база данных просмотра* <br>Пользователь, ввоженный в систему, использующий ADF, должен быть уполномочен читать метаданные базы данных. | Пользователь может предоставить имя базы данных вручную. |
| | Проверить подключение | *монитор базы данных* или *настольный ингатор* <br>Директор службы должен быть `.show` уполномочен выполнять команды уровня базы данных или проглатку уровня таблицы. | <ul><li>TestConnection проверяет подключение к кластеру, а не к базе данных. Он может быть успешным, даже если база данных не существует.</li><li>Разрешения на админ таблицы недостаточны.</li></ul>|
| **Создание набора данных** | Навигация таблицы | *монитор базы данных* <br>Зарегистрированный в пользователясне с помощью ADF, должен быть уполномочен выполнять команды уровня `.show` базы данных. | Пользователь может предоставить имя таблицы вручную.|
| **Создание набора данных** или **копирование действий** | Предварительные данные | *база данных просмотра* <br>Директор службы должен быть уполномочен читать метаданные базы данных. | | 
|   | Схема импорта | *база данных просмотра* <br>Директор службы должен быть уполномочен читать метаданные базы данных. | Когда ADX является источником табулярной копии, ADF импортирует схему автоматически, даже если пользователь не импортировал схему явно. |
| **ADX — раковина** | Создание отображения столбца по имени | *монитор базы данных* <br>Директор службы должен быть `.show` уполномочен выполнять команды уровня базы данных. | <ul><li>Все обязательные операции будут работать с *настольным ingestor.*</li><li> Некоторые дополнительные операции могут выполнить неудачу.</li></ul> |
|   | <ul><li>Создание отображения CSV на столе</li><li>Бросьте отображение</li></ul>| *настольный ингатор* или *админ базы данных* <br>Директор службы должен быть уполномочен вносить изменения в таблицу. | |
|   | Прием данных | *настольный ингатор* или *админ базы данных* <br>Директор службы должен быть уполномочен вносить изменения в таблицу. | | 
| **ADX как источник** | Выполнение запроса | *база данных просмотра* <br>Директор службы должен быть уполномочен читать метаданные базы данных. | |
| **Команда Кусто** | | В соответствии с уровнем разрешений каждой команды. |

## <a name="performance"></a>Производительность 

Если Источником является Azure Data Explorer и вы используете действия Lookup, copy или command, содержащие запрос, в котором ссылаются на [рекомендации по](/azure/kusto/query/best-practices) запросу для информации о производительности и [документацию ADF для копирования.](/azure/data-factory/copy-activity-performance)
  
В этом разделе рассматривается использование копирования деятельности, где Azure Data Explorer является раковиной. Расчетная пропускная стоимость раковины Azure Data Explorer составляет 11-13 Мбит/с. В следующей таблице подробно описаны параметры, влияющие на производительность раковины Azure Data Explorer.

| Параметр | Примечания |
|---|---|
| **Компоненты географической близости** | Поместите все компоненты в одном регионе:<ul><li>хранилища данных источника и раковины.</li><li>Время выполнения интеграции ADF.</li><li>Ваш кластер ADX.</li></ul>Убедитесь, что время выполнения интеграции находится в том же регионе, что и кластер ADX. |
| **Количество двуза** | 1 ВМ на каждые 4 DIUs, используемых ADF. <br>Увеличение DIUs поможет только в том случае, если ваш источник является файловым хранилищем с несколькими файлами. Каждый VM будет обрабатывать другой файл параллельно. Таким образом, копирование одного большого файла будет иметь более высокую задержку, чем копирование нескольких меньших файлов.|
|**Сумма и SKU вашего кластера ADX** | Большое количество узлов ADX увеличит время обработки приема.|
| **Параллелизма** | Чтобы скопировать очень большой объем данных из базы данных, разделить данные, а затем использовать цикл ForEach, который копирует каждый раздел параллельно или использовать [массовую копию из базы данных в шаблон Azure Data Explorer.](data-factory-template.md) Примечание:**Степень паралдарализма** **Settings** > в деятельности Copy не имеет отношения к ADX. |
| **Сложность обработки данных** | Задержка варьируется в зависимости от формата исходного файла, отображения столбцов и сжатия.|
| **VM работает ваше время выполнения интеграции** | <ul><li>Для копии Azure нельзя менять ADF VMs и см. SkUS.</li><li> Для копирования on-prem to Azure определите, что VM, хозяйничающая ваша самостоятельно размещенная ИК, достаточно сильна.</li></ul>|

## <a name="tips-and-common-pitfalls"></a>Советы и общие подводные камни

### <a name="monitor-activity-progress"></a>Мониторинг хода деятельности

* При мониторинге хода действия письменное свойство *данных* может быть намного больше, чем свойство *чтения данных,* поскольку *чтение данных* рассчитывается в соответствии с размером двоичного файла, в то время как *записанные данные* рассчитываются в соответствии с размером памяти, после того, как данные десериализируются и разгоняются.

* При мониторинге хода действия можно увидеть, что данные записываются в раковину Azure Data Explorer. При запросе в таблице Azure Data Explorer вы видите, что данные не поступили. Это связано с двумя этапами копирования в Azure Data Explorer. 
    * Первый этап считывает исходные данные, разбивает их на 900 МБ и загружает каждый кусок в Azure Blob. На первом этапе видно, как представлен ход деятельности ADF. 
    * Второй этап начинается после загрузки всех данных в Azure Blobs. Узлы движка Azure Data Explorer загружают капли и загружают данные в таблицу раковины. Затем данные отосланы в таблице Azure Data Explorer.

### <a name="failure-to-ingest-csv-files-due-to-improper-escaping"></a>Неспособность глоттить CSV файлы из-за неправильного побега

Azure Data Explorer ожидает, что CSV-файлы будут соответствовать [RFC 4180.](https://www.ietf.org/rfc/rfc4180.txt)
Он ожидает:
* Поля, содержащие символы, требующие побега (например, «и **"** новые линии»), должны начинаться и заканчиваться «символом», без белого пространства. Все **"символы** *внутри* поля убегали, используя двойной **"** характер **(").** Например, _"Hello, "World" -_ это действительный файл CSV с одной записью, имеющей одну колонку или поле с содержанием _Hello, "World"._
* Все записи в файле должны иметь одинаковое количество столбцов и полей.

Фабрика данных Azure позволяет символу backslash (escape). При генерации файла CSV с символом backslash с помощью Azure Data Factory, использование файла в Azure Data Explorer завершится неудачей.

#### <a name="example"></a>Пример

Следующие значения текста: Здравствуйте, "Мир"<br/>
ABC DEF<br/>
"АВСЗД"EF<br/>
"АВК ДЕФ<br/>

Должен отображаться в надлежащем файле CSV следующим образом: "Здравствуйте,"Мир""<br/>
"ABC DEF"<br/>
"""ABC DEF"<br/>
"""АВСК"D"""EF"<br/>

Используя символ побега по умолчанию (backslash), следующий CSV не будет \"работать\"с Azure Data Explorer: "Здравствуйте, Мир"<br/>
"ABC DEF"<br/>
"ABC\"DEF"<br/>
"\"ABC-D\"EF"<br/>

### <a name="nested-json-objects"></a>Вложенные объекты JSON

При копировании файла JSON в Azure Data Explorer обратите внимание, что:
* Массивы не поддерживаются.
* Если структура JSON содержит типы данных объектов, Azure Data Factory разглагольется над элементами ребенка объекта и попытается сопоставить каждый элемент ребенка с другим столбцом в таблице Azure Data Explorer. Если вы хотите, чтобы весь элемент объекта был отображены в одном столбце в Azure Data Explorer:
    * Проемните всю строку JSON в одну динамическую колонку в Azure Data Explorer.
    * Вручную отредумите определение конвейера с помощью редактора JSON фабрики данных Azure Data Factory. В **картографиях**
       * Удалите несколько отображений, которые были созданы для каждого элемента ребенка, и добавьте одно отображение, которое отображает тип объекта в столбце таблицы.
       * После закрытия квадратной скобки добавьте запятую, за которой следуют:<br/>
       `"mapComplexValuesToString": true`.

### <a name="specify-additionalproperties-when-copying-to-azure-data-explorer"></a>Указать дополнительные свойства при копировании в Azure Data Explorer

> [!NOTE]
> Эта функция в настоящее время доступна путем ручного редактирования полезной нагрузки JSON. 

Добавьте одну строку под разделом «потопить» действия копирования следующим образом:

```json
"sink": {
    "type": "AzureDataExplorerSink",
    "additionalProperties": "{\"tags\":\"[\\\"drop-by:account_FiscalYearID_2020\\\"]\"}"
},
```

Избежать значения может быть сложно. Используйте следующий фрагмент кода в качестве ссылки:

```csharp
static void Main(string[] args)
{
       Dictionary<string, string> additionalProperties = new Dictionary<string, string>();
       additionalProperties.Add("ignoreFirstRecord", "false");
       additionalProperties.Add("csvMappingReference", "Table1_mapping_1");
       IEnumerable<string> ingestIfNotExists = new List<string> { "Part0001" };
       additionalProperties.Add("ingestIfNotExists", JsonConvert.SerializeObject(ingestIfNotExists));
       IEnumerable<string> tags = new List<string> { "ingest-by:Part0001", "ingest-by:IngestedByTest" };
       additionalProperties.Add("tags", JsonConvert.SerializeObject(tags));
       var additionalPropertiesForPayload = JsonConvert.SerializeObject(additionalProperties);
       Console.WriteLine(additionalPropertiesForPayload);
       Console.ReadLine();
}
```

Печатное значение:

```json
{"ignoreFirstRecord":"false","csvMappingReference":"Table1_mapping_1","ingestIfNotExists":"[\"Part0001\"]","tags":"[\"ingest-by:Part0001\",\"ingest-by:IngestedByTest\"]"}
```

## <a name="next-steps"></a>Дальнейшие действия

* Узнайте, как [скопировать данные в Azure Data Explorer с помощью Azure Data Factory.](data-factory-load-data.md)
* Узнайте об использовании [шаблона Azure Data Factory для массовой копии из базы данных в Azure Data Explorer.](data-factory-template.md)
* Узнайте об использовании [командной деятельности Azure Data Factory для выполнения команд управления Azure Data Explorer.](data-factory-command-activity.md)
* Узнайте о [запросах Azure Data Explorer](/azure/data-explorer/web-query-data) для запроса данных.



