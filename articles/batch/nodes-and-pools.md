---
title: Узлы и пулы в пакетной службе Azure
description: Сведения о вычислительных узлах и пулах, а также о том, как они используются в рабочем процессе пакетной службы Azure с точки зрения разработки.
ms.topic: conceptual
ms.date: 06/16/2020
ms.openlocfilehash: f71be75c0358dbc7f76a61680df2c54f44bc4173
ms.sourcegitcommit: 845a55e6c391c79d2c1585ac1625ea7dc953ea89
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 07/05/2020
ms.locfileid: "85964048"
---
# <a name="nodes-and-pools-in-azure-batch"></a>Узлы и пулы в пакетной службе Azure

В рабочем процессе пакетной службы Azure *вычислительным узлом* (или просто *узлом*) называется виртуальная машина, которая обрабатывает часть рабочей нагрузки приложения. *Пул* представляет собой коллекцию таких узлов, на которых выполняется приложение. В этой статье собраны сведения об узлах и пулах, а также рекомендации по их созданию и использованию в рабочем процессе пакетной службы Azure.

## <a name="nodes"></a>Узлы

Узел — это виртуальная машина Azure или облачной службы, назначенная для обработки определенной рабочей нагрузки вашего приложения. Размер узла определяет количество ядер ЦП, объем памяти и размер локальной файловой системы, которые выделяются узлу.

Из узлов Windows или Linux можно создавать пулы с помощью Облачных служб Azure, образов из [Azure Marketplace для виртуальных машин](https://azuremarketplace.microsoft.com/en-us/marketplace/apps/category/compute?filters=virtual-machine-images&page=1) или самостоятельно подготовленных пользовательских образов.

На узлах может выполняться любой исполняемый файл или скрипт, поддерживаемый его операционной системой: файлы \*.exe, \*.cmd, \*.bat и скрипты PowerShell (для Windows), а также двоичные файлы, скрипты оболочки и скрипты Python (для Linux).

Для всех вычислительных узлов в пакетной службе характерно следующее:

- Стандартная [структура папок](files-and-directories.md) и связанные [переменные среды](jobs-and-tasks.md), на которые могут ссылаться задачи.
- **брандмауэра**, настроенные для управления доступом.
- [Удаленный доступ](error-handling.md#connect-to-compute-nodes) к узлам Windows (по протоколу RDP) и Linux (по протоколу SSH).

По умолчанию узлы могут взаимодействовать друг с другом, но не могут взаимодействовать с виртуальными машинами, которые не являются частью одного пула. Чтобы обеспечить безопасную связь между узлами с другими виртуальными машинами или с локальной сетью, пул можно подготавливать [в подсети виртуальной сети Azure (VNet)](batch-virtual-network.md). При этом доступ к узлам можно получить через общедоступные IP-адреса. Эти общедоступные IP-адреса создаются пакетной службой и могут быть изменены в течение всего времени существования пула. Можно также [создать пул со статическими общедоступными IP-адресами](create-pool-public-ip.md) , которыми вы управляете, что гарантирует их непредвиденное изменение.

## <a name="pools"></a>Пулы

Пул — это коллекция узлов, на которых выполняется приложение.

Пулы пакетной службы Azure основаны на базовой вычислительной платформе Azure. Они удобны для крупномасштабных операций выделения, установки приложений, распространения данных и мониторинга работоспособности. Кроме того, пулы позволяют выполнять [масштабирование](#automatic-scaling-policy), то есть гибкое изменение числа вычислительных узлов в пуле.

Каждому узлу, который добавляется в пул, присваивается уникальное имя и IP-адрес. При удалении узла из пула будут потеряны любые изменения, внесенные в операционную систему или файлы. Имя и IP-адрес удаленного узла освобождаются для использования в других целях. Когда узел покидает пул, он перестает существовать.

Пул может использоваться только той учетной записью пакетной службы, в которой он был создан. Учетная запись пакетной службы может создать несколько пулов, чтобы обеспечить достаточный объем ресурсов для выполнения приложений.

Пул может быть создан вручную или автоматически (пакетной службой), когда вы назначаете работу для выполнения. При создании пула можно указать следующие атрибуты:

- [название и версия операционной системы узла](#operating-system-and-version);
- [тип узла и целевое количество узлов](#node-type-and-target);
- [Размер узла](#node-size)
- [политика автоматического масштабирования](#automatic-scaling-policy);
- [Политика планирования задач.](#task-scheduling-policy)
- [состояние обмена данными](#communication-status);
- [задачи запуска](#start-tasks);
- [Пакеты приложений](#application-packages)
- [конфигурация виртуальной сети и брандмауэра](#virtual-network-vnet-and-firewall-configuration);
- [время существования](#pool-and-compute-node-lifetime).

> [!IMPORTANT]
> Для учетных записей пакетной службы установлена квота по умолчанию, которая ограничивает количество ядер в учетной записи хранения. Число ядер соответствует количеству вычислительных узлов. Дополнительные сведения о квотах по умолчанию и инструкцию по [увеличению квоты](batch-quota-limit.md#increase-a-quota) см. в статье [Квоты и ограничения пакетной службы Azure](batch-quota-limit.md). Если пул не достигает целевого количества узлов, причиной может быть основная квота.

## <a name="operating-system-and-version"></a>Название и версия операционной системы

При создании пула пакетной службы вы указываете конфигурацию виртуальной машины Azure и тип операционной системы, которую вы хотите запустить на каждом вычислительном узле в этом пуле.

## <a name="configurations"></a>Конфигурации

В пакетной службе доступны два типа конфигураций пула.

### <a name="virtual-machine-configuration"></a>Конфигурация виртуальной машины

**Конфигурация виртуальной машины** указывает, что пул состоит из виртуальных машин Azure. Эти виртуальные машины могут быть созданы из образов Windows или Linux.

Создавая пул, основанный на конфигурации виртуальных машин, необходимо указать не только размер узлов и источник образов, которые использовались для их создания, но и **ссылку на образ виртуальной машины** и **номер SKU агента узла** пакетной службы для установки на узлах. Дополнительные сведения об указании этих свойств пула см. в статье [Подготовка вычислительных узлов Linux в пулах пакетной службы Azure](batch-linux-nodes.md). При необходимости можно подключить один или несколько пустых дисков данных к виртуальным машинам в пуле, созданных на основе образов Marketplace, или включить диски данных в пользовательские образы, используемые для создания виртуальных машин. При добавлении дисков данных необходимо подключить и отформатировать эти диски на виртуальной машине, чтобы использовать их.

### <a name="cloud-services-configuration"></a>Конфигурация облачных служб

**Конфигурация облачных служб** указывает, что пул состоит из узлов Облачных служб Azure. Облачные службы предоставляют *только* вычислительные узлы Windows.

Доступные операционные системы пулов с конфигурацией облачных служб перечислены в статье [Таблица совместимости выпусков гостевых ОС Azure и пакетов SDK](../cloud-services/cloud-services-guestos-update-matrix.md). При создании пула, содержащего узлы облачных служб, необходимо указать размер узла и соответствующее *семейство ОС*, которое определяет версии .NET, устанавливаемые вместе с операционной системой. Облачные службы развертываются в Azure быстрее, чем виртуальные машины под управлением Windows. Если вам нужны пулы вычислительных узлов Windows, вы можете обнаружить, что облачные службы предоставляют преимущества производительности с точки зрения времени развертывания.

Вы можете выбрать для узлов *версию ОС*, так же как и для рабочих ролей в облачных службах (см. дополнительные сведения об [облачных службах](../cloud-services/cloud-services-choose-me.md)). Мы рекомендуем указывать в качестве *версии ОС* значение `Latest (*)`, тогда узлы будут обновляться автоматически, и вам не потребуются дополнительные действия при выходе новых версий. Выбор конкретной версии ОС обычно нужен только для гарантии совместимости приложений. Это позволит протестировать обратную совместимость перед установкой обновлений. После успешной проверки вы можете обновить *версию ОС* для пула и установить новый образ ОС. Все выполняемые задачи будут прерваны и повторно поставлены в очередь.

### <a name="node-agent-skus"></a>Номера SKU агентов узлов

При создании пула вам необходимо выбрать соответствующий **nodeAgentSkuId** в зависимости от ОС базового образа вашего VHD. Вы можете получить сопоставление идентификаторов SKU доступных агентов узлов со ссылками на образы на ОС, вызвав операцию [List Supported Node Agent SKUs](/rest/api/batchservice/list-supported-node-agent-skus) (Вывод списка поддерживаемых SKU агентов узлов).

### <a name="custom-images-for-virtual-machine-pools"></a>Пользовательские образы для пулов виртуальных машин

Узнайте, [как использовать Общую коллекцию образов для создания пользовательского пула](batch-sig-images.md).

Кроме того, можно создать пользовательский пул виртуальных машин на основе ресурса [управляемого образа](batch-custom-images.md). Сведения о подготовке пользовательских образов под управлением Linux на основе виртуальных машин Azure см. в статье [Создание образа виртуальной машины или виртуального жесткого диска](../virtual-machines/linux/capture-image.md). Сведения о подготовке пользовательских образов под управлением Windows на основе виртуальных машин Azure см. в статье [Создание управляемого образа универсальной виртуальной машины в Azure](../virtual-machines/windows/capture-image-resource.md).

### <a name="container-support-in-virtual-machine-pools"></a>Поддержка контейнера в пулах виртуальных машин

При создании пула конфигурации виртуальной машины с помощью API-интерфейсов пакетной службы можно настроить пул для выполнения задач в контейнерах Docker. Сейчас пул нужно создавать, используя образ, который поддерживает контейнеры Docker. Используйте Windows Server 2016 Datacenter с образами контейнеров из Azure Marketplace или укажите пользовательский образ виртуальной машины, включающий Docker Community или Enterprise Edition и все необходимые драйверы. Параметры пула должны включать [конфигурацию контейнера](/rest/api/batchservice/pool/add), которая копирует образы контейнеров на виртуальные машины при создании пула. Задачи, выполняющиеся в пуле, могут ссылаться на образы контейнеров и параметры выполнения контейнера.

См. дополнительные сведения о [выполнении контейнерных приложений Docker в пакетной службе Azure](batch-docker-container-workloads.md).

## <a name="node-type-and-target"></a>Тип узла и целевое количество узлов

При создании пула можно указать нужные типы узлов и целевое количество узлов каждого типа. Узлы бывают двух типов:

- **Выделенные узлы.** Выделенные вычислительные узлы зарезервированы для рабочих нагрузок. Они более затратные, чем низкоприоритетные узлы, но они никогда не замещаются.
- **Низкоприоритетные узлы.** Низкоприоритетные узлы используют избыточные ресурсы в Azure для выполнения рабочих нагрузок пакетной службы. Час работы низкоприоритетных узлов дешевле, чем у выделенных узлов, и они позволяют выполнять рабочие нагрузки, требующие значительной вычислительной мощности. Дополнительные сведения см. в статье [Использование низкоприоритетных виртуальных машин в пакетной службе (предварительная версия)](batch-low-pri-vms.md).

При недостаточном количестве избыточных ресурсов в Azure низкоприоритетные узлы могут вытесняться. Если при выполнении задач узел замещается, задачи помещаются в очередь и перезапускаются, как только он снова станет доступным. Низкоприоритетные узлы — это оптимальный вариант для рабочих нагрузок, где время завершения задания гибкое, а работа распределяется по нескольким узлам. Прежде чем использовать низкоприоритетные узлы, убедитесь, что потерянная в случае вытеснения работа в вашем сценарии будет минимальной и легко воспроизводимой.

Низкоприоритетные и выделенные вычислительные узлы могут находиться в одном и том же пуле. Каждый тип узла имеет свой параметр целевого значения, в котором можно указать желаемое количество узлов.

Оно называется *целевым*, так как в некоторых случаях пул не может достигнуть требуемого числа узлов. Например, это может произойти из-за достижения [квоты на ядра](batch-quota-limit.md) для учетной записи пакетной службы или если примененная к пулу формула автоматического масштабирования ограничивает максимальное количество узлов.

Дополнительные сведения о ценах на низкоприоритетные и выделенные узлы см. на странице [цен на пакетную службу](https://azure.microsoft.com/pricing/details/batch/).

## <a name="node-size"></a>Размер узла

При создании пула пакетной службы Azure вы можете использовать любые семейства и размеры виртуальной машины, доступные в Azure. Azure предлагает ряд размеров виртуальных машин для разных рабочих нагрузок, включая специализированные размеры [HPC](../virtual-machines/linux/sizes-hpc.md) или размеры с поддержкой [GPU](../virtual-machines/linux/sizes-gpu.md). 

Дополнительные сведения см. в статье [Выбор размера виртуальной машины для вычислительных узлов в пуле пакетной службы Azure](batch-pool-vm-sizes.md).

## <a name="automatic-scaling-policy"></a>Политика автоматического масштабирования

Для динамических рабочих нагрузок к пулу можно применить политику автоматического масштабирования. В этом случае пакетная служба будет периодически вычислять предложенную формулу и динамически изменять количество узлов в пуле с учетом текущей рабочей нагрузки и потребления ресурсов в рамках вашего сценария вычислений. Это позволит снизить общую стоимость работы приложения за счет использования только необходимых ресурсов и освобождения остальных.

Чтобы включить автоматическое масштабирование, необходимо написать [соответствующую формулу](batch-automatic-scaling.md#automatic-scaling-formulas) и связать ее с пулом. Пакетная служба использует эту формулу для определения целевого количества узлов в пуле для следующего интервала масштабирования (интервал, который можно настроить). Вы можете указать параметры автоматического масштабирования для пула при его создании или включить масштабирование позже. Вы также можете обновить параметры масштабирования в пуле с включенным масштабированием.

Рассмотрим для примера задание, которое требует отправки большого количества задач для выполнения. Вы можете назначить для пула формулу масштабирования, которая изменяет количество узлов пула в зависимости от текущего числа задач в очереди и скорости выполнения этих задач, входящих в задание. Пакетная служба периодически вычисляет формулу и изменяет размер пула в зависимости от рабочей нагрузки и других параметров формулы. По мере необходимости служба добавляет узлы, если в очереди стоит большое количество задач, и удаляет узлы при отсутствии задач в очереди или выполняемых задач.

Формула масштабирования может использовать следующие метрики.

- **Метрики времени** — основываются на статистических данных, которые собираются каждые 5 минут за указанное число часов.
- **Метрики ресурсов** — зависят от показателей загрузки ЦП, использования пропускной способности и памяти, а также количества узлов.
- **Метрики задач** зависят от состояния задачи: *Активная* (в очереди), *Выполняется* или *Завершена*.

Если при автоматическом масштабировании уменьшается количество вычислительных узлов в пуле, необходимо учитывать способ обработки текущих выполняемых задач. Чтобы решить эту проблему, пакетная служба предоставляет [*параметр отмены выделения узла*](/rest/api/batchservice/pool/removenodes#computenodedeallocationoption), который можно добавлять в формулы. Например, можно указать, чтобы выполняемые задачи сразу останавливались, а затем помещались в очередь на выполнение на другом узле или завершались до удаления узла из пула. Обратите внимание, что установка значения `taskcompletion` или `retaineddata` для параметра отмены выделения узла не позволит изменять размер пула до тех пор, пока для всех задач не завершится выполнение или не истечет период хранения.

Подробные сведения об автоматическом масштабировании приложения см. в статье [Автоматическое масштабирование вычислительных узлов в пуле пакетной службы Azure](batch-automatic-scaling.md).

> [!TIP]
> Для максимально эффективного использования ресурсов укажите значение "ноль" для целевого количества узлов на момент завершения задания, но позвольте текущим задачам завершиться нормально.

## <a name="task-scheduling-policy"></a>Политика планирования задач

Параметр конфигурации [Максимальное число заданий на узел](batch-parallel-node-tasks.md) определяет максимальное число задач, которые могут параллельно выполняться на каждом вычислительном узле пула.

В конфигурации по умолчанию указывается выполнение только одной задачи на узле в любое время. Но в некоторых ситуациях выполнение нескольких задач на одном узле будет более правильным выбором. Сведения о преимуществах выполнения нескольких задач на узле см. в разделе [Пример сценария](batch-parallel-node-tasks.md#example-scenario) статьи [Повышение эффективности вычислительных ресурсов в пакетной службе Azure благодаря параллельному выполнению задач на узлах](batch-parallel-node-tasks.md).

Вы также можете выбрать *тип заполнения*. Пакетная служба может равномерно распределять задачи между всеми узлами в пуле или назначать каждому узлу максимально возможное число задач, прежде чем переходить к загрузке следующего узла.

## <a name="communication-status"></a>Состояние обмена данными

В большинстве случаев задачи работают независимо друг от друга и взаимодействие между ними не требуется. Но в некоторых приложениях задачи должны взаимодействовать (например, при использовании [задач с несколькими экземплярами](batch-mpi.md)).

Вы можете разрешить **обмен данными между узлами**, входящими в один пул, для взаимодействия во время выполнения. При включении обмена данными между узлами узлы в пулах с конфигурацией облачных служб могут взаимодействовать друг с другом через порты с номерами выше 1100. При этом пулы с конфигурацией виртуальной машины не ограничивают трафик через какой-либо порт.

Включение обмена данными между узлами также влияет на размещение узлов в кластерах, что в силу применяемых ограничений развертывания может снизить максимальное количество узлов в пуле. Если приложению не требуется обмен данными между узлами, пакетная служба сможет выделить для пула больше узлов из разных кластеров и центров обработки данных, что позволит увеличить производительность параллельной обработки.

## <a name="start-tasks"></a>Задачи запуска

При желании вы можете добавить [задачу запуска](jobs-and-tasks.md#start-task), которая будет выполняться на каждом узле при его присоединении к пулу, а также при каждом перезапуске узла или пересоздании образа узла. Она особенно полезна для подготовки вычислительных узлов к выполнению таких операций, как установка приложений, которые запускаются задачами на вычислительных узлах.

## <a name="application-packages"></a>Пакеты приложений

Вы можете указать пакеты приложений для развертывания на вычислительных узлах в пуле. Пакеты приложений обеспечивают упрощенное развертывание и управление версиями для приложений, запускаемых с помощью задач. Пакеты приложений, которые указываются для пула, устанавливаются на каждый вычислительный узел, который присоединяется к пулу, а также каждый раз, когда узел перезагружается или для него пересоздается образ.

Дополнительные сведения о развертывании приложений на узлах пакетной службы с помощью пакетов приложений см .в [этой статье](batch-application-packages.md).

## <a name="virtual-network-vnet-and-firewall-configuration"></a>Конфигурация виртуальной сети и брандмауэра

При подготовке пула вычислительных узлов в пакетной службе можно связать его с подсетью [виртуальной сети Azure](../virtual-network/virtual-networks-overview.md). Для использования виртуальной сети Azure API клиента пакетной службы должен использовать проверку подлинности Azure Active Directory (AD). Поддержка Azure AD пакетной службой Azure описана в статье [Аутентификация решений пакетной службы с помощью Active Directory](batch-aad-auth.md).

### <a name="vnet-requirements"></a>Требования к виртуальной сети

[!INCLUDE [batch-virtual-network-ports](../../includes/batch-virtual-network-ports.md)]

Дополнительные сведения о настройке пула пакетной службы в виртуальной сети см. в статье [Create an Azure Batch pool in a virtual network](batch-virtual-network.md) (Создание пула пакетной службы Azure в виртуальной сети).

> [!TIP]
> Чтобы гарантировать, что общедоступные IP-адреса, используемые для доступа к узлам, не изменяются, можно [создать пул с указанными общедоступными IP-адресами, которыми вы управляете](create-pool-public-ip.md).

## <a name="pool-and-compute-node-lifetime"></a>Время существования пула и вычислительного узла

При проектировании решения на базе пакетной службы Azure нужно указать, когда и как будут создаваться пулы и как долго будут доступны вычислительные узлы в этих пулах.

Одной из крайностей является создание отдельного пула для каждого отправляемого задания и его удаление сразу по завершении выполнения задач. Такой вариант позволит максимально эффективно использовать ресурсы, так как узлы выделяются в необходимом количестве и завершают работу, как только переходят в состояние простоя. В этом случае заданию придется ожидать выделения узлов, но зато планирование задач для выполнения происходит по мере выделения каждого отдельного узла, сразу после выполнения на нем задачи запуска. Иными словами, пакетная служба *не* дожидается, пока все узлы в пуле станут доступными, чтобы назначить задачи узлам. Это позволяет обеспечить максимально эффективное использование ресурсов.

Другим крайним вариантом является заблаговременное создание пула и подготовка его узлов до запуска заданий. Этот вариант применим для тех ситуаций, когда немедленный запуск задания имеет наивысший приоритет. В этом случае задачи будут запускаться немедленно, но при этом узлы могут некоторое время простаивать в ожидании назначения задач.

Смешанный подход обычно используется для обработки постоянной нагрузки, интенсивность которой изменяется. Вы можете создать пул для отправки нескольких заданий и изменять количество узлов в нем в зависимости от текущей нагрузки. Масштабирование можно выполнять по мере изменения интенсивности нагрузки или с упреждением, если нагрузка является прогнозируемой. Дополнительные сведения см. в разделе [Политика автоматического масштабирования](#automatic-scaling-policy).

## <a name="security-with-certificates"></a>Безопасность с использованием сертификатов

Обычно сертификаты нужны для шифрования и расшифровки конфиденциальных сведений, используемых задачами, например ключей для [учетной записи хранения Azure](accounts.md#azure-storage-accounts). Такие сертификаты можно установить на узлах. Зашифрованные данные передаются задачам через параметры командной строки или через ресурсы задачи, а установленные сертификаты позволяют расшифровать их.

Для добавления сертификата к учетной записи пакетной службы используйте операцию [добавления сертификата](/rest/api/batchservice/certificate/add) в REST API пакетной службы или метод [CertificateOperations.CreateCertificate](/dotnet/api/microsoft.azure.batch.certificateoperations) в .NET для пакетной службы. Затем можно будет связать сертификат с новым или существующим пулом.

Если к пулу привязан сертификат, пакетная служба устанавливает этот сертификат на каждом узле пула. Пакетная служба устанавливает нужные сертификаты при запуске узла до начала выполнения каких-либо задач (в том числе [задач запуска](jobs-and-tasks.md#start-task) и [задач диспетчера заданий](jobs-and-tasks.md#job-manager-task)).

При добавлении сертификатов в существующий пул необходимо перезагрузить его вычислительные узлы, чтобы применить к ним этот сертификат.

## <a name="next-steps"></a>Дальнейшие действия

- Изучите сведения [о заданиях и задачах](jobs-and-tasks.md).
