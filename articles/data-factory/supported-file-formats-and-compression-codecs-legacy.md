---
title: Поддерживаемые форматы файлов на фабрике данных Azure (наследие)
description: В этой статье описаны форматы файлов и коды сжатия, поддерживаемые соединителями на основе файлов в фабрике данных Azure.
author: linda33wj
manager: shwang
ms.reviewer: craigg
ms.service: data-factory
ms.workload: data-services
ms.topic: conceptual
ms.date: 12/10/2019
ms.author: jingwang
ms.openlocfilehash: b1f11a1ff25117c07e61475e7e83fc0c170cd552
ms.sourcegitcommit: b80aafd2c71d7366838811e92bd234ddbab507b6
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 04/16/2020
ms.locfileid: "81414657"
---
# <a name="supported-file-formats-and-compression-codecs-in-azure-data-factory-legacy"></a>Поддерживаемые форматы файлов и кодеки сжатия на фабрике данных Azure (наследие)

[!INCLUDE[appliesto-adf-asa-md](includes/appliesto-adf-asa-md.md)]

*Эта статья относится к следующим разъемым: [Amazon S3](connector-amazon-simple-storage-service.md), [Azure Blob](connector-azure-blob-storage.md), [Azure Data Lake Storage Gen1](connector-azure-data-lake-store.md), [Azure Data Lake Storage Gen2](connector-azure-data-lake-storage.md), [Azure File Storage](connector-azure-file-storage.md), File [System,](connector-file-system.md) [FTP,](connector-ftp.md) [Google Cloud Storage](connector-google-cloud-storage.md), [HDFS,](connector-hdfs.md) [HTTP](connector-http.md), и [SFTP](connector-sftp.md).*

>[!IMPORTANT]
>Data Factory представила новую модель набора данных на основе формата, см. соответствующую статью формата с подробной информацией: <br>- [Формат Avro](format-avro.md)<br>- [Двоичный формат](format-binary.md)<br>- [Формат разграниченного текста](format-delimited-text.md)<br>- [Формат JSON](format-json.md)<br>- [Формат ORC](format-orc.md)<br>- [Формат паркета](format-parquet.md)<br>Остальные конфигурации, упомянутые в этой статье, по-прежнему поддерживаются как для обратной компабитильности. Предлагается использовать новую модель в будущем. 

## <a name="text-format-legacy"></a><a name="text-format"></a>Текстовый формат (наследие)

>[!NOTE]
>Узнайте новую модель из статьи [формата Delimited.](format-delimited-text.md) Следующие конфигурации на наборе данных хранилища данных на основе файлов по-прежнему поддерживаются как для обратной компабитильности. Предлагается использовать новую модель в будущем.

Если вам нужно считать данные из текстового файла или записать в него данные, задайте для свойства `type` в разделе `format` набора данных значение **TextFormat**. В разделе `format` также можно указать следующие **необязательные** свойства. Инструкции по настройке см. в разделе [Пример TextFormat](#textformat-example).

| Свойство | Описание | Допустимые значения | Обязательно |
| --- | --- | --- | --- |
| columnDelimiter |Знак, используемый для разделения столбцов в файле. Вы можете использовать редкие непечатаемые символы, которые не содержатся в ваших данных. Например, укажите "\u0001", что соответствует символу начала заголовка (SOH). |Допускается только один знак. Значение **по умолчанию** — **запятая (',')**. <br/><br/>Чтобы использовать символ Юникода, см. соответствующие коды в статье о [символах Юникода](https://en.wikipedia.org/wiki/List_of_Unicode_characters). |нет |
| rowDelimiter |символ, используемый для разделения строк в файле. |Допускается только один знак. Значение **по умолчанию** является любым из следующих значений на следующем: **«зрён», «Зр», «Зр», «Зн»** и **«Зр»** в записи. |нет |
| escapeChar |Специальный символ, используемый для экранирования разделителя столбцов в содержимом входного файла. <br/><br/>Не следует указывать escapeChar и quoteChar для таблицы одновременно. |Допускается только один знак. Нет значения по умолчанию. <br/><br/>Пример: если у вас есть запятая (',') в качестве делимитера столбца, но вы хотите иметь символ запятой в тексте (например: "Здравствуйте, мир"), вы можете определить '$' как персонаж побега и использовать строку "Hello$, мир" в источнике. |нет |
| quoteChar |Символ, используемый для заключения строкового значения в кавычки. Разделители столбцов и строк внутри знаков кавычек будут рассматриваться как часть строкового значения. Это свойство применяется к входному и выходному наборам данных.<br/><br/>Не следует указывать escapeChar и quoteChar для таблицы одновременно. |Допускается только один знак. Нет значения по умолчанию. <br/><br/>Например, если в качестве разделителя столбцов используется запятая (,) и нужно, чтобы этот знак встречался в тексте (например, <Hello, world>), то можно в качестве знака кавычек определить двойную кавычку (") и использовать в исходном тексте строку "Hello, world". |нет |
| nullValue |один или несколько символов, используемых для представления значения NULL. |Один или несколько знаков. Значения **по умолчанию:** **"N" и "NULL"** на прочитанном и **"N"** на записи. |нет |
| encodingName |задает имя кодировки. |Допустимое имя кодировки. Ознакомьтесь с описанием свойства [Encoding.EncodingName](https://msdn.microsoft.com/library/system.text.encoding.aspx). Пример: windows-1250 или shift_jis. Значение **по умолчанию** **UTF-8**. |нет |
| firstRowAsHeader |Указывает, следует ли рассматривать первую строки в качестве заголовка. Фабрика данных считывает первую строку входного набора данных как заголовок. Фабрика данных записывает первую строку как заголовок в выходной набор данных. <br/><br/>Примеры сценариев см. в разделе [Сценарии использования `firstRowAsHeader` и `skipLineCount`](#scenarios-for-using-firstrowasheader-and-skiplinecount). |True<br/><b>False (по умолчанию)</b> |нет |
| skipLineCount |Указывает количество **непустых** строк, которые нужно пропустить при чтении данных из входных файлов. Если указаны skipLineCount и firstRowAsHeader, то сначала пропускаются строки, а затем считываются данные заголовка из входного файла. <br/><br/>Примеры сценариев см. в разделе [Сценарии использования `firstRowAsHeader` и `skipLineCount`](#scenarios-for-using-firstrowasheader-and-skiplinecount). |Целое число |нет |
| treatEmptyAsNull |Указывает, следует ли интерпретировать NULL или пустую строку как значение NULL при считывании данных из входного файла. |**Правда (по умолчанию)**<br/>False |нет |

### <a name="textformat-example"></a>Пример TextFormat

В следующем определении JSON для набора данных задаются некоторые необязательные свойства.

```json
"typeProperties":
{
    "folderPath": "mycontainer/myfolder",
    "fileName": "myblobname",
    "format":
    {
        "type": "TextFormat",
        "columnDelimiter": ",",
        "rowDelimiter": ";",
        "quoteChar": "\"",
        "NullValue": "NaN",
        "firstRowAsHeader": true,
        "skipLineCount": 0,
        "treatEmptyAsNull": true
    }
},
```

Чтобы использовать `escapeChar` вместо `quoteChar`, замените строку с `quoteChar` следующим escape-символом:

```json
"escapeChar": "$",
```

### <a name="scenarios-for-using-firstrowasheader-and-skiplinecount"></a>Сценарии использования firstRowAsHeader и skipLineCount

* Вы выполняете копирование из нефайлового источника в текстовый файл и хотите добавить строку заголовка, содержащую метаданные схемы (например, схемы SQL). В этом случае укажите `firstRowAsHeader` со значением true в выходном наборе данных.
* Вы копируете данные из текстового файла, содержащего строку заголовка, в нефайловый приемник и хотите удалить эту строку. Укажите `firstRowAsHeader` со значением true во входном наборе данных.
* Вы копируете данные из текстового файла и хотите пропустить несколько строк в начале, которые не содержат ни данных, ни заголовка. Укажите `skipLineCount`, чтобы задать число пропускаемых строк. Если остальная часть файла содержит строку заголовка, можно также указать `firstRowAsHeader`. Если указаны `skipLineCount` и `firstRowAsHeader`, сначала пропускаются строки, а затем из входного файла считываются данные заголовка.

## <a name="json-format-legacy"></a><a name="json-format"></a>Формат JSON (наследие)

>[!NOTE]
>Узнайте новую модель из статьи [формата JSON.](format-json.md) Следующие конфигурации на наборе данных хранилища данных на основе файлов по-прежнему поддерживаются как для обратной компабитильности. Предлагается использовать новую модель в будущем.

Чтобы **импортировать JSON-файл "как есть" в базу данных Azure Cosmos DB или экспортировать его из нее**, см. раздел "Документы JSON для импорта и экспорта" статьи о [перемещении данных в базу данных Azure Cosmos DB и из нее](connector-azure-cosmos-db.md).

Если требуется проанализировать JSON-файлы или записать данные в формате JSON, задайте для свойства `type` в разделе `format` значение **JsonFormat**. В разделе `format` также можно указать следующие **необязательные** свойства. Инструкции по настройке см. в разделе [Пример JsonFormat](#jsonformat-example).

| Свойство | Описание | Обязательно |
| --- | --- | --- |
| filePattern |Шаблон данных, хранящихся в каждом JSON-файле. Допустимые значения: **setOfObjects** и **arrayOfObjects**. Значение **по умолчанию** **— это setOfObjects.** Подробные сведения об этих шаблонах см. в разделе [Шаблоны файлов JSON](#json-file-patterns). |нет |
| jsonNodeReference | Для итерации и извлечения данных из объектов в поле массива с таким же шаблоном укажите путь JSON этого массива. Это свойство поддерживается только в том случае, если данные копируются **из** JSON-файлов. | нет |
| jsonPathDefinition | Выражение пути JSON для каждого столбца с его сопоставлением с настраиваемым именем столбца (начало в нижнем регистре). Это свойство поддерживается только в том случае, если данные копируются **из** JSON-файлов и данные можно извлечь из объекта или массива. <br/><br/> Для полей в области корневого объекта выражение пути должно начинаться с корня $. Для полей внутри массива, выбранных с помощью свойства `jsonNodeReference`, выражение должно начинаться с элемента массива. Инструкции по настройке см. в разделе [Пример JsonFormat](#jsonformat-example). | нет |
| encodingName |задает имя кодировки. Список допустимых имен кодировок приведен в описании свойства [Encoding.EncodingName](https://msdn.microsoft.com/library/system.text.encoding.aspx). Например: windows-1250 или shift_jis. Значение **по умолчанию:** **UTF-8**. |нет |
| nestingSeparator |Символ, используемый для разделения уровней вложенности. Значение по умолчанию — точка (.). |нет |

>[!NOTE]
>В случае перекрестного применения данных в массиве в несколько строк (в случае 1 -> образца `jsonNodeReference`2 в [примерах JsonFormat)](#jsonformat-example)можно расширить только один массив с помощью свойства.

### <a name="json-file-patterns"></a>Шаблоны файлов JSON

Действие копирования может проанализировать следующие шаблоны JSON-файлов.

- **Тип 1: setOfObjects**

    Каждый файл содержит один объект или несколько разделенных строками или объединенных объектов. Если этот параметр выбран в выходном наборе данных, то в результате копирования будет создан JSON-файл, где каждый объект будет находиться в отдельной строке (файл с разделителем-строкой).

    * **Пример единого объекта JSON**

        ```json
        {
            "time": "2015-04-29T07:12:20.9100000Z",
            "callingimsi": "466920403025604",
            "callingnum1": "678948008",
            "callingnum2": "567834760",
            "switch1": "China",
            "switch2": "Germany"
        }
        ```

    * **Пример JSON-файла с разделителем-строкой**

        ```json
        {"time":"2015-04-29T07:12:20.9100000Z","callingimsi":"466920403025604","callingnum1":"678948008","callingnum2":"567834760","switch1":"China","switch2":"Germany"}
        {"time":"2015-04-29T07:13:21.0220000Z","callingimsi":"466922202613463","callingnum1":"123436380","callingnum2":"789037573","switch1":"US","switch2":"UK"}
        {"time":"2015-04-29T07:13:21.4370000Z","callingimsi":"466923101048691","callingnum1":"678901578","callingnum2":"345626404","switch1":"Germany","switch2":"UK"}
        ```

    * **Пример объединенного JSON-файла**

        ```json
        {
            "time": "2015-04-29T07:12:20.9100000Z",
            "callingimsi": "466920403025604",
            "callingnum1": "678948008",
            "callingnum2": "567834760",
            "switch1": "China",
            "switch2": "Germany"
        }
        {
            "time": "2015-04-29T07:13:21.0220000Z",
            "callingimsi": "466922202613463",
            "callingnum1": "123436380",
            "callingnum2": "789037573",
            "switch1": "US",
            "switch2": "UK"
        }
        {
            "time": "2015-04-29T07:13:21.4370000Z",
            "callingimsi": "466923101048691",
            "callingnum1": "678901578",
            "callingnum2": "345626404",
            "switch1": "Germany",
            "switch2": "UK"
        }
        ```

- **Тип 2: arrayOfObjects**

    Каждый файл содержит массив объектов.

    ```json
    [
        {
            "time": "2015-04-29T07:12:20.9100000Z",
            "callingimsi": "466920403025604",
            "callingnum1": "678948008",
            "callingnum2": "567834760",
            "switch1": "China",
            "switch2": "Germany"
        },
        {
            "time": "2015-04-29T07:13:21.0220000Z",
            "callingimsi": "466922202613463",
            "callingnum1": "123436380",
            "callingnum2": "789037573",
            "switch1": "US",
            "switch2": "UK"
        },
        {
            "time": "2015-04-29T07:13:21.4370000Z",
            "callingimsi": "466923101048691",
            "callingnum1": "678901578",
            "callingnum2": "345626404",
            "switch1": "Germany",
            "switch2": "UK"
        }
    ]
    ```

### <a name="jsonformat-example"></a>Пример JsonFormat

**Вариант 1. Копирование данных из JSON-файлов**

**Пример 1. Извлечение данных из объекта и массива**

В этом примере предполагается, что один корневой объект JSON соответствует одной записи в таблице результатов. Если у вас есть JSON-файл со следующим содержимым:

```json
{
    "id": "ed0e4960-d9c5-11e6-85dc-d7996816aad3",
    "context": {
        "device": {
            "type": "PC"
        },
        "custom": {
            "dimensions": [
                {
                    "TargetResourceType": "Microsoft.Compute/virtualMachines"
                },
                {
                    "ResourceManagementProcessRunId": "827f8aaa-ab72-437c-ba48-d8917a7336a3"
                },
                {
                    "OccurrenceTime": "1/13/2017 11:24:37 AM"
                }
            ]
        }
    }
}
```

и вы хотите скопировать это содержимое (посредством извлечения данных из объекта и массива) в таблицу SQL Azure в следующем формате:

| ID | deviceType | targetResourceType | resourceManagementProcessRunId | occurrenceTime |
| --- | --- | --- | --- | --- |
| ed0e4960-d9c5-11e6-85dc-d7996816aad3 | PC | Microsoft.Compute/virtualMachines | 827f8aaa-ab72-437c-ba48-d8917a7336a3 | 1/13/2017 11:24:37 AM |

Набор входных данных с типом **JsonFormat** определяется следующим образом: (частичное определение только с соответствующими частями). В частности:

- Раздел `structure` определяет настраиваемые имена столбцов и соответствующие типы данных при преобразовании в табличные данные. Этот раздел является **необязательным**, если вам не нужно сопоставлять столбцы. Дополнительные сведения см. в статье о [сопоставлении столбцов исходного набора данных со столбцами целевого набора данных](copy-activity-schema-and-type-mapping.md).
- `jsonPathDefinition` указывает путь к файлу JSON для каждого столбца, который определяет, откуда следует извлекать данные. Чтобы скопировать данные из массива, с помощью `array[x].property` можно извлечь значение нужного свойства из объекта `xth` или с помощью `array[*].property` найти нужное значение в любом объекте с таким свойством.

```json
"properties": {
    "structure": [
        {
            "name": "id",
            "type": "String"
        },
        {
            "name": "deviceType",
            "type": "String"
        },
        {
            "name": "targetResourceType",
            "type": "String"
        },
        {
            "name": "resourceManagementProcessRunId",
            "type": "String"
        },
        {
            "name": "occurrenceTime",
            "type": "DateTime"
        }
    ],
    "typeProperties": {
        "folderPath": "mycontainer/myfolder",
        "format": {
            "type": "JsonFormat",
            "filePattern": "setOfObjects",
            "jsonPathDefinition": {"id": "$.id", "deviceType": "$.context.device.type", "targetResourceType": "$.context.custom.dimensions[0].TargetResourceType", "resourceManagementProcessRunId": "$.context.custom.dimensions[1].ResourceManagementProcessRunId", "occurrenceTime": " $.context.custom.dimensions[2].OccurrenceTime"}
        }
    }
}
```

**Пример 2. Применение нескольких объектов с одинаковым шаблоном из массива**

В этом примере предполагается, что один корневой объект JSON будет преобразован в несколько записей в таблице результатов. Если у вас есть JSON-файл со следующим содержимым:

```json
{
    "ordernumber": "01",
    "orderdate": "20170122",
    "orderlines": [
        {
            "prod": "p1",
            "price": 23
        },
        {
            "prod": "p2",
            "price": 13
        },
        {
            "prod": "p3",
            "price": 231
        }
    ],
    "city": [ { "sanmateo": "No 1" } ]
}
```

И если вы хотите скопировать этот файл в таблицу Azure SQL в следующем формате путем сведения данных внутри массива и перекрестного соединения с общими сведениями о корневом объекте:

| `ordernumber` | `orderdate` | `order_pd` | `order_price` | `city` |
| --- | --- | --- | --- | --- |
| 01 | 20170122 | P1 | 23 | `[{"sanmateo":"No 1"}]` |
| 01 | 20170122 | P2 | 13 | `[{"sanmateo":"No 1"}]` |
| 01 | 20170122 | P3 | 231 | `[{"sanmateo":"No 1"}]` |


Набор входных данных с типом **JsonFormat** определяется следующим образом: (частичное определение только с соответствующими частями). В частности:

- Раздел `structure` определяет настраиваемые имена столбцов и соответствующие типы данных при преобразовании в табличные данные. Этот раздел является **необязательным**, если вам не нужно сопоставлять столбцы. Дополнительные сведения см. в статье о [сопоставлении столбцов исходного набора данных со столбцами целевого набора данных](copy-activity-schema-and-type-mapping.md).
- Параметр `jsonNodeReference` обозначает итерацию и извлечение данных из объектов с одинаковым шаблоном в разделе **массива** `orderlines`.
- `jsonPathDefinition` указывает путь к файлу JSON для каждого столбца, который определяет, откуда следует извлекать данные. В этом примере `ordernumber`, `orderdate` и `city` расположены в корневом объекте. Путь JSON к нему начинается с `$.`, а `order_pd` и `order_price` определяются с помощью пути, производного от элемента массива без `$.`.

```json
"properties": {
    "structure": [
        {
            "name": "ordernumber",
            "type": "String"
        },
        {
            "name": "orderdate",
            "type": "String"
        },
        {
            "name": "order_pd",
            "type": "String"
        },
        {
            "name": "order_price",
            "type": "Int64"
        },
        {
            "name": "city",
            "type": "String"
        }
    ],
    "typeProperties": {
        "folderPath": "mycontainer/myfolder",
        "format": {
            "type": "JsonFormat",
            "filePattern": "setOfObjects",
            "jsonNodeReference": "$.orderlines",
            "jsonPathDefinition": {"ordernumber": "$.ordernumber", "orderdate": "$.orderdate", "order_pd": "prod", "order_price": "price", "city": " $.city"}
        }
    }
}
```

**Обратите внимание на следующие моменты.**

* Если параметры `structure` и `jsonPathDefinition` не определены в наборе данных фабрики данных, действие копирования обнаружит схему из первого объекта и выполнит сведение всего объекта.
* Если входной JSON-файл содержит массив, по умолчанию действие копирования преобразует все значение массива в строку. Вы можете извлечь данные из строки с помощью `jsonNodeReference` или `jsonPathDefinition`. Или можно пропустить строку, не указывая ее в `jsonPathDefinition`.
* Если на том же уровне существует повторяющиеся имена, то действие копирования выберет последнее из них.
* В именах свойств учитывается регистр. Два свойства с одинаковым именем, но в разных регистрах, рассматриваются как два отдельных свойства.

**Вариант 2. Запись данных в JSON-файл**

Если в базе данных SQL есть следующая таблица:

| ID | order_date | order_price | order_by |
| --- | --- | --- | --- |
| 1 | 20170119 | 2000 | David |
| 2 | 20170120 | 3500 | Patrick |
| 3 | 20170121 | 4000 | Jason |

и для каждой записи вы предполагаете запись в объект JSON в следующем формате:

```json
{
    "id": "1",
    "order": {
        "date": "20170119",
        "price": 2000,
        "customer": "David"
    }
}
```

Выходной набор данных с типом **JsonFormat** определяется следующим образом (частичное определение только соответствующих частей). В частности, раздел `structure` определяет настраиваемые имена свойств в конечном файле. Для определения уровня вложенности от имен будет использоваться разделитель вложенности `nestingSeparator` (по умолчанию — точка (.)). Этот раздел является **необязательным**, если вы не собираетесь изменять исходное имя свойства или вкладывать свойства.

```json
"properties": {
    "structure": [
        {
            "name": "id",
            "type": "String"
        },
        {
            "name": "order.date",
            "type": "String"
        },
        {
            "name": "order.price",
            "type": "Int64"
        },
        {
            "name": "order.customer",
            "type": "String"
        }
    ],
    "typeProperties": {
        "folderPath": "mycontainer/myfolder",
        "format": {
            "type": "JsonFormat"
        }
    }
}
```

## <a name="parquet-format-legacy"></a><a name="parquet-format"></a>Формат паркета (наследие)

>[!NOTE]
>Узнайте новую модель из статьи [формата Паркет.](format-parquet.md) Следующие конфигурации на наборе данных хранилища данных на основе файлов по-прежнему поддерживаются как для обратной компабитильности. Предлагается использовать новую модель в будущем.

Если требуется проанализировать файлы Parquet или записать данные в формате Parquet, установите для свойства `format` `type` значение **ParquetFormat**. Вам не нужно указывать какие-либо свойства в подразделе Format раздела typeProperties. Пример

```json
"format":
{
    "type": "ParquetFormat"
}
```

Обратите внимание на следующие моменты.

* Данные сложных типов (MAP, LIST) не поддерживаются.
* Пробелы в именах столбцов не поддерживаются.
* Parquet-файл имеет следующие варианты сжатия: NONE, SNAPPY, GZIP и LZO. Фабрика данных поддерживает чтение данных из файла Parquet в любом из указанных форматов сжатия за исключением LZO — для чтения данных используется кодек сжатия, указанный в метаданных. Однако при записи в Parquet-файл фабрика данных по умолчанию выбирает SNAPPY. В настоящее время изменить это поведение нельзя.

> [!IMPORTANT]
> Для копирования посредством локальной среды выполнения интеграции (IR), то есть между локальным и облачным хранилищами данных, если вы не копируете файлы Parquet **как есть**, на компьютере среды выполнения интеграции необходимо установить **64-разрядную JRE 8 (среду выполнения Java) или OpenJDK**. Подробные сведения приведены в следующем абзаце.

Для копирования, работаемого на Самостоятельно размещенном IR с сериализацией файла Parquet/deserialization, ADF находит время выполнения Java, во-первых проверяя реестр *`(SOFTWARE\JavaSoft\Java Runtime Environment\{Current Version}\JavaHome)`* для JRE, если не найден, во-вторых, проверяя переменную *`JAVA_HOME`* системы OpenJDK.

- **Для использования JRE**: 64-разрядный ИК требует 64-битного JRE. Ее можно найти [здесь](https://go.microsoft.com/fwlink/?LinkId=808605).
- **Для использования OpenJDK**: он поддерживается в среде выполнения интеграции, начиная с версии 3.13. Упакуйте jvm.dll со всеми другими необходимыми сборками OpenJDK на компьютере с локальной IR и соответственно установите системную переменную среды JAVA_HOME.

>[!TIP]
>Если вы копируете данные в формат Parquet или из формата Parquet с помощью локальной среди выполнения интеграции и возникает ошибка: "Ошибка при вызове Java, сообщение: **java.lang.OutOfMemoryError:Java heap space**", можно добавить переменную среды `_JAVA_OPTIONS` в компьютере, на котором размещена локальная среда выполнения интеграции для настройки минимального и максимального размера кучи для виртуальной машины Java, чтобы расширить возможности такой копии, а затем повторно запустить конвейер.

![Установка размера кучи виртуальной машины Java на локальной среде выполнения интеграции](./media/supported-file-formats-and-compression-codecs/set-jvm-heap-size-on-selfhosted-ir.png)

Пример: установите переменную `_JAVA_OPTIONS` со значением `-Xms256m -Xmx16g`. Флаг `Xms` указывает начальный пул выделения памяти для виртуальной машины Java (JVM), а `Xmx` указывает максимальный пул выделения памяти. Это означает, что JVM будет запущена с объемом памяти `Xms` и сможет использовать не более `Xmx` объема памяти. По умолчанию ADF использует минимум 64 МБ и максимум 1 ГБ.

### <a name="data-type-mapping-for-parquet-files"></a>Сопоставление типов данных для файлов Parquet

| Тип промежуточных данных фабрики данных | Тип-примитив Parquet | Исходный тип Parquet (десериализация) | Исходный тип Parquet (сериализация) |
|:--- |:--- |:--- |:--- |
| Логическое | Логическое | Недоступно | Недоступно |
| SByte | Int32 | Int8 | Int8 |
| Byte | Int32 | UInt8 | Int16 |
| Int16 | Int32 | Int16 | Int16 |
| UInt16 | Int32 | UInt16 | Int32 |
| Int32 | Int32 | Int32 | Int32 |
| UInt32 | Int64 | UInt32 | Int64 |
| Int64 | Int64 | Int64 | Int64 |
| UInt64 | Binary или Int64 | UInt64 | Decimal |
| Один | Float | Недоступно | Недоступно |
| Double | Double | Недоступно | Недоступно |
| Decimal | Двоичные данные | Decimal | Decimal |
| Строка | Двоичные данные | Utf8 | Utf8 |
| Дата и время | Int96 | Недоступно | Недоступно |
| TimeSpan | Int96 | Недоступно | Недоступно |
| DateTimeOffset | Int96 | Недоступно | Недоступно |
| ByteArray | Двоичные данные | Недоступно | Недоступно |
| Guid | Двоичные данные | Utf8 | Utf8 |
| CHAR | Двоичные данные | Utf8 | Utf8 |
| CharArray | Не поддерживается | Недоступно | Недоступно |

## <a name="orc-format-legacy"></a><a name="orc-format"></a>Формат ORC (наследие)

>[!NOTE]
>Узнайте новую модель из статьи [формата ORC.](format-orc.md) Следующие конфигурации на наборе данных хранилища данных на основе файлов по-прежнему поддерживаются как для обратной компабитильности. Предлагается использовать новую модель в будущем.

Если требуется проанализировать ORC-файлы или записать данные в формате ORC, установите для свойства `format` `type` значение **OrcFormat**. Вам не нужно указывать какие-либо свойства в подразделе Format раздела typeProperties. Пример

```json
"format":
{
    "type": "OrcFormat"
}
```

Обратите внимание на следующие моменты.

* Данные сложных типов (STRUCT, MAP, LIST, UNION) не поддерживаются.
* Пробелы в именах столбцов не поддерживаются.
* Для ORC-файлов используется три [параметра сжатия](https://hortonworks.com/blog/orcfile-in-hdp-2-better-compression-better-performance/): NONE, ZLIB и SNAPPY. Фабрика данных поддерживает чтение данных из ORC-файла в любом из этих форматов. Для чтения данных используется кодек сжатия из метаданных. Однако при записи в ORC-файл фабрика данных по умолчанию выбирает ZLIB. В настоящее время изменить это поведение нельзя.

> [!IMPORTANT]
> Для копирования посредством локальной среды выполнения интеграции (IR), то есть между локальным и облачным хранилищами данных, если вы не копируете файлы ORC **как есть**, на компьютере среды выполнения интеграции необходимо установить **64-разрядную JRE 8 (среда выполнения Java) или OpenJDK**. Подробные сведения приведены в следующем абзаце.

Для копирования, работаемого на самостоятельно размещенном ИК с сериализацией/десериализацией *`(SOFTWARE\JavaSoft\Java Runtime Environment\{Current Version}\JavaHome)`* файлов ORC, ADF находит время *`JAVA_HOME`* выполнения Java, сначала проверяя реестр для JRE, если не найден, во-вторых, проверяя переменную системы OpenJDK.

- **Для использования JRE**: 64-разрядный ИК требует 64-битного JRE. Ее можно найти [здесь](https://go.microsoft.com/fwlink/?LinkId=808605).
- **Для использования OpenJDK**: он поддерживается в среде выполнения интеграции, начиная с версии 3.13. Упакуйте jvm.dll со всеми другими необходимыми сборками OpenJDK на компьютере с локальной IR и соответственно установите системную переменную среды JAVA_HOME.

### <a name="data-type-mapping-for-orc-files"></a>Сопоставление типов данных для ORC-файлов

| Тип промежуточных данных фабрики данных | Типы ORC |
|:--- |:--- |
| Логическое | Логическое |
| SByte | Byte |
| Byte | Short |
| Int16 | Short |
| UInt16 | Int |
| Int32 | Int |
| UInt32 | Long |
| Int64 | Long |
| UInt64 | Строка |
| Один | Float |
| Double | Double |
| Decimal | Decimal |
| Строка | Строка |
| Дата и время | Отметка времени |
| DateTimeOffset | Отметка времени |
| TimeSpan | Отметка времени |
| ByteArray | Двоичные данные |
| Guid | Строка |
| CHAR | Char(1) |

## <a name="avro-format-legacy"></a><a name="avro-format"></a>Формат AVRO (наследие)

>[!NOTE]
>Узнайте новую модель из статьи [формата Avro.](format-avro.md) Следующие конфигурации на наборе данных хранилища данных на основе файлов по-прежнему поддерживаются как для обратной компабитильности. Предлагается использовать новую модель в будущем.

Если требуется проанализировать файлы Avro или записать данные в формате Avro, установите для свойства `format` `type` значение **AvroFormat**. Вам не нужно указывать какие-либо свойства в подразделе Format раздела typeProperties. Пример

```json
"format":
{
    "type": "AvroFormat",
}
```

Чтобы использовать формат Avro в таблице Hive, вы можете обратиться к [учебнику Apache Hive.](https://cwiki.apache.org/confluence/display/Hive/AvroSerDe)

Обратите внимание на следующие моменты.

* [Сложные типы данных](https://avro.apache.org/docs/current/spec.html#schema_complex) не поддерживаются (записи, цифры, массивы, карты, союзы и фиксированные).

## <a name="compression-support-legacy"></a><a name="compression-support"></a>Поддержка сжатия (наследие)

Фабрика данных Azure поддерживает сжатие и распаковку данных во время копирования. Если вы указываете свойство `compression` во входном наборе данных, действие копирования читает сжатые файлы из источника и распаковывает их. При указании этого свойства в выходном наборе данных действие копирования сжимает, а затем записывает данные в приемник. Ниже приведено несколько примеров сценариев:

* Считайте сжатые с помощью кодека GZIP данные из BLOB-объекта Azure, распакуйте их и запишите результирующие данные в базу данных SQL Azure. Набор данных ввода Azure Blob `compression` `type` с свойством определяется как GIP.
* Считайте данные из обычного текстового файла в локальной файловой системе, сожмите их в формате GZip и запишите сжатые данные в BLOB-объект Azure. Вы определяете выходный набор данных `compression` `type` Azure Blob с свойством как Gip.
* Считайте ZIP-файл с FTP-сервера, распакуйте его, чтобы получить содержащиеся в нем файлы, и отправьте их в хранилище Azure Data Lake Store. Набор данных ввода FTP `compression` `type` с свойством определяется как qipDeflate.
* Считайте сжатые с помощью кодека GZIP данные из BLOB-объекта Azure, распакуйте их и сожмите с помощью BZIP2, а затем запишите результирующие данные в BLOB-объект Azure. Вы определяете набор данных ввода `compression` `type` Azure Blob с набором на GZIP и набор выходных данных с `compression` `type` набором на B'IP2.

Чтобы указать сжатие для набора данных, используйте свойство **compression** в наборе данных JSON, как показано в следующем примере.

```json
{
    "name": "AzureBlobDataSet",
    "properties": {
        "type": "AzureBlob",
        "linkedServiceName": {
            "referenceName": "StorageLinkedService",
            "type": "LinkedServiceReference"
        },
        "typeProperties": {
            "fileName": "pagecounts.csv.gz",
            "folderPath": "compression/file/",
            "format": {
                "type": "TextFormat"
            },
            "compression": {
                "type": "GZip",
                "level": "Optimal"
            }
        }
    }
}
```

Раздел **compression** содержит два свойства:

* **Тип:** кодек сжатия, который может быть **ГЗИП,** **Deflate**, **B'IP2**, или **ципдефлировать**. Обратите внимание при использовании копирования деятельности для декомпрессии файла (ы) и записи в `<path specified in dataset>/<folder named as source zip file>/`файл на основе хранения данных о раковине, файлы будут извлечены в папку: .
* **Level** — коэффициент сжатия; возможные значения: **Optimal** и **Fastest**.

  * **Fastest:** операция сжатия должна выполняться как можно быстрее, даже если итоговый файл сжимается не оптимально.
  * **Optimal**: операция сжатия должна выполняться оптимально, даже если для ее завершения требуется больше времени.

    Дополнительные сведения см. в разделе [Уровень сжатия](https://msdn.microsoft.com/library/system.io.compression.compressionlevel.aspx).

> [!NOTE]
> Параметры сжатия для данных в форматах **AvroFormat**, **OrcFormat** или **ParquetFormat** не поддерживаются. Для чтения данных в этих форматах фабрика данных выявляет и использует в метаданных кодек сжатия. При записи в файл в одном из этих форматов фабрика данных выбирает кодек сжатия по умолчанию для этого формата. Например, ZLIB для OrcFormat и SNAPPY для ParquetFormat.

## <a name="unsupported-file-types-and-compression-formats"></a>Неподдерживаемые типы файлов и форматы сжатия

Для преобразования файлов, которые не поддерживаются, можно использовать функции расширяемости Фабрики данных Azure.
Два варианта включают функции Azure и пользовательские задачи с помощью Azure Batch.

Можно увидеть образец, в который используется функция Azure для [извлечения содержимого файла смолы.](https://github.com/Azure/Azure-DataFactory/tree/master/SamplesV2/UntarAzureFilesWithAzureFunction) Для получения дополнительной [Azure Functions activity](https://docs.microsoft.com/azure/data-factory/control-flow-azure-function-activity)информации см.

Вы также можете создать эту функциональность, используя пользовательскую деятельность Dotnet. Дополнительная информация доступна [здесь](https://docs.microsoft.com/azure/data-factory/transform-data-using-dotnet-custom-activity)

## <a name="next-steps"></a>Дальнейшие действия

Узнайте последние поддерживаемые форматы файлов и сжатия из [поддерживаемых форматов файлов и сжатий.](supported-file-formats-and-compression-codecs.md)
