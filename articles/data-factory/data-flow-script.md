---
title: Сценарий потока данных сопоставления
description: Общие сведения о языке кода программной части сценария потока данных фабрики данных
author: kromerm
ms.author: nimoolen
ms.service: data-factory
ms.topic: conceptual
ms.custom: seo-lt-2019
ms.date: 09/29/2020
ms.openlocfilehash: 6802e3f6c0892993f9ffe4373f43274362b8a003
ms.sourcegitcommit: f796e1b7b46eb9a9b5c104348a673ad41422ea97
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 09/30/2020
ms.locfileid: "91569681"
---
# <a name="data-flow-script-dfs"></a>Сценарий потока данных (DFS)

[!INCLUDE[appliesto-adf-asa-md](includes/appliesto-adf-asa-md.md)]

Сценарий потока данных (DFS) — это базовые метаданные, аналогичные языку программирования, используемому для выполнения преобразований, включенных в поток данных сопоставления. Каждое преобразование представлено рядом свойств, которые предоставляют необходимые сведения для правильного выполнения задания. Сценарий отображается и редактируется из ADF. для этого нажмите кнопку "Скрипт" на верхней ленте пользовательского интерфейса браузера.

![Кнопка скрипта](media/data-flow/scriptbutton.png "Кнопка скрипта")

Например, `allowSchemaDrift: true,` в преобразовании «источник» указывает службе включать все столбцы из исходного набора данных в потоке данных, даже если они не включены в проекцию схемы.

## <a name="use-cases"></a>Варианты использования
DFS автоматически создается интерфейсом пользователя. Можно нажать кнопку Скрипт, чтобы просмотреть и настроить скрипт. Кроме того, можно создавать скрипты за пределами пользовательского интерфейса ADF, а затем передавать их в командлет PowerShell. При отладке сложных потоков данных может оказаться проще проверять код программной части сценария, а не проверять представление графа пользовательского интерфейса для потоков.

Ниже приведены примеры использования.
- Программное создание многих похожих потоков данных, т. е. потоков данных с отметкой.
- Сложные выражения, которые трудно управлять в пользовательском интерфейсе или которые могут приводить к проблемам проверки.
- Отладка и более эффективное понимание различных ошибок, возвращаемых во время выполнения.

При создании скрипта потока данных для использования с PowerShell или API необходимо свернуть форматированный текст в одну строку. Символы табуляции и новой строки можно размещать в виде escape-символов. Но текст должен быть отформатирован в соответствии со свойством JSON. В нижней части пользовательского интерфейса редактора сценариев есть кнопка, которая будет форматировать сценарий как единую строку.

![Кнопка "Копировать"](media/data-flow/copybutton.png "Кнопка "Копировать"")

## <a name="how-to-add-transforms"></a>Добавление преобразований
Добавление преобразований требует выполнения трех основных шагов: Добавление основных данных преобразования, перенаправление входного потока и перенаправление потока вывода. В качестве примера это можно увидеть проще всего.
Предположим, что начнем с простого источника данных приемника, как в следующем примере:

```
source(output(
        movieId as string,
        title as string,
        genres as string
    ),
    allowSchemaDrift: true,
    validateSchema: false) ~> source1
source1 sink(allowSchemaDrift: true,
    validateSchema: false) ~> sink1
```

Если мы решили добавить преобразование «Производный», сначала необходимо создать основной текст преобразования, имеющий простое выражение для добавления нового столбца в верхнем регистре `upperCaseTitle` :
```
derive(upperCaseTitle = upper(title)) ~> deriveTransformationName
```

Затем мы принимаем существующую DFS и добавим преобразование:
```
source(output(
        movieId as string,
        title as string,
        genres as string
    ),
    allowSchemaDrift: true,
    validateSchema: false) ~> source1
derive(upperCaseTitle = upper(title)) ~> deriveTransformationName
source1 sink(allowSchemaDrift: true,
    validateSchema: false) ~> sink1
```

Теперь мы перенаправляем входящий поток, определив преобразование, которое должно быть после (в данном случае) новым преобразованием, `source1` и скопировав имя потока в новое преобразование:
```
source(output(
        movieId as string,
        title as string,
        genres as string
    ),
    allowSchemaDrift: true,
    validateSchema: false) ~> source1
source1 derive(upperCaseTitle = upper(title)) ~> deriveTransformationName
source1 sink(allowSchemaDrift: true,
    validateSchema: false) ~> sink1
```

Наконец, мы выведем преобразование, которое мы хотим сделать после этого нового преобразования, и замените входной поток (в данном случае `sink1` ) на имя выходного потока нашего нового преобразования:
```
source(output(
        movieId as string,
        title as string,
        genres as string
    ),
    allowSchemaDrift: true,
    validateSchema: false) ~> source1
source1 derive(upperCaseTitle = upper(title)) ~> deriveTransformationName
deriveTransformationName sink(allowSchemaDrift: true,
    validateSchema: false) ~> sink1
```

## <a name="dfs-fundamentals"></a>Основы DFS
DFS состоит из ряда подключенных преобразований, включая источники, приемники и различные другие, которые могут добавлять новые столбцы, фильтровать данные, объединять данные и многое другое. Обычно сценарий начинается с одного или нескольких источников, за которым следует много преобразований и заканчивается одним или несколькими приемниками.

Все источники имеют одинаковую базовую конструкцию:
```
source(
  source properties
) ~> source_name
```

Например, простой источник с тремя столбцами (Мовиеид, Title, жанры) будет выглядеть следующим образом:
```
source(output(
        movieId as string,
        title as string,
        genres as string
    ),
    allowSchemaDrift: true,
    validateSchema: false) ~> source1
```

Все преобразования, отличные от источников, имеют одну базовую конструкцию:
```
name_of_incoming_stream transformation_type(
  properties
) ~> new_stream_name
```

Например, простое производное преобразование, которое принимает столбец (заголовок) и перезаписывает его версией в верхнем регистре, будет выглядеть следующим образом:
```
source1 derive(
  title = upper(title)
) ~> derive1
```

И приемник без схемы будет просто:
```
derive1 sink(allowSchemaDrift: true,
    validateSchema: false) ~> sink1
```

## <a name="script-snippets"></a>Фрагменты сценариев

Фрагменты сценариев — это общий код сценария потока данных, который можно использовать для совместного использования в потоках данных. В этом видеоролике рассказывается о том, как использовать фрагменты сценариев и использование сценария потока данных для копирования и вставки частей сценария, расположенных в графах потоков данных:

> [!VIDEO https://www.microsoft.com/en-us/videoplayer/embed/RE4tA9b]


### <a name="aggregated-summary-stats"></a>Агрегированная сводная статистика
Добавьте преобразование «Статистическая обработка» в поток данных с именем «Суммаристатс», а затем вставьте этот код ниже для агрегатной функции в скрипте, заменив существующий Суммаристатс. Это обеспечит общий шаблон статистической статистики для профиля данных.

```
aggregate(each(match(true()), $$+'_NotNull' = countIf(!isNull($$)), $$ + '_Null' = countIf(isNull($$))),
        each(match(type=='double'||type=='integer'||type=='short'||type=='decimal'), $$+'_stddev' = round(stddev($$),2), $$ + '_min' = min ($$), $$ + '_max' = max($$), $$ + '_average' = round(avg($$),2), $$ + '_variance' = round(variance($$),2)),
        each(match(type=='string'), $$+'_maxLength' = max(length($$)))) ~> SummaryStats
```
Приведенный ниже пример также можно использовать для подсчета числа уникальных и количества уникальных строк в данных. Приведенный ниже пример можно вставлять в поток данных с преобразованием «Статистическая обработка» с именем Валуедистагг. В этом примере используется столбец с именем Title. Обязательно замените "Title" строковым столбцом данных, который вы хотите использовать для получения счетчиков значений.

```
aggregate(groupBy(title),
    countunique = count()) ~> ValueDistAgg
ValueDistAgg aggregate(numofunique = countIf(countunique==1),
        numofdistinct = countDistinct(title)) ~> UniqDist
```

### <a name="include-all-columns-in-an-aggregate"></a>Включить все столбцы в статистическое выражение
Это универсальный шаблон статистического выражения, демонстрирующий, как можно обеспечить возможность сохранения оставшихся столбцов в выходных метаданных при построении статистических выражений. В этом случае мы используем функцию, ```first()``` чтобы выбрать первое значение в каждом столбце, имя которого не является «Movie». Чтобы использовать эту функцию, создайте преобразование «Статистическая обработка» с именем Дистинктровс и вставьте его в скрипт поверх существующего статистического сценария Дистинктровс.

```
aggregate(groupBy(movie),
    each(match(name!='movie'), $$ = first($$))) ~> DistinctRows
```

### <a name="create-row-hash-fingerprint"></a>Создать отпечаток хэша строки 
Используйте этот код в скрипте потока данных для создания нового производного столбца ```DWhash``` с именем, создающего ```sha1``` хэш трех столбцов.

```
derive(DWhash = sha1(Name,ProductNumber,Color))
```

Этот скрипт также можно использовать для создания хэша строки с использованием всех столбцов, имеющихся в вашем потоке, без необходимости присвоить каждому столбцу имя.

```
derive(DWhash = sha1(columns()))
```

### <a name="string_agg-equivalent"></a>Эквивалент String_agg
Этот код будет действовать как функция T-SQL ```string_agg()``` и будет объединять строковые значения в массив. Затем этот массив можно привести в строку для использования с назначениями SQL.

```
source1 aggregate(groupBy(year),
    string_agg = collect(title)) ~> Aggregate1
Aggregate1 derive(string_agg = toString(string_agg)) ~> DerivedColumn2
```

### <a name="count-number-of-updates-upserts-inserts-deletes"></a>Число обновлений, операции Upsert, вставок, удалений
При использовании преобразования ALTER Row может потребоваться подсчитать количество операций Updates, операции Upsert, Inserts, которые являются результатом политик изменения строк. Добавьте преобразование «Статистическая обработка» после изменения строки и вставьте этот скрипт потока данных в определение агрегата для этих счетчиков.

```
aggregate(updates = countIf(isUpdate(), 1),
        inserts = countIf(isInsert(), 1),
        upserts = countIf(isUpsert(), 1),
        deletes = countIf(isDelete(),1)) ~> RowCount
```

### <a name="distinct-row-using-all-columns"></a>Уникальная строка с использованием всех столбцов
Этот фрагмент кода добавит в поток данных новое преобразование «Статистическая обработка», которое будет принимать все входящие столбцы, формировать хэш, используемый для группирования, чтобы исключить дубликаты, а затем предоставить первое вхождение каждого дубликата в качестве выходных данных. Явное имя столбцов не требуется, они будут автоматически создаваться из входящего потока данных.

```
aggregate(groupBy(mycols = sha2(256,columns())),
    each(match(true()), $$ = first($$))) ~> DistinctRows
```

### <a name="check-for-nulls-in-all-columns"></a>Проверять наличие значений NULL во всех столбцах
Это фрагмент кода, который можно вставить в поток данных для универсальной проверки всех столбцов на наличие значений NULL. Этот метод использует смещение схемы, чтобы просмотреть все столбцы во всех строках и использовать условное разбиение для разделения строк значениями NULL из строк без значений NULL. 

```
CreateColumnArray split(contains(array(columns()),isNull(#item)),
    disjoint: false) ~> LookForNULLs@(hasNULLs, noNULLs)
```

## <a name="next-steps"></a>Дальнейшие действия

Проанализируйте потоки данных, начав с помощью [статьи общие сведения о потоках данных](concepts-data-flow-overview.md)
