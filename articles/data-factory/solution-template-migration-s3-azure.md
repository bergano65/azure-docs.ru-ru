---
title: Перенос данных из Amazon S3 в Azure Data Lake Storage 2-го поколения с помощью фабрики данных Azure
description: Узнайте, как использовать шаблон решения для переноса данных из Amazon S3 с помощью внешней таблицы управления для хранения списка секций в AWS S3 с фабрикой данных Azure.
services: data-factory
documentationcenter: ''
author: dearandyxu
ms.author: yexu
ms.reviewer: ''
manager: ''
ms.service: data-factory
ms.workload: data-services
ms.tgt_pltfrm: na
ms.devlang: na
ms.topic: conceptual
ms.date: 09/07/2019
ms.openlocfilehash: a8591762bf4e8eccd5e1b7d67538674feed720b9
ms.sourcegitcommit: 609d4bdb0467fd0af40e14a86eb40b9d03669ea1
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 11/06/2019
ms.locfileid: "73684198"
---
# <a name="migrate-data-from-amazon-s3-to-azure-data-lake-storage-gen2"></a>Перенос данных из Amazon S3 в Azure Data Lake Storage 2-го поколения

Используйте шаблоны для переноса петабайтов данных, состоящих из сотен миллионов файлов из Amazon S3, в Azure Data Lake Storage 2-го поколения. 

 > [!NOTE]
 > Если вы хотите скопировать небольшой объем данных из AWS S3 в Azure (например, менее 10 ТБ), более эффективно и удобно использовать [средство копирование данных фабрики данных Azure](copy-data-tool.md). Шаблон, описанный в этой статье, больше, чем требуется.

## <a name="about-the-solution-templates"></a>О шаблонах решений

Рекомендуется использовать секцию данных, особенно при переносе более 10 ТБ данных. Чтобы секционировать данные, используйте параметр "префикс" для фильтрации папок и файлов в Amazon S3 по имени, а затем каждое задание копирования ADF может копировать по одной секции за раз. Для повышения пропускной способности можно одновременно запустить несколько заданий копирования ADF.

Для переноса данных обычно требуется однократная миграция данных, а также периодическая синхронизация изменений из AWS S3 с Azure. Ниже приведено два шаблона, в которых один шаблон охватывает однократную миграцию данных, а другой шаблон охватывает синхронизацию изменений из AWS S3 с Azure.

### <a name="for-the-template-to-migrate-historical-data-from-amazon-s3-to-azure-data-lake-storage-gen2"></a>Для шаблона, чтобы перенести исторические данные из Amazon S3 в Azure Data Lake Storage 2-го поколения

Этот шаблон (*имя шаблона. Перенос исторических данных из AWS S3 в Azure Data Lake Storage 2-го поколения*) предполагает, что вы написали список секций во внешней таблице управления в базе данных SQL Azure. Поэтому он будет использовать действие *уточняющего запроса* для получения списка секций из внешней таблицы элементов управления, итерации по каждой секции и создания каждой копии задания копирования ADF по одной секции за раз. После завершения задания копирования оно использует действие *хранимой процедуры* для обновления состояния копирования каждой секции в таблице управления.

Шаблон содержит пять действий:
- **Уточняющий запрос** извлекает секции, которые не были скопированы в Azure Data Lake Storage 2-го поколения из внешней таблицы управления. Имя таблицы — *s3_partition_control_table* , а запрос для загрузки данных из таблицы — *SELECT ПАРТИТИОНПРЕФИКС from s3_partition_control_table WHERE сукцессорфаилуре = 0*.
- **Foreach** получает список секций из действия *уточняющего запроса* и перебирает каждую секцию в действие *тригжеркопи* . Можно настроить *батчкаунт* для одновременного запуска нескольких заданий копирования ADF. Мы установили 2 в этом шаблоне.
- **ExecutePipeline** выполняет конвейер *CopyFolderPartitionFromS3* . Причина, по которой мы создаем другой конвейер для копирования каждого задания копирования в секцию, заключается в том, что это облегчит повторное выполнение неудачного задания копирования для повторной загрузки этой секции из AWS S3. Все остальные задания копирования, загружают другие секции, не будут затронуты.
- **Copy** копирует каждую секцию из AWS S3 в Azure Data Lake Storage 2-го поколения.
- **SqlServerStoredProcedure** обновление состояния копирования каждой секции в таблице управления.

Шаблон содержит два параметра:
- **AWS_S3_bucketName** — это имя вашего сегмента в AWS S3, куда нужно перенести данные. Если вы хотите перенести данные из нескольких контейнеров в AWS S3, можно добавить еще один столбец во внешнюю таблицу управления для хранения имени контейнера для каждой секции, а также обновить конвейер для получения данных из этого столбца соответствующим образом.
- **Azure_Storage_fileSystem** — это имя файловой системы на Azure Data Lake Storage 2-го поколения, куда нужно перенести данные.

### <a name="for-the-template-to-copy-changed-files-only-from-amazon-s3-to-azure-data-lake-storage-gen2"></a>Чтобы шаблон скопировал измененные файлы только из Amazon S3 в Azure Data Lake Storage 2-го поколения

Этот шаблон (*имя шаблона: копирование Дельта-данных из AWS S3 в Azure Data Lake Storage 2-го поколения*) использует время каждого файла для копирования новых или обновленных файлов только из AWS S3 в Azure. Имейте в виду, что если файлы или папки уже были разделены на разделы с данными тимеслице как часть имени файла или папки в AWS S3 (например,/ИИИИ/мм/дд/филе.КСВ), можно перейти к этому [учебнику](tutorial-incremental-copy-partitioned-file-name-copy-data-tool.md) , чтобы получить более производительный подход к добавочной Загрузка новых файлов. В этом шаблоне предполагается, что вы написали список секций во внешней таблице элементов управления в базе данных SQL Azure. Поэтому он будет использовать действие *уточняющего запроса* для получения списка секций из внешней таблицы элементов управления, итерации по каждой секции и создания каждой копии задания копирования ADF по одной секции за раз. Когда каждое задание копирования начинает копирование файлов из AWS S3, оно использует свойство время для обнаружения и копирования только новых или обновленных файлов. После завершения задания копирования оно использует действие *хранимой процедуры* для обновления состояния копирования каждой секции в таблице управления.

Шаблон содержит семь действий:
- **Уточняющий запрос** извлекает секции из внешней таблицы управления. Имя таблицы — *s3_partition_delta_control_table* , а запрос для загрузки данных из таблицы — *SELECT DISTINCT партитионпрефикс from s3_partition_delta_control_table*.
- **Foreach** получает список секций из действия *уточняющего запроса* и перебирает каждую секцию в действие *тригжерделтакопи* . Можно настроить *батчкаунт* для одновременного запуска нескольких заданий копирования ADF. Мы установили 2 в этом шаблоне.
- **ExecutePipeline** выполняет конвейер *DeltaCopyFolderPartitionFromS3* . Причина, по которой мы создаем другой конвейер для копирования каждого задания копирования в секцию, заключается в том, что это облегчит повторное выполнение неудачного задания копирования для повторной загрузки этой секции из AWS S3. Все остальные задания копирования, загружают другие секции, не будут затронуты.
- **Уточняющий запрос** получает Последнее время выполнения задания копирования из внешней таблицы управления, чтобы новые или обновленные файлы могли быть идентифицированы через время. Имя таблицы — *s3_partition_delta_control_table* , а запрос для загрузки данных из таблицы — *"SELECT MAX (Жобрунтиме) as время from s3_partition_delta_control_table WHERE партитионпрефикс = ' @ {конвейер (). parameters. префиксстр } "и Сукцессорфаилуре = 1"* .
- **Copy** копирует новые или измененные файлы только для каждого раздела из AWS S3 в Azure Data Lake Storage 2-го поколения. Свойству *модифиеддатетиместарт* задано Последнее время выполнения задания копирования. Свойству *модифиеддатетиминд* задано текущее время выполнения задания копирования. Имейте в виду, что время применяется к часовому поясу UTC.
- **SqlServerStoredProcedure** обновляет состояние копирования каждого раздела и время выполнения копирования в таблице управления, когда она выполняется. Столбцу Сукцессорфаилуре присваивается значение 1.
- **SqlServerStoredProcedure** обновляет состояние копирования каждого раздела и время выполнения копирования в таблице управления в случае сбоя. Столбцу Сукцессорфаилуре присваивается значение 0.

Шаблон содержит два параметра:
- **AWS_S3_bucketName** — это имя вашего сегмента в AWS S3, куда нужно перенести данные. Если вы хотите перенести данные из нескольких контейнеров в AWS S3, можно добавить еще один столбец во внешнюю таблицу управления для хранения имени контейнера для каждой секции, а также обновить конвейер для получения данных из этого столбца соответствующим образом.
- **Azure_Storage_fileSystem** — это имя файловой системы на Azure Data Lake Storage 2-го поколения, куда нужно перенести данные.

## <a name="how-to-use-these-two-solution-templates"></a>Использование этих двух шаблонов решений

### <a name="for-the-template-to-migrate-historical-data-from-amazon-s3-to-azure-data-lake-storage-gen2"></a>Для шаблона, чтобы перенести исторические данные из Amazon S3 в Azure Data Lake Storage 2-го поколения 

1. Создайте таблицу элементов управления в базе данных SQL Azure, чтобы сохранить список разделов AWS S3. 

    > [!NOTE]
    > Имя таблицы — s3_partition_control_table.
    > Схема таблицы управления — Партитионпрефикс и Сукцессорфаилуре, где Партитионпрефикс — это префиксный параметр в S3 для фильтрации папок и файлов в Amazon S3 по имени, а Сукцессорфаилуре — состояние копирования каждого раздела: 0 означает, что этот раздел не скопировано в Azure, и 1 означает, что этот раздел успешно скопирован в Azure.
    > В таблице элементов управления определено 5 секций, и состояние по умолчанию для копирования каждой секции — 0.

    ```sql
    CREATE TABLE [dbo].[s3_partition_control_table](
        [PartitionPrefix] [varchar](255) NULL,
        [SuccessOrFailure] [bit] NULL
    )

    INSERT INTO s3_partition_control_table (PartitionPrefix, SuccessOrFailure)
    VALUES
    ('a', 0),
    ('b', 0),
    ('c', 0),
    ('d', 0),
    ('e', 0);
    ```

2. Создайте хранимую процедуру в той же базе данных SQL Azure для управления таблицей. 

    > [!NOTE]
    > Имя хранимой процедуры — sp_update_partition_success. Он будет вызываться действием SqlServerStoredProcedure в конвейере ADF.

    ```sql
    CREATE PROCEDURE [dbo].[sp_update_partition_success] @PartPrefix varchar(255)
    AS
    BEGIN
    
        UPDATE s3_partition_control_table
        SET [SuccessOrFailure] = 1 WHERE [PartitionPrefix] = @PartPrefix
    END
    GO
    ```

3. Перейдите к разделу **Миграция исторических данных из AWS S3 в шаблон Azure Data Lake Storage 2-го поколения** . Введите подключения к внешней таблице управления, AWS S3 в качестве хранилища источников данных и Azure Data Lake Storage 2-го поколения в качестве целевого хранилища. Имейте в виду, что внешняя таблица элемента управления и хранимая процедура ссылаются на одно и то же соединение.

    ![Создание подключения](media/solution-template-migration-s3-azure/historical-migration-s3-azure1.png)

4. Выберите **использовать этот шаблон**.

    ![Использование шаблона](media/solution-template-migration-s3-azure/historical-migration-s3-azure2.png)
    
5. Вы увидите два конвейера и 3 набора данных, как показано в следующем примере:

    ![Проверка конвейера](media/solution-template-migration-s3-azure/historical-migration-s3-azure3.png)

6. Выберите **Отладка**, введите **Параметры**и нажмите кнопку **Готово**.

    ![Щелкните * * Отладка * *](media/solution-template-migration-s3-azure/historical-migration-s3-azure4.png)

7. Отобразятся результаты, аналогичные приведенным в следующем примере:

    ![Просмотр результатов](media/solution-template-migration-s3-azure/historical-migration-s3-azure5.png)


### <a name="for-the-template-to-copy-changed-files-only-from-amazon-s3-to-azure-data-lake-storage-gen2"></a>Чтобы шаблон скопировал измененные файлы только из Amazon S3 в Azure Data Lake Storage 2-го поколения

1. Создайте таблицу элементов управления в базе данных SQL Azure, чтобы сохранить список разделов AWS S3. 

    > [!NOTE]
    > Имя таблицы — s3_partition_delta_control_table.
    > Схема таблицы элементов управления — Партитионпрефикс, Жобрунтиме и Сукцессорфаилуре, где Партитионпрефикс — это параметр префикса в S3 для фильтрации папок и файлов в Amazon S3 по имени, Жобрунтиме — это значение DateTime при выполнении заданий копирования, а Сукцессорфаилуре — Состояние копирования каждой секции: 0 означает, что эта Секция не была скопирована в Azure, а 1 означает, что этот раздел был успешно скопирован в Azure.
    > В таблице элементов управления определено 5 секций. Значением по умолчанию для Жобрунтиме может быть время, когда начинается однократная миграция данных журнала. Действие копирования ADF скопирует файлы на AWS S3, которые были в последний раз изменены. Состояние по умолчанию для копирования каждой секции — 1.

    ```sql
    CREATE TABLE [dbo].[s3_partition_delta_control_table](
        [PartitionPrefix] [varchar](255) NULL,
        [JobRunTime] [datetime] NULL,
        [SuccessOrFailure] [bit] NULL
        )

    INSERT INTO s3_partition_delta_control_table (PartitionPrefix, JobRunTime, SuccessOrFailure)
    VALUES
    ('a','1/1/2019 12:00:00 AM',1),
    ('b','1/1/2019 12:00:00 AM',1),
    ('c','1/1/2019 12:00:00 AM',1),
    ('d','1/1/2019 12:00:00 AM',1),
    ('e','1/1/2019 12:00:00 AM',1);
    ```

2. Создайте хранимую процедуру в той же базе данных SQL Azure для управления таблицей. 

    > [!NOTE]
    > Имя хранимой процедуры — sp_insert_partition_JobRunTime_success. Он будет вызываться действием SqlServerStoredProcedure в конвейере ADF.

    ```sql
        CREATE PROCEDURE [dbo].[sp_insert_partition_JobRunTime_success] @PartPrefix varchar(255), @JobRunTime datetime, @SuccessOrFailure bit
        AS
        BEGIN
            INSERT INTO s3_partition_delta_control_table (PartitionPrefix, JobRunTime, SuccessOrFailure)
            VALUES
            (@PartPrefix,@JobRunTime,@SuccessOrFailure)
        END
        GO
    ```


3. Перейдите к разделу **копирование разностных данных из AWS S3 в шаблон Azure Data Lake Storage 2-го поколения** . Введите подключения к внешней таблице управления, AWS S3 в качестве хранилища источников данных и Azure Data Lake Storage 2-го поколения в качестве целевого хранилища. Имейте в виду, что внешняя таблица элемента управления и хранимая процедура ссылаются на одно и то же соединение.

    ![Создание подключения](media/solution-template-migration-s3-azure/delta-migration-s3-azure1.png)

4. Выберите **использовать этот шаблон**.

    ![Использование шаблона](media/solution-template-migration-s3-azure/delta-migration-s3-azure2.png)
    
5. Вы увидите два конвейера и 3 набора данных, как показано в следующем примере:

    ![Проверка конвейера](media/solution-template-migration-s3-azure/delta-migration-s3-azure3.png)

6. Выберите **Отладка**, введите **Параметры**и нажмите кнопку **Готово**.

    ![Щелкните * * Отладка * *](media/solution-template-migration-s3-azure/delta-migration-s3-azure4.png)

7. Отобразятся результаты, аналогичные приведенным в следующем примере:

    ![Просмотр результатов](media/solution-template-migration-s3-azure/delta-migration-s3-azure5.png)

8. Кроме того, результаты можно проверить в таблице элементов управления с помощью запроса *"SELECT * FROM s3_partition_delta_control_table" (выборка*), вы увидите результат, аналогичный приведенному в следующем примере:

    ![Просмотр результатов](media/solution-template-migration-s3-azure/delta-migration-s3-azure6.png)
    
## <a name="next-steps"></a>Дальнейшие действия

- [Копирование файлов из нескольких контейнеров](solution-template-copy-files-multiple-containers.md)
- [Перемещение файлов](solution-template-move-files.md)