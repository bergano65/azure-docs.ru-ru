---
title: Миграция данных из Amazon S3 в службу хранилища Azure
description: Используйте Фабрику данных Azure для миграции данных из Amazon S3 в службу хранилища Azure.
services: data-factory
ms.author: yexu
author: dearandyxu
ms.reviewer: ''
manager: shwang
ms.service: data-factory
ms.workload: data-services
ms.topic: conceptual
ms.custom: seo-lt-2019
ms.date: 8/04/2019
ms.openlocfilehash: 3f40ad7346219b48a38ade38b2a75ddf71940875
ms.sourcegitcommit: 849bb1729b89d075eed579aa36395bf4d29f3bd9
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 04/28/2020
ms.locfileid: "81416419"
---
# <a name="use-azure-data-factory-to-migrate-data-from-amazon-s3-to-azure-storage"></a>Миграция данных из Amazon S3 в службу хранилища Azure с помощью Фабрики данных Azure 

[!INCLUDE[appliesto-adf-xxx-md](includes/appliesto-adf-xxx-md.md)]

Фабрика данных Azure обеспечивает эффективный, надежный и экономичный механизм переноса любых объемов данных из Amazon S3 в Хранилище BLOB-объектов Azure или Azure Data Lake Storage 2-го поколения.  В этой статье содержатся следующие сведения для специалистов по обработке и анализу данных и разработчиков: 

> [!div class="checklist"]
> * Производительность 
> * Устойчивость копирования
> * Безопасность сети
> * Общая архитектура решения 
> * Рекомендации по реализации  

## <a name="performance"></a>Производительность

Фабрика данных Azure имеет бессерверную архитектуру, которая обеспечивает параллелизм на разных уровнях. Это позволяет разработчикам создавать конвейеры для полного использования пропускной способности сети и хранилища с целью передачи данных в среде с максимальной скоростью. 

Клиенты успешно выполнили перенос сотен миллионов файлов, общий объем которых измеряется петабайтами, из Amazon S3 в Хранилище BLOB-объектов Azure со стабильной пропускной способностью 2 Гбит/с и выше. 

![производительность](media/data-migration-guidance-s3-to-azure-storage/performance.png)

На рисунке выше показано, как можно обеспечить высокую скорость перемещения данных с помощью различных уровней параллелизма.
 
- Для одного действия копирования можно использовать масштабируемые вычислительные ресурсы: при использовании Azure Integration Runtime можно указать [до 256 единиц интеграции данных](https://docs.microsoft.com/azure/data-factory/copy-activity-performance#data-integration-units) для каждого бессерверного действия копирования. При использовании локальной среды выполнения интеграции можно вручную масштабировать компьютер по вертикали или выполнить масштабирование по горизонтали до нескольких компьютеров ([до 4 узлов](https://docs.microsoft.com/azure/data-factory/create-self-hosted-integration-runtime#high-availability-and-scalability)), и действие копирования разделит набор файлов по всем узлам. 
- Одно действие копирования считывает данные из хранилища данных и записывает их в него с помощью нескольких потоков. 
- Поток управления Фабрики данных Azure может запускать несколько операций копирования параллельно, например с помощью [цикла For Each](https://docs.microsoft.com/azure/data-factory/control-flow-for-each-activity). 

## <a name="resilience"></a>Устойчивость

Фабрика данных Azure имеет встроенный механизм повтора в рамках одного действия копирования, позволяющий справляться с определенным количеством временных сбоев в хранилищах данных или в базовой сети. 

При копировании двоичных данных из S3 в Хранилище BLOB-объектов и из S3 в ADLS 2-го поколения Фабрика данных Azure автоматически создает контрольные точки.  Если при выполнении действия копирования происходит сбой или истекает время ожидания, при последующей попытке копирование возобновляется с последней точки сбоя, а не с начала. 

## <a name="network-security"></a>Безопасность сети 

По умолчанию Фабрика данных Azure передает данные из Amazon S3 в Хранилище BLOB-объектов Azure или Azure Data Lake Storage 2-го поколения через зашифрованное подключение по протоколу HTTPS.  HTTPS обеспечивает шифрование данных при передаче и предотвращает прослушивание трафика и атаки типа "злоумышленник в середине". 

Кроме того, если вы не хотите, чтобы данные передавались через общедоступный Интернет, вы можете повысить уровень безопасности, передавая данные через канал частного пиринга между AWS Direct Connect и Azure Express Route.  Сведения о том, как это можно сделать, см. в описании архитектуры решения ниже. 

## <a name="solution-architecture"></a>Архитектура решения

Перенос данных через общедоступный Интернет:

![solution-architecture-public-network](media/data-migration-guidance-s3-to-azure-storage/solution-architecture-public-network.png)

- В этой архитектуре данные безопасно передаются по протоколу HTTPS через общедоступный Интернет. 
- Как исходная служба Amazon S3, так и целевое Хранилище BLOB-объектов Azure или Azure Data Lake Storage 2-го поколения настроены так, чтобы трафик был разрешен со всех сетевых IP-адресов.  Если вам нужно ограничить сетевой доступ определенным диапазоном IP-адресов, см. вторую архитектуру ниже. 
- Вы можете легко масштабировать ресурсы бессерверным образом, чтобы полностью использовать пропускную способность сети и хранилища с целью получения максимальной скорости передачи данных в среде. 
- С помощью этой архитектуры можно обеспечить как перенос исходных моментальных снимков, так и перенос разностных данных. 

Перенос данных через частный канал: 

![solution-architecture-private-network](media/data-migration-guidance-s3-to-azure-storage/solution-architecture-private-network.png)

- В этой архитектуре перенос данных осуществляется через канал частного пиринга между AWS Direct Connect и Azure Express Route, так что данные никогда не передаются через общедоступный Интернет.  Для этого требуется использовать AWS VPC и виртуальную сеть Azure. 
- Для реализации этой архитектуры необходимо установить локальную среду выполнения интеграции Фабрики данных Azure на виртуальной машине Windows в виртуальной сети Azure.  Вы можете вручную масштабировать виртуальные машины локальной среды выполнения интеграции по вертикали или масштабировать среду до нескольких виртуальных машин (до 4 узлов), чтобы полностью использовать возможности ввода-вывода и пропускную способность сети и хранилища. 
- Если данные можно передавать по протоколу HTTPS, но сетевой доступ к исходной службе S3 следует ограничить определенным диапазоном IP-адресов, можно использовать разновидность этой архитектуры, убрав AWS VPC и заменив частный канал на HTTPS.  Виртуальную сеть Azure и локальную среду выполнения интеграции в этом случае желательно размещать на виртуальной машине Azure, чтобы можно было использовать статический IP-адрес с общедоступной маршрутизацией для списка разрешений. 
- С помощью этой архитектуры можно обеспечить как перенос исходных моментальных снимков, так и перенос разностных данных. 

## <a name="implementation-best-practices"></a>Рекомендации по реализации 

### <a name="authentication-and-credential-management"></a>Управление проверкой подлинности и учетными данными 

- Для проверки подлинности учетной записи Amazon S3 необходимо использовать [ключ доступа для учетной записи IAM](https://docs.microsoft.com/azure/data-factory/connector-amazon-simple-storage-service#linked-service-properties). 
- Для подключения к Хранилищу BLOB-объектов Azure поддерживаются несколько типов проверки подлинности.  Настоятельно рекомендуется использовать [управляемые удостоверения для ресурсов Azure](https://docs.microsoft.com/azure/data-factory/connector-azure-blob-storage#managed-identity): они основаны на автоматически управляемой системе идентификации Фабрики данных Azure в Azure AD и позволяют настраивать конвейеры, не предоставляя учетные данные в определении связанной службы.  Кроме того, проверку подлинности в Хранилище BLOB-объектов Azure можно проходить, используя [субъект-службу](https://docs.microsoft.com/azure/data-factory/connector-azure-blob-storage#service-principal-authentication), [подписанный URL-адрес](https://docs.microsoft.com/azure/data-factory/connector-azure-blob-storage#shared-access-signature-authentication) или [ключ учетной записи хранения](https://docs.microsoft.com/azure/data-factory/connector-azure-blob-storage#account-key-authentication). 
- Для подключения к Azure Data Lake Storage 2-го поколения также поддерживаются несколько типов проверки подлинности.  Настоятельно рекомендуется использовать [управляемые удостоверения для ресурсов Azure](https://docs.microsoft.com/azure/data-factory/connector-azure-data-lake-storage#managed-identity), хотя также можно применять [субъект-службу](https://docs.microsoft.com/azure/data-factory/connector-azure-data-lake-storage#service-principal-authentication) или [ключ учетной записи хранения](https://docs.microsoft.com/azure/data-factory/connector-azure-data-lake-storage#account-key-authentication). 
- Если вы не используете управляемые удостоверения для ресурсов Azure, настоятельно рекомендуется [хранить учетные данные в Azure Key Vault](https://docs.microsoft.com/azure/data-factory/store-credentials-in-key-vault), чтобы упростить централизованное управление и смену ключей без изменения связанных служб Фабрики данных Azure.  Это также одна из [рекомендаций для CI/CD](https://docs.microsoft.com/azure/data-factory/continuous-integration-deployment#best-practices-for-cicd). 

### <a name="initial-snapshot-data-migration"></a>Перенос исходных моментальных снимков 

Секционирование данных особенно рекомендуется при переносе более чем 100 ТБ данных.  Чтобы секционировать данные, используйте параметр prefix для фильтрации папок и файлов в Amazon S3 по имени. После этого каждое задание копирования в Фабрике данных Azure может копировать по одной секции за раз.  Для повышения пропускной способности можно одновременно запустить несколько заданий копирования в Фабрике данных Azure. 

Если какое-либо из заданий копирования завершится сбоем из-за временной проблемы с сетью или хранилищем данных, оно сможет повторно загрузить эту секцию из AWS S3.  Остальные задания копирования, которые загружают другие секции, не будут затронуты. 

### <a name="delta-data-migration"></a>Перенос разностных данных 

Наиболее эффективный способ обнаружения новых или измененных файлов в AWS S3 заключается в использовании соглашения об именовании с разделением по времени. Если данные в AWS S3 разделены по времени и в именах файлов или папок содержатся интервалы времени (например, /гггг/мм/дд/файл.csv), конвейер может легко определить файлы и папки, подлежащие добавочному копированию. 

Кроме того, если данные в AWS S3 не разделены по времени, Фабрика данных Azure может обнаруживать новые или измененные файлы по дате последнего изменения.   В этом случае Фабрика данных Azure будет сканировать все файлы в AWS S3 и копировать только новые и обновленные файлы, метка времени последнего изменения которых больше определенного значения.  Имейте в виду, что при наличии большого количества файлов в S3 начальное сканирование файлов может занять много времени независимо от того, сколько файлов соответствуют условию фильтрации.  В этом случае рекомендуется сначала разделить данные, используя тот же параметр префикса, что и для переноса исходных моментальных снимков, чтобы сканирование файлов можно было выполнять параллельно.  

### <a name="for-scenarios-that-require-self-hosted-integration-runtime-on-azure-vm"></a>Сценарии, для которых требуется локальная среда выполнения интеграции на виртуальной машине Azure 

Независимо от того, выполняется ли перенос данных по частному каналу или требуется разрешить определенный диапазон IP-адресов в брандмауэре Amazon S3, необходимо установить локальную среду выполнения интеграции на виртуальной машине Windows Azure. 

- Рекомендуемая начальная конфигурация для каждой виртуальной машины Azure — Standard_D32s_v3 с 32 виртуальными ЦП и 128 ГБ памяти.  Вы можете отслеживать использование ЦП и памяти виртуальной машины среды выполнения интеграции во время переноса данных, чтобы узнать, нужно ли увеличить масштаб виртуальной машины для повышения производительности или уменьшить его, чтобы сократить затраты. 
- Кроме того, можно выполнить горизонтальное масштабирование, связав до 4 узлов виртуальных машин с одной локальной средой выполнения интеграции.  Одно задание копирования, выполняемое в локальной среде выполнения интеграции, автоматически разделяет набор файлов и использует все узлы виртуальных машин для параллельного копирования файлов.  Для обеспечения высокой доступности рекомендуется начать с двух узлов виртуальных машин, чтобы избежать единой точки отказа во время переноса данных. 

### <a name="rate-limiting"></a>Ограничение частоты 

Рекомендуется провести оценку производительности с помощью репрезентативного образца набора данных, чтобы можно было определить подходящий размер секции. 

Начните с одной секции и одного действия копирования с количеством единиц интеграции данных по умолчанию.  Постепенно увеличивайте количество единиц интеграции данных, пока не будет достигнуто ограничение пропускной способности и скорости ввода-вывода сети или хранилищ данных либо максимальное количество единиц интеграции данных (256), разрешенное для одного действия копирования. 

Затем постепенно увеличивайте количество одновременных действий копирования, пока не будет достигнут предел для вашей среды. 

При возникновении ошибок регулирования, о которых сообщает действие копирования Фабрики данных Azure, уменьшите степень параллелизма или количество единиц интеграции данных в Фабрике данных Azure либо повысьте предельную пропускную способность и скорость ввода-вывода сети и хранилищ данных.  

### <a name="estimating-price"></a>Оценка цены 

> [!NOTE]
> Это гипотетический пример цен.  Реальная цена зависит от фактической пропускной способности в среде.

Рассмотрим следующий конвейер, созданный для переноса данных из S3 в Хранилище BLOB-объектов Azure: 

![pricing-pipeline](media/data-migration-guidance-s3-to-azure-storage/pricing-pipeline.png)

Предположим следующее. 

- Общий объем данных составляет 2 ПБ. 
- Данные переносятся по протоколу HTTPS с помощью первой архитектуры. 
- 2 ПБ делятся на секции по 1 КБ, и каждая операция копирования перемещает одну секцию. 
- Для каждой копии настраивается 256 единиц интеграции данных и достигается пропускная способность 1 Гбит/с. 
- Степень параллелизма ForEach имеет значение 2, а суммарная пропускная способность — 2 Гбит/с 
- В итоге для завершения переноса потребуется 292 часа. 

Ниже приведена оценочная цена на основе указанных выше допущений: 

![pricing-table](media/data-migration-guidance-s3-to-azure-storage/pricing-table.png)

### <a name="additional-references"></a>Дополнительные ссылки 
- [Соединитель Amazon Simple Storage Service](https://docs.microsoft.com/azure/data-factory/connector-amazon-simple-storage-service)
- [Соединитель хранилища BLOB-объектов Azure](https://docs.microsoft.com/azure/data-factory/connector-azure-blob-storage)
- [Copy data to or from Azure Data Lake Storage Gen2 Preview using Azure Data Factory (Preview)](https://docs.microsoft.com/azure/data-factory/connector-azure-data-lake-storage) (Копирование данных в Azure Data Lake Storage Gen2 (предварительная версия) или из него с помощью фабрики данных Azure)
- [Руководство по настройке производительности действия копирования](https://docs.microsoft.com/azure/data-factory/copy-activity-performance)
- [Создание и настройка локальной среды выполнения интеграции](https://docs.microsoft.com/azure/data-factory/create-self-hosted-integration-runtime)
- [Высокая доступность и масштабируемость локальной среды выполнения интеграции](https://docs.microsoft.com/azure/data-factory/create-self-hosted-integration-runtime#high-availability-and-scalability)
- [Вопросы безопасности при перемещении данных](https://docs.microsoft.com/azure/data-factory/data-movement-security-considerations)
- [Хранение учетных данных в Azure Key Vault](https://docs.microsoft.com/azure/data-factory/store-credentials-in-key-vault)
- [Добавочное копирование файлов на основе имен файлов с разделением по времени](https://docs.microsoft.com/azure/data-factory/tutorial-incremental-copy-partitioned-file-name-copy-data-tool)
- [Копирование новых и измененных файлов на основе LastModifiedDate](https://docs.microsoft.com/azure/data-factory/tutorial-incremental-copy-lastmodified-copy-data-tool)
- [Страница цен на Фабрику данных Azure](https://azure.microsoft.com/pricing/details/data-factory/data-pipeline/)

## <a name="template"></a>Шаблон

Существует [шаблон](solution-template-migration-s3-azure.md), с помощью которого можно приступить к переносу сотен миллионов файлов, общий объем которых измеряется петабайтами, из Amazon S3 в Azure Data Lake Storage 2-го поколения.

## <a name="next-steps"></a>Дальнейшие действия

- [Копирование файлов из нескольких контейнеров с помощью Фабрики данных Azure](solution-template-copy-files-multiple-containers.md)
