---
title: Описание кластера с помощью диспетчер ресурсов кластера
description: Опишите кластер Service Fabric, указав домены сбоя, домены обновления, свойства узлов и емкость узлов для диспетчер ресурсов кластера.
author: masnider
ms.topic: conceptual
ms.date: 08/18/2017
ms.author: masnider
ms.custom: devx-track-csharp
ms.openlocfilehash: 5ec5db2b2fefeba3bffb7e30a77850b30dccf95e
ms.sourcegitcommit: 419cf179f9597936378ed5098ef77437dbf16295
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 08/27/2020
ms.locfileid: "89005638"
---
# <a name="describe-a-service-fabric-cluster-by-using-cluster-resource-manager"></a>Описание кластера Service Fabric с помощью диспетчер ресурсов кластера
Функция диспетчер ресурсов кластера Service Fabric Azure предоставляет несколько механизмов описания кластера.

* Домены сбоя
* Домены обновления
* Свойства узла
* Емкость узла

Во время выполнения диспетчер ресурсов кластера использует эти сведения для обеспечения высокой доступности служб, работающих в кластере. При соблюдении этих важных правил также пытается оптимизировать потребление ресурсов в кластере.

## <a name="fault-domains"></a>Домены сбоя
Домен сбоя — это любая область координированного сбоя. Один компьютер является доменом сбоя. По различным причинам это может привести к сбою, от сбоев источника питания до того, что неполадки вызваны дефектом сетевого адаптера. 

Компьютеры, подключенные к одному коммутатору Ethernet, находятся в одном домене сбоя. Итак, компьютеры, которые совместно используют один источник питания или находятся в одном месте. 

Так как это естественным образом перекрытие аппаратных сбоев, домены сбоя являются иерархическими. Они представлены в виде URI в Service Fabric.

Важно правильно настроить домены сбоя, так как Service Fabric использует эти сведения для безопасного размещения служб. Service Fabric не хочет размещать службы таким, что утрата домена сбоя (вызванного сбоем какого-либо компонента) приведет к сбою службы. 

В среде Azure Service Fabric использует сведения о домене сбоя, предоставленные средой, для правильной настройки узлов в кластере от вашего имени. Для изолированных экземпляров Service Fabric домены сбоя определяются во время настройки кластера. 

> [!WARNING]
> Важно, чтобы сведения о домене сбоя, предоставленные Service Fabric, были точными. Например, предположим, что узлы кластера Service Fabric работают на 10 виртуальных машинах, работающих на 5 физических узлах. В этом случае, даже если используется 10 виртуальных машин, имеется только 5 разных доменов сбоя (верхнего уровня). Совместное использование одного физического узла приводит к тому, что виртуальные машины совместно используют один и тот же корневой домен сбоя, так как виртуальные машины сталкиваются со сбоем в случае сбоя физического узла.  
>
> Service Fabric ждет, что домен сбоя узла не изменится. Другие механизмы обеспечения высокой доступности виртуальных машин, таких как [Ha-VM](/previous-versions/system-center/virtual-machine-manager-2008-r2/cc967323(v=technet.10)), могут вызвать конфликты с Service Fabric. Эти механизмы используют прозрачную миграцию виртуальных машин с одного узла на другой. Они не перестраивают или не уведомляют выполняющийся код в виртуальной машине. Таким образом, они *не поддерживаются* в качестве сред для запуска кластеров Service Fabric. 
>
> Единственной применяемой технологией обеспечения высокого уровня доступности должна быть система Service Fabric. Такие механизмы, как динамическая миграция виртуальных машин и San, не являются обязательными. Если эти механизмы используются совместно с Service Fabric, они _снижают_ доступность и надежность приложений. Причина заключается в том, что они предоставляют дополнительную сложность, добавляют централизованные источники сбоев и используют стратегии надежности и доступности, которые конфликтуют с Service Fabric. 
>
>

На следующем рисунке мы раскрасем все сущности, которые вносят вклад в домены сбоя, и перечислите все домены сбоя, которые привели к этому результату. В этом примере у нас есть центры обработки данных (DC), стойки (R) и колонки (B). Если в каждой колонке содержится несколько виртуальных машин, в иерархии доменов сбоя может быть еще один слой.

<center>

![Узлы, организованные через домены сбоя][Image1]
</center>

Во время выполнения Service Fabric кластера диспетчер ресурсов считает домены сбоя в макетах кластера и планов. Реплики с отслеживанием состояния или экземпляры без отслеживания состояния для службы распределяются, так что они находятся в разных доменах сбоя. Распределение службы между доменами сбоя гарантирует, что доступность службы не будет скомпрометирована при сбое домена сбоя на любом уровне иерархии.

Диспетчер ресурсов кластера не имеет значения, сколько уровней находится в иерархии доменов сбоя. Он пытается гарантировать, что потери одной части иерархии не повлияли на службы, работающие в ней. 

Лучше, если одинаковое количество узлов находится на каждом уровне глубины в иерархии доменов сбоя. Если "дерево" доменов сбоя не сбалансировано в кластере, диспетчер ресурсов кластера сложнее, чтобы определить оптимальное распределение служб. Несбалансированные макеты доменов сбоя означают, что потери некоторых доменов влияют на доступность служб больше, чем другие домены. В результате кластерный диспетчер ресурсов передается между двумя целями: 

* Он хочет использовать компьютеры в этом "тяжелом" домене, размещая на них службы. 
* Он хочет разместить службы в других доменах, чтобы потери домена не вызывали проблем. 

Как выглядят несбалансированные домены? На следующей схеме показаны две различные структуры кластеров. В первом примере узлы распределяются равномерно по доменам сбоя. Во втором примере у одного домена сбоя имеется больше узлов, чем в других доменах сбоя. 

<center>

![Две различные структуры кластеров][Image2]
</center>

В Azure выбор домена сбоя, который содержит узел, является управляемым. Но в зависимости от количества узлов, которые вы подготовили, вы по-прежнему можете использовать домены сбоя, в которых больше узлов, чем в других. 

Например, предположим, что в кластере имеется пять доменов сбоя, но для типа узла (**NodeType**) подготавливаются семь узлов. В этом случае в первых двух доменах сбоя будет больше узлов. Если продолжить развертывание других экземпляров **NodeType** только с несколькими экземплярами, проблема становится хуже. По этой причине рекомендуется, чтобы количество узлов в каждом типе узла было кратно количеству доменов сбоя.

## <a name="upgrade-domains"></a>Домены обновления
Домены обновления — это еще одна функция, которая помогает Service Fabric кластеру диспетчер ресурсов понять структуру кластера. Домены обновления определяют наборы узлов, которые обновляются одновременно. Домены обновления помогают кластеру диспетчер ресурсов понимать и координировать операции управления, такие как обновления.

Домены обновления во многом подобны доменам сбоя, но с несколькими ключевыми отличиями. Во первых, области координированных сбоев оборудования определяют домены сбоя. Домены обновления, с другой стороны, определяются политикой. Вы сможете решить, сколько нужно, а не позволить среде задиктовать число. В качестве узлов можно использовать столько доменов обновления. Еще одно различие между доменами сбоя и доменами обновления заключается в том, что домены обновления не являются иерархическими. Вместо этого они больше подобны простому тегу. 

На следующей схеме показаны три домена обновления, чередующиеся по трем доменам сбоя. Здесь также показано одно возможное размещение для трех различных реплик службы с отслеживанием состояния, где каждая из них завершается в разных доменах сбоя и обновления. Такое размещение позволяет избежать потери домена сбоя в процессе обновления службы и по-прежнему иметь одну копию кода и данных.  

<center>

![Размещение с доменами сбоя и обновления][Image3]
</center>

Существует множество достоинств и недостатков для большого числа доменов обновления. Несколько доменов обновления означают, что каждый шаг обновления более детализирован и влияет на меньшее количество узлов или служб. Меньшее количество служб приходится перемещать за раз, представляя меньше изменений в систему. Это повышает надежность, так как на все проблемы, появившиеся во время обновления, влияет меньшая часть службы. Дополнительные домены обновления также означают, что для решения влияния обновления требуется меньше доступного буфера на других узлах. 

Например, если имеется пять доменов обновления, узлы в каждом обрабатывают примерно 20% трафика. Если необходимо отработать этот домен обновления для обновления, обычно требуется выполнить загрузку в любое место. Так как у вас есть четыре оставшихся домена обновления, каждый из них должен иметь место около 25% общего трафика. Дополнительные домены обновления означают, что на узлах кластера требуется меньшее количество буферов.

Рассмотрите возможность использования 10 доменов обновления. В этом случае каждый домен обновления будет обрабатывать только около 10 процентов от общего трафика. При выполнении шага обновления в кластере каждый домен должен иметь место только около 11% общего трафика. Другие домены обновления обычно позволяют запускать узлы с более высоким уровнем использования, так как требуется меньше зарезервированных ресурсов. Это справедливо и для доменов сбоя.  

Недостаток нескольких доменов обновления состоит в том, что обновления обычно занимают больше времени. Service Fabric ожидает короткий период после завершения домена обновления и выполняет проверки перед началом обновления следующего. Эти задержки позволяют обнаружить проблемы, вызванные обновлением, прежде чем обновление будет продолжено. Возникающие при этом негативные последствия допустимы, так как если выбран этот вариант, то неверные изменения не влияют слишком сильно на слишком большую часть службы в определенный момент времени.

Наличие слишком большого количества доменов обновления имеет много негативных побочных эффектов. При отключении и обновлении каждого домена обновления большая часть общей емкости становится недоступной. Например, если у вас есть только три домена обновления, в каждый момент времени вы занимаетесь третьим числом общей емкости службы или кластера. Наличие подобной службы в любой момент не желательно, так как в остальной части кластера требуется достаточная емкость, чтобы справиться с рабочей нагрузкой. Поддержание этого буфера означает, что во время нормальной работы эти узлы будут менее загружены, чем в противном случае. Это увеличивает затраты на выполнение службы.

Фактических ограничений на общее количество доменов сбоя или обновления в среде либо же на то, как они перекрываются, не существует. Но существуют распространенные шаблоны:

- Домены сбоя и домены обновления, сопоставленные 1:1
- Один домен обновления на узел (физический или виртуальный экземпляр ОС)
- Модель "чередования" или "Матрица", в которой домены сбоя и домены обновления формируют матрицу с компьютерами, которые обычно работают по диагонали

<center>

![Структуры доменов сбоя и обновления][Image4]
</center>

Нет лучшего ответа, для которого нужно выбрать макет. У каждого варианта есть преимущества и недостатки. Например, модель "1 домен сбоя — 1 домен обновления" отличается простотой настройки. Модель одного домена обновления на каждую модель узла наиболее похожа на то, к чему относятся пользователи. Во время обновления каждый узел обновляется независимо друг от друга. Это аналогично тому, как раньше вручную обновлялись небольшие наборы компьютеров.

Наиболее распространенная модель — это матрица ДЕМОна/обновления, в которой домены сбоя и домены обновления формируют таблицу и узлы, начиная по диагонали. Это модель, используемая по умолчанию в кластерах Service Fabric в Azure. Для кластеров с большим количеством узлов все в итоге выглядит как плотный шаблон матрицы.

> [!NOTE]
> Service Fabric кластеры, размещенные в Azure, не поддерживают изменение стратегии по умолчанию. Только автономные кластеры предлагают эту настройку.
>

## <a name="fault-and-upgrade-domain-constraints-and-resulting-behavior"></a>Ограничения доменов сбоя и обновления и соответствующее поведение
### <a name="default-approach"></a>Стандартный подход
По умолчанию кластер диспетчер ресурсов поддерживает распределение служб между доменами сбоя и обновления. Это моделируется как [ограничение](service-fabric-cluster-resource-manager-management-integration.md). Ограничение для доменов сбоя и обновления: "для конкретного раздела службы не должно быть разницы больше единицы в количестве объектов службы (экземпляры службы без отслеживания состояния или реплики службы с отслеживанием состояния) между двумя доменами на одном уровне иерархии".

Предположим, что это ограничение предоставляет гарантию «максимальная разница». Ограничение для доменов сбоя и обновления предотвращает определенные перемещения или упорядочения, нарушающие правило.

Например, предположим, что у нас есть кластер с шестью узлами, настроенный с пятью доменами сбоя и пятью доменами обновления.

|  | ДС0 | ДС1 | ДС2 | ДС3 | ДС4 |
| --- |:---:|:---:|:---:|:---:|:---:|
| **ДО0** |У1 | | | | |
| **ДО1** |У6 |У2 | | | |
| **UD2** | | |У3 | | |
| **ДО3** | | | |У4 | |
| **ДО4** | | | | |У5 |

Теперь предположим, что мы создаем службу с **TargetReplicaSetSize** (или для службы без отслеживания состояния, **InstanceCount**) со значением 5. Реплики размещаются на узлах У1-У5. Узел У6 фактически никогда не используется, вне зависимости от количества создаваемых служб. Но почему? Давайте рассмотрим разницу между текущей структурой и тем, что произошло бы, если бы мы выбрали У6.

Вот макет, который мы получили, и общее число реплик на домен сбоя и обновления:

|  | ДС0 | ДС1 | ДС2 | ДС3 | ДС4 | Всего ДО |
| --- |:---:|:---:|:---:|:---:|:---:|:---:|
| **ДО0** |Р1 | | | | |1 |
| **ДО1** | |R2 | | | |1 |
| **UD2** | | |Р3 | | |1 |
| **ДО3** | | | |Р4 | |1 |
| **ДО4** | | | | |Р5 |1 |
| **Всего ДС** |1 |1 |1 |1 |1 |- |

Этот макет сбалансирован с точки зрения узлов в домене сбоя и домене обновления. Он также сбалансирован с точки зрения количества реплик на домен сбоя и обновления. На каждый домен приходится одинаковое количество узлов и реплик.

Теперь давайте посмотрим, что произошло бы, если бы вместо У2 мы использовали У6. Как бы тогда распределялись реплики?

|  | ДС0 | ДС1 | ДС2 | ДС3 | ДС4 | Всего ДО |
| --- |:---:|:---:|:---:|:---:|:---:|:---:|
| **ДО0** |Р1 | | | | |1 |
| **ДО1** |Р5 | | | | |1 |
| **UD2** | | |R2 | | |1 |
| **ДО3** | | | |Р3 | |1 |
| **ДО4** | | | | |Р4 |1 |
| **Всего ДС** |2 |0 |1 |1 |1 |- |

Этот макет нарушает определение гарантии "максимальное различие" для ограничения домена сбоя. НА ДС0 ПРИХОДИТСЯ имеет две реплики, в то время как ДС1 — имеет нуль. Разница между на ДС0 ПРИХОДИТСЯ и ДС1 — является суммой двух значений, которая больше, чем максимальная разница в единице. Так как ограничение нарушается, диспетчер ресурсов кластера не допускает такого упорядочения. Аналогично, если мы выбрали N2 и У6 (вместо N1 и N2), мы получаем:

|  | ДС0 | ДС1 | ДС2 | ДС3 | ДС4 | Всего ДО |
| --- |:---:|:---:|:---:|:---:|:---:|:---:|
| **ДО0** | | | | | |0 |
| **ДО1** |Р5 |Р1 | | | |2 |
| **UD2** | | |R2 | | |1 |
| **ДО3** | | | |Р3 | |1 |
| **ДО4** | | | | |Р4 |1 |
| **Всего ДС** |1 |1 |1 |1 |1 |- |

Этот макет сбалансирован с точки зрения доменов сбоя. Но теперь это нарушает ограничение домена обновления, так как ДО0 имеет нулевые реплики, а ДО1 имеет два. Этот макет также является недопустимым и не будет выбран в диспетчер ресурсов кластера.

Такой подход к распределению реплик состояния или экземпляров без отслеживания состояния обеспечивает наилучшую отказоустойчивость из возможных. Если один из доменов выйдет из строя, будет потеряно минимальное число реплик или экземпляров. 

С другой стороны, этот подход может не позволять кластеру использовать все ресурсы. Для некоторых конфигураций кластера нельзя использовать определенные узлы. Это может привести к тому, что Service Fabric не поместит службы, что приведет к появлению предупреждающих сообщений. В предыдущем примере не удается использовать некоторые узлы кластера (У6 в примере). Даже если вы добавили узлы в этот кластер (N7-N10), реплики и экземпляры будут помещены только на N1 – N5 из-за ограничений в доменах сбоя и обновления. 

|  | ДС0 | ДС1 | ДС2 | ДС3 | ДС4 |
| --- |:---:|:---:|:---:|:---:|:---:|
| **ДО0** |У1 | | | |У10 |
| **ДО1** |У6 |У2 | | | |
| **UD2** | |У7 |У3 | | |
| **ДО3** | | |У8 |У4 | |
| **ДО4** | | | |У9 |У5 |



### <a name="alternative-approach"></a>Альтернативный подход

Кластерный диспетчер ресурсов поддерживает еще одну версию ограничения для доменов сбоя и обновления. Она позволяет разместить и при этом гарантировать минимальный уровень безопасности. Альтернативное ограничение можно указать следующим образом: "для данного раздела службы распределение реплики между доменами должно гарантировать, что Секция не будет иметь потери кворума". Предположим, что это ограничение предоставляет гарантию кворума "Сейф". 

> [!NOTE]
> Для службы с отслеживанием состояния мы определяем *потерю кворума* в ситуации, когда большинство реплик секций отключаются одновременно. Например, если **TargetReplicaSetSize** имеет значение 5, набор всех трех реплик представляет кворум. Аналогично, если **TargetReplicaSetSize** имеет значение шесть, для кворума необходимы четыре реплики. В обоих случаях одновременно может быть отключено не более двух реплик, если Секция хочет продолжать работу нормально. 
>
> Для службы без отслеживания состояния не существует такой возможности, как *потери кворума*. Службы без отслеживания состояния продолжают работать в обычном режиме, даже если большинство экземпляров работают одновременно. Итак, в оставшейся части этой статьи мы рассмотрим службы с отслеживанием состояния.
>

Вернемся к предыдущему примеру. При использовании версии ограничения "надежность кворума" все три макета будут допустимыми. Даже если на ДС0 ПРИХОДИТСЯ не удалось выполнить во втором макете или ДО1 в третьем макете, Секция по-прежнему будет иметь кворум. (Большинство реплик будет по-прежнему работать.) В этой версии ограничения У6 можно использовать почти всегда.

Подход "надежный кворум" обеспечивает большую гибкость, чем подход "максимальная разница". Причина заключается в том, что проще найти распределения реплик, которые действительны почти в любой топологии кластера. Однако этот подход не может гарантировать лучшие характеристики отказоустойчивости, так как некоторые сбои хуже других. 

В худшем случае большинство реплик могут быть потеряны из-за сбоя одного домена и одной дополнительной реплики. Например, вместо трех сбоев, необходимых для потери кворума с пятью репликами или экземплярами, теперь можно потерять большинство с двумя сбоями. 

### <a name="adaptive-approach"></a>Адаптивный подход
Так как оба подхода имеют сильные стороны и недостатки, мы предоставили адаптивный подход, объединяющий эти две стратегии.

> [!NOTE]
> Это поведение по умолчанию, начинающееся с Service Fabric версии 6,2. 
> 
> Адаптивный подход по умолчанию использует логику "максимальной разницы", а при необходимости переключается на логику "сохранения кворума". Кластер диспетчер ресурсов автоматически определяет, какая стратегия необходима, просматривая настройку кластера и служб.
> 
> Кластерный диспетчер ресурсов должен использовать логику на основе кворума для службы оба эти условия имеют значение true:
>
> * **TargetReplicaSetSize** для службы равномерно делится на число доменов сбоя и число доменов обновления.
> * Количество узлов меньше или равно числу доменов сбоя, умноженному на число доменов обновления.
>
> Помните, что кластер диспетчер ресурсов будет использовать этот подход как для служб с отслеживанием состояния, так и для службы с отслеживанием состояния, даже если потери кворума не важны для служб без отслеживания состояния.

Вернемся к предыдущему примеру и предположим, что кластер теперь имеет восемь узлов. Кластер по-прежнему настроен с пятью доменами сбоя и пятью доменами обновления, а значение **TargetReplicaSetSize** службы, размещенной в этом кластере, остается пятью. 

|  | ДС0 | ДС1 | ДС2 | ДС3 | ДС4 |
| --- |:---:|:---:|:---:|:---:|:---:|
| **ДО0** |У1 | | | | |
| **ДО1** |У6 |У2 | | | |
| **UD2** | |У7 |У3 | | |
| **ДО3** | | |У8 |У4 | |
| **ДО4** | | | | |У5 |

Так как все необходимые условия удовлетворены, кластер диспетчер ресурсов будет использовать логику на основе кворума при распределении службы. Это обеспечивает использование У6-N8. Одно из возможных распределений служб в этом случае может выглядеть следующим образом:

|  | ДС0 | ДС1 | ДС2 | ДС3 | ДС4 | Всего ДО |
| --- |:---:|:---:|:---:|:---:|:---:|:---:|
| **ДО0** |Р1 | | | | |1 |
| **ДО1** |R2 | | | | |1 |
| **UD2** | |Р3 |Р4 | | |2 |
| **ДО3** | | | | | |0 |
| **ДО4** | | | | |Р5 |1 |
| **Всего ДС** |2 |1 |1 |0 |1 |- |

Если значение **TargetReplicaSetSize** службы уменьшилось до четырех (например), кластер диспетчер ресурсов заметит это изменение. Возобновится использование логики "максимальная разница", так как **TargetReplicaSetSize** не может быть больше, чем число доменов сбоя и доменов обновления. В результате будут выполнены определенные перемещения реплики для распределения оставшихся четырех реплик на узлах N1-N5. Таким образом, версия "максимальная разница" для логики домена сбоя и домена обновления не нарушает работу. 

В предыдущем макете, если значение **TargetReplicaSetSize** равно 5, а N1 удаляется из кластера, число доменов обновления становится равным четырем. Опять же, диспетчер ресурсов кластера начинает использовать логику "максимальная разница", поскольку количество доменов обновления больше не делит значение **TargetReplicaSetSize** службы. В результате реплика R1, при ее повторном создании, должна направляться на N4, чтобы ограничение для домена сбоя и обновления не нарушалось.

|  | ДС0 | ДС1 | ДС2 | ДС3 | ДС4 | Всего ДО |
| --- |:---:|:---:|:---:|:---:|:---:|:---:|
| **ДО0** |Н/Д |Н/Д |Н/Д |Н/Д |Н/Д |Н/Д |
| **ДО1** |R2 | | | | |1 |
| **UD2** | |Р3 |Р4 | | |2 |
| **ДО3** | | | |Р1 | |1 |
| **ДО4** | | | | |Р5 |1 |
| **Всего ДС** |1 |1 |1 |1 |1 |- |

## <a name="configuring-fault-and-upgrade-domains"></a>Настройка доменов сбоя и обновления
В развертываниях Service Fabric, размещенных в Azure, домены сбоя и домены обновления определяются автоматически. Service Fabric просто извлекает и использует сведения о среде из Azure.

Если вы создаете собственный кластер (или хотите запустить определенную топологию в разработке), вы можете самостоятельно предоставить сведения о домене сбоя и домене обновления. В этом примере мы определим кластер локальной разработки из девяти узлов, охватывающий три центра обработки данных (каждый с тремя стойками). В этом кластере также есть три домена обновления, чередующиеся по трем центрам обработки данных. Ниже приведен пример конфигурации в ClusterManifest.xml.

```xml
  <Infrastructure>
    <!-- IsScaleMin indicates that this cluster runs on one box/one single server -->
    <WindowsServer IsScaleMin="true">
      <NodeList>
        <Node NodeName="Node01" IPAddressOrFQDN="localhost" NodeTypeRef="NodeType01" FaultDomain="fd:/DC01/Rack01" UpgradeDomain="UpgradeDomain1" IsSeedNode="true" />
        <Node NodeName="Node02" IPAddressOrFQDN="localhost" NodeTypeRef="NodeType02" FaultDomain="fd:/DC01/Rack02" UpgradeDomain="UpgradeDomain2" IsSeedNode="true" />
        <Node NodeName="Node03" IPAddressOrFQDN="localhost" NodeTypeRef="NodeType03" FaultDomain="fd:/DC01/Rack03" UpgradeDomain="UpgradeDomain3" IsSeedNode="true" />
        <Node NodeName="Node04" IPAddressOrFQDN="localhost" NodeTypeRef="NodeType04" FaultDomain="fd:/DC02/Rack01" UpgradeDomain="UpgradeDomain1" IsSeedNode="true" />
        <Node NodeName="Node05" IPAddressOrFQDN="localhost" NodeTypeRef="NodeType05" FaultDomain="fd:/DC02/Rack02" UpgradeDomain="UpgradeDomain2" IsSeedNode="true" />
        <Node NodeName="Node06" IPAddressOrFQDN="localhost" NodeTypeRef="NodeType06" FaultDomain="fd:/DC02/Rack03" UpgradeDomain="UpgradeDomain3" IsSeedNode="true" />
        <Node NodeName="Node07" IPAddressOrFQDN="localhost" NodeTypeRef="NodeType07" FaultDomain="fd:/DC03/Rack01" UpgradeDomain="UpgradeDomain1" IsSeedNode="true" />
        <Node NodeName="Node08" IPAddressOrFQDN="localhost" NodeTypeRef="NodeType08" FaultDomain="fd:/DC03/Rack02" UpgradeDomain="UpgradeDomain2" IsSeedNode="true" />
        <Node NodeName="Node09" IPAddressOrFQDN="localhost" NodeTypeRef="NodeType09" FaultDomain="fd:/DC03/Rack03" UpgradeDomain="UpgradeDomain3" IsSeedNode="true" />
      </NodeList>
    </WindowsServer>
  </Infrastructure>
```

В этом примере для автономных развертываний используется ClusterConfig.js.

```json
"nodes": [
  {
    "nodeName": "vm1",
    "iPAddress": "localhost",
    "nodeTypeRef": "NodeType0",
    "faultDomain": "fd:/dc1/r0",
    "upgradeDomain": "UD1"
  },
  {
    "nodeName": "vm2",
    "iPAddress": "localhost",
    "nodeTypeRef": "NodeType0",
    "faultDomain": "fd:/dc1/r0",
    "upgradeDomain": "UD2"
  },
  {
    "nodeName": "vm3",
    "iPAddress": "localhost",
    "nodeTypeRef": "NodeType0",
    "faultDomain": "fd:/dc1/r0",
    "upgradeDomain": "UD3"
  },
  {
    "nodeName": "vm4",
    "iPAddress": "localhost",
    "nodeTypeRef": "NodeType0",
    "faultDomain": "fd:/dc2/r0",
    "upgradeDomain": "UD1"
  },
  {
    "nodeName": "vm5",
    "iPAddress": "localhost",
    "nodeTypeRef": "NodeType0",
    "faultDomain": "fd:/dc2/r0",
    "upgradeDomain": "UD2"
  },
  {
    "nodeName": "vm6",
    "iPAddress": "localhost",
    "nodeTypeRef": "NodeType0",
    "faultDomain": "fd:/dc2/r0",
    "upgradeDomain": "UD3"
  },
  {
    "nodeName": "vm7",
    "iPAddress": "localhost",
    "nodeTypeRef": "NodeType0",
    "faultDomain": "fd:/dc3/r0",
    "upgradeDomain": "UD1"
  },
  {
    "nodeName": "vm8",
    "iPAddress": "localhost",
    "nodeTypeRef": "NodeType0",
    "faultDomain": "fd:/dc3/r0",
    "upgradeDomain": "UD2"
  },
  {
    "nodeName": "vm9",
    "iPAddress": "localhost",
    "nodeTypeRef": "NodeType0",
    "faultDomain": "fd:/dc3/r0",
    "upgradeDomain": "UD3"
  }
],
```

> [!NOTE]
> При определении кластеров с помощью Azure Resource Manager Azure назначает домены сбоя и домены обновления. Поэтому определение типов узлов и масштабируемых наборов виртуальных машин в шаблоне Azure Resource Manager не содержит сведений о домене сбоя или домене обновления.
>

## <a name="node-properties-and-placement-constraints"></a>Свойства узлов и ограничения размещения
Иногда (в большинстве случаев) необходимо убедиться, что определенные рабочие нагрузки выполняются только для определенных типов узлов в кластере. Например, для некоторых рабочих нагрузок может потребоваться GPU или SSDs, а другие — нет. 

Хорошим примером нацеливания оборудования на конкретные рабочие нагрузки является почти каждая n-уровневая архитектура. Некоторые компьютеры служат в качестве интерфейсной или серверной стороны приложения и предоставляются клиентам или Интернету. Другие компьютеры (часто с разными аппаратными ресурсами) обрабатывают рабочие нагрузки уровней вычислений и хранилища. Как правило, они _не_ предоставляются непосредственно клиентам или в Интернете. 

Service Fabric ожидать, что в некоторых случаях может потребоваться выполнение определенных рабочих нагрузок в определенных конфигурациях оборудования. Пример:

* Существующее n-уровневое приложение было "перенесено и перемещено" в Service Fabricную среду.
* Рабочая нагрузка должна выполняться на определенном оборудовании для повышения производительности, масштабирования или изоляции безопасности.
* Рабочая нагрузка должна быть изолирована от других рабочих нагрузок для политики или использования ресурсов.

Для поддержки таких видов конфигураций Service Fabric содержит теги, которые можно применять к узлам. Эти теги называются *свойствами узла*. *Ограничения на размещение* — это инструкции, прикрепленные к отдельным службам, выбранным для одного или нескольких свойств узла. Ограничения размещения определяют, где должны запускаться службы. Набор ограничений является расширяемым. Любая пара "ключ-значение" может работать. 

<center>

![Различные рабочие нагрузки для макета кластера][Image5]
</center>

### <a name="built-in-node-properties"></a>Свойства встроенных узлов
Service Fabric определяет некоторые свойства узла по умолчанию, которые могут использоваться автоматически, поэтому их не нужно определять. Свойствами по умолчанию, определенными на каждом узле, являются **NodeType** и **nodeName**. 

Например, можно написать ограничение на размещение как `"(NodeType == NodeType03)"` . **NodeType** — это часто используемое свойство. Это полезно, так как оно соответствует 1:1 с типом компьютера. Каждый тип компьютера соответствует типу рабочей нагрузки в традиционном n-уровневом приложении.

<center>

![Ограничения размещения и свойства узла][Image6]
</center>

## <a name="placement-constraints-and-node-property-syntax"></a>Ограничения на размещение и синтаксис свойства Node 
Значение, указанное в свойстве Node, может быть строкой, логическим или длинным знаком. Инструкция в службе называется *ограничением* на размещение, так как оно ограничивает возможности запуска службы в кластере. Ограничение может быть любой логической инструкцией, которая работает со свойствами узла в кластере. Допустимые селекторы в следующих логических инструкциях:

* Условные проверки для создания определенных инструкций:

  | . | Синтаксис |
  | --- |:---:|
  | "равно" | "==" |
  | "не равно" | "!=" |
  | "больше" | ">" |
  | "больше или равно" | ">=" |
  | "меньше" | "<" |
  | "меньше или равно" | "<=" |

* Логические инструкции для группирования и логических операций:

  | . | Синтаксис |
  | --- |:---:|
  | "и" | "&&" |
  | "или" | "&#124;&#124;" |
  | "не" | "!" |
  | "группа как отдельный оператор" | "()" |

Ниже приведены некоторые примеры основных операторов ограничений.

  * `"Value >= 5"`
  * `"NodeColor != green"`
  * `"((OneProperty < 100) || ((AnotherProperty == false) && (OneProperty >= 100)))"`

Служба может быть размещена только на тех узлах, где оператор ограничения размещения принимает общее значение True. Узлы, для которых не определено свойство, не соответствуют ограничению размещения, содержащему свойство.

Предположим, что для типа узла в ClusterManifest.xml были определены следующие свойства узла:

```xml
    <NodeType Name="NodeType01">
      <PlacementProperties>
        <Property Name="HasSSD" Value="true"/>
        <Property Name="NodeColor" Value="green"/>
        <Property Name="SomeProperty" Value="5"/>
      </PlacementProperties>
    </NodeType>
```

В следующем примере показаны свойства узла, определенные с помощью ClusterConfig.json для автономных развертываний или Template.jsдля кластеров, размещенных в Azure. 

> [!NOTE]
> В шаблоне Azure Resource Manager тип узла обычно является параметризованным. Он будет выглядеть `"[parameters('vmNodeType1Name')]"` , как, а не NodeType01.
>

```json
"nodeTypes": [
    {
        "name": "NodeType01",
        "placementProperties": {
            "HasSSD": "true",
            "NodeColor": "green",
            "SomeProperty": "5"
        },
    }
],
```

Для службы можно создать *ограничения* на размещение служб следующим образом.

```csharp
FabricClient fabricClient = new FabricClient();
StatefulServiceDescription serviceDescription = new StatefulServiceDescription();
serviceDescription.PlacementConstraints = "(HasSSD == true && SomeProperty >= 4)";
// Add other required ServiceDescription fields
//...
await fabricClient.ServiceManager.CreateServiceAsync(serviceDescription);
```

```PowerShell
New-ServiceFabricService -ApplicationName $applicationName -ServiceName $serviceName -ServiceTypeName $serviceType -Stateful -MinReplicaSetSize 3 -TargetReplicaSetSize 3 -PartitionSchemeSingleton -PlacementConstraint "HasSSD == true && SomeProperty >= 4"
```

Если все узлы NodeType01 являются допустимыми, можно также выбрать этот тип узла с ограничением `"(NodeType == NodeType01)"` .

Ограничения на размещение службы можно обновлять динамически во время выполнения. При необходимости можно переместить службу в кластере, добавить и удалить требования и т. д. Service Fabric гарантирует, что служба останется и доступна даже при внесении этих типов изменений.

```csharp
StatefulServiceUpdateDescription updateDescription = new StatefulServiceUpdateDescription();
updateDescription.PlacementConstraints = "NodeType == NodeType01";
await fabricClient.ServiceManager.UpdateServiceAsync(new Uri("fabric:/app/service"), updateDescription);
```

```PowerShell
Update-ServiceFabricService -Stateful -ServiceName $serviceName -PlacementConstraints "NodeType == NodeType01"
```

Ограничения на размещение указываются для каждого экземпляра именованной службы. Обновления всегда заменяют (перезаписывают) свойства, заданные ранее.

Определение кластера задает свойства узла. Для изменения свойств узла требуется обновить конфигурацию кластера. Для обновления свойств узла нужно перезапустить каждый затронутый узел, чтобы он сообщил о своих новых свойствах. Service Fabric управляет этими последовательными обновлениями.

## <a name="describing-and-managing-cluster-resources"></a>Описание и управление ресурсами кластера
Одна из важнейших задач любого оркестратора — помощь в управлении потреблением ресурсов в кластере. Управление кластерными ресурсами связано с несколькими аспектами. 

Во-первых, необходимо гарантировать, что компьютеры не будут перегружены. То есть нужно сделать так, чтобы на компьютерах не было запущено больше служб, чем они могут обрабатывать. 

Во вторых, существует балансировка и оптимизация, которые важны для эффективного выполнения служб. Экономичные или зависящие от производительности предложения служб не могут позволить некоторым узлам быть горячими, а другие — холодными. Критические узлы ведут к состязанию за ресурсы и низкой производительности. Холодные узлы представляют нерасходные ресурсы и увеличивают затраты. 

Service Fabric представляет ресурсы в виде *метрик*. Метрики — это любые логические или физические ресурсы, которые нужно описать в Service Fabric. Примерами метрик являются "WorkQueueDepth" или "MemoryInMb". Сведения о физических ресурсах, которые Service Fabric могут управлять узлами, см. в разделе [Управление ресурсами](service-fabric-resource-governance.md). Сведения о метриках по умолчанию, используемых кластерной диспетчер ресурсов и о настройке пользовательских метрик, см. в [этой статье](service-fabric-cluster-resource-manager-metrics.md).

Метрики отличаются от ограничений на размещение и свойств узла. Свойства узлов — это статические дескрипторы самих узлов. Метрики описывают ресурсы, которые имеются у узлов и используются службами при их запуске на узле. Свойство Node может быть **HasSSD** и может иметь значение true или false. Объем пространства, доступного на этом SSD, и объем памяти, потребляемой службами, будет такой же, как «DriveSpaceInMb». 

Так же, как и ограничения на размещение и свойства узла, Service Fabric диспетчер ресурсов кластера не понимает, что означают имена метрик. Имена метрик — это просто строки. Рекомендуется объявлять единицы как часть имен метрик, которые вы создаете, когда они могут быть неоднозначными.

## <a name="capacity"></a>Capacity
Если вы отключили *балансировку*ресурсов, Service Fabric диспетчер ресурсов кластера, будет гарантировать, что ни один из узлов не превысит его емкость. Управление превышением емкости возможно в том случае, если кластер не перегружен и рабочая нагрузка не превышает возможности любого узла. Емкость — это еще одно *ограничение* , которое кластер диспетчер ресурсов использует для понимания того, какая часть ресурса имеет узел. Оставшаяся емкость также отслеживается для кластера в целом. 

На уровне службы и емкость, и потребление выражаются в виде метрик. Например, метрика может быть "ClientConnections", а узел может иметь емкость "ClientConnections" для 32 768. Другие узлы могут иметь другие ограничения. Служба, работающая на этом узле, может сказать, что в настоящее время потребляется 32 256 метрики "ClientConnections".

Во время выполнения кластер диспетчер ресурсов отслеживает оставшуюся емкость в кластере и на узлах. Для наблюдения за емкостью кластерный диспетчер ресурсов вычитает использование каждой службы из емкости узла, где выполняется служба. С помощью этой информации кластер диспетчер ресурсов может выяснить, куда поместить или переместить реплики, чтобы узлы не переходят за пределы емкости.

<center>

![Узлы и емкость кластера][Image7]
</center>

```csharp
StatefulServiceDescription serviceDescription = new StatefulServiceDescription();
ServiceLoadMetricDescription metric = new ServiceLoadMetricDescription();
metric.Name = "ClientConnections";
metric.PrimaryDefaultLoad = 1024;
metric.SecondaryDefaultLoad = 0;
metric.Weight = ServiceLoadMetricWeight.High;
serviceDescription.Metrics.Add(metric);
await fabricClient.ServiceManager.CreateServiceAsync(serviceDescription);
```

```PowerShell
New-ServiceFabricService -ApplicationName $applicationName -ServiceName $serviceName -ServiceTypeName $serviceTypeName –Stateful -MinReplicaSetSize 3 -TargetReplicaSetSize 3 -PartitionSchemeSingleton –Metric @("ClientConnections,High,1024,0)
```

Можно просмотреть емкость, определенную в манифесте кластера. Ниже приведен пример для ClusterManifest.xml.

```xml
    <NodeType Name="NodeType03">
      <Capacities>
        <Capacity Name="ClientConnections" Value="65536"/>
      </Capacities>
    </NodeType>
```

Ниже приведен пример емкости, определенных с помощью ClusterConfig.jsдля автономных развертываний или Template.jsдля кластеров, размещенных в Azure: 

```json
"nodeTypes": [
    {
        "name": "NodeType03",
        "capacities": {
            "ClientConnections": "65536",
        }
    }
],
```

Нагрузка службы часто изменяется динамически. Предположим, что нагрузка реплики "ClientConnections" изменилась с 1 024 на 2 048. Узел, на котором он выполнялся, имел емкость только 512 оставшихся для этой метрики. Теперь эта реплика или размещение экземпляра являются недопустимыми, так как на этом узле недостаточно места. Кластерный диспетчер ресурсов должен вернуть узел ниже емкости. Он сокращает нагрузку на узел, который превышает емкость, перемещая одну или несколько реплик или экземпляров из этого узла на другие узлы. 

Кластерный диспетчер ресурсов пытается максимально сэкономить затраты на перемещение реплик. Вы можете узнать больше о [стоимости перемещения](service-fabric-cluster-resource-manager-movement-cost.md) и о [стратегиях и правилах ребалансировки](service-fabric-cluster-resource-manager-metrics.md).

## <a name="cluster-capacity"></a>Емкость кластера
Как кластер Service Fabric диспетчер ресурсов, чтобы общий кластер оставался слишком полным? С динамической нагрузкой это не имеет большого количества операций. Службы могут загружает нагрузку независимо от действий, которые выполняет диспетчер ресурсов кластера. В результате ваш кластер с большим запасом на сегодняшний день может быть недостаточным, если у вас будет пик завтра. 

Элементы управления в кластерной диспетчер ресурсов помогают предотвратить возникновение проблем. Первое, что можно сделать, — предотвратить создание новых рабочих нагрузок, которые приведут к заполнению кластера.

Предположим, что вы создаете службу без отслеживания состояния и с ней связана определенная нагрузка. Служба заботится о метрике "DiskSpaceInMb". Служба будет использовать пять единиц "DiskSpaceInMb" для каждого экземпляра службы. Вы хотите создать три экземпляра службы. Это означает, что в кластере требуется 15 единиц "DiskSpaceInMb", чтобы вы даже создали эти экземпляры службы.

Кластерная диспетчер ресурсов непрерывно вычисляет емкость и потребление каждой метрики, чтобы она могла определить оставшуюся емкость в кластере. Если места недостаточно, кластер диспетчер ресурсов отклоняет вызов для создания службы.

Так как требуется только 15 единиц, можно выделить это пространство различными способами. Например, может существовать одна оставшаяся единица емкости на 15 разных узлах или три оставшиеся единицы емкости на пяти разных узлах. Если кластер диспетчер ресурсов может изменить расположение объектов, чтобы на трех узлах были доступны пять единиц, он помещает службу. Реорганизация кластера невозможна, если кластер почти полностью заполнен или существующие службы по какой-либо причине нельзя объединить. В остальных случаях она, как правило, возможна.

## <a name="buffered-capacity"></a>Буферизованная емкость
Буферизованная емкость — это еще одна функция диспетчер ресурсов кластера. Она позволяет резервировать некоторую часть общей емкости узла. Этот буфер емкости используется только для размещения служб во время обновлений и сбоев узлов. 

Буферизованная емкость указывается глобально на метрику для всех узлов. Значение, выбранное для зарезервированной емкости, является функцией числа доменов сбоя и обновления в кластере. Дополнительные домены сбоя и обновления означают, что можно выбрать меньшее значение для буферизованной емкости. Чем больше доменов, тем меньшее количество кластеров будет недоступным во время обновлений и сбоев. Указание емкости с буферизацией имеет смысл только в том случае, если для метрики также указана емкость узла.

Ниже приведен пример того, как можно указать буферизованную емкость в ClusterManifest.xml.

```xml
        <Section Name="NodeBufferPercentage">
            <Parameter Name="SomeMetric" Value="0.15" />
            <Parameter Name="SomeOtherMetric" Value="0.20" />
        </Section>
```

Ниже приведен пример того, как можно указать буферизованную емкость с помощью ClusterConfig.jsдля автономных развертываний или Template.jsдля кластеров, размещенных в Azure:

```json
"fabricSettings": [
  {
    "name": "NodeBufferPercentage",
    "parameters": [
      {
          "name": "SomeMetric",
          "value": "0.15"
      },
      {
          "name": "SomeOtherMetric",
          "value": "0.20"
      }
    ]
  }
]
```

При недостаточной емкости буфера для метрики создание служб завершается сбоем. Предотвращение создания новых служб для сохранения буфера гарантирует, что обновления и сбои не приведут к превышению емкости узлов. Буферизованная емкость является необязательной, но рекомендуется в любом кластере, который определяет емкость для метрики.

Диспетчер ресурсов кластера предоставляет эти сведения о загрузке. Для каждой метрики эти сведения содержат: 
- Параметры буферизованной емкости.
- Общая емкость.
- Текущее потребление.
- Считается ли каждая метрика сбалансированной.
- Статистика стандартного отклонения.
- Узлы с наибольшей и минимальной нагрузкой.  
  
В следующем коде показан пример выходных данных:

```PowerShell
PS C:\Users\user> Get-ServiceFabricClusterLoadInformation
LastBalancingStartTimeUtc : 9/1/2016 12:54:59 AM
LastBalancingEndTimeUtc   : 9/1/2016 12:54:59 AM
LoadMetricInformation     :
                            LoadMetricName        : Metric1
                            IsBalancedBefore      : False
                            IsBalancedAfter       : False
                            DeviationBefore       : 0.192450089729875
                            DeviationAfter        : 0.192450089729875
                            BalancingThreshold    : 1
                            Action                : NoActionNeeded
                            ActivityThreshold     : 0
                            ClusterCapacity       : 189
                            ClusterLoad           : 45
                            ClusterRemainingCapacity : 144
                            NodeBufferPercentage  : 10
                            ClusterBufferedCapacity : 170
                            ClusterRemainingBufferedCapacity : 125
                            ClusterCapacityViolation : False
                            MinNodeLoadValue      : 0
                            MinNodeLoadNodeId     : 3ea71e8e01f4b0999b121abcbf27d74d
                            MaxNodeLoadValue      : 15
                            MaxNodeLoadNodeId     : 2cc648b6770be1bc9824fa995d5b68b1
```

## <a name="next-steps"></a>Дальнейшие действия
* Сведения об архитектуре и потоке информации в кластерной диспетчер ресурсов см. в разделе [Общие сведения об архитектуре диспетчер ресурсов кластера](service-fabric-cluster-resource-manager-architecture.md).
* Определение метрик дефрагментации — это один из способов консолидации нагрузки на узлах, а не их распространения. Дополнительные сведения о настройке дефрагментации см. [в разделе Дефрагментация метрик и загрузка в Service Fabric](service-fabric-cluster-resource-manager-defragmentation-metrics.md).
* Начните с начала и [получите введение в Service Fabric диспетчер ресурсов кластера](service-fabric-cluster-resource-manager-introduction.md).
* Сведения о том, как кластерное диспетчер ресурсов управляет и распределяет нагрузку в кластере, см. в разделе [балансировка Service Fabric кластера](service-fabric-cluster-resource-manager-balancing.md).

[Image1]:./media/service-fabric-cluster-resource-manager-cluster-description/cluster-fault-domains.png
[Image2]:./media/service-fabric-cluster-resource-manager-cluster-description/cluster-uneven-fault-domain-layout.png
[Image3]:./media/service-fabric-cluster-resource-manager-cluster-description/cluster-fault-and-upgrade-domains-with-placement.png
[Image4]:./media/service-fabric-cluster-resource-manager-cluster-description/cluster-fault-and-upgrade-domain-layout-strategies.png
[Image5]:./media/service-fabric-cluster-resource-manager-cluster-description/cluster-layout-different-workloads.png
[Image6]:./media/service-fabric-cluster-resource-manager-cluster-description/cluster-placement-constraints-node-properties.png
[Image7]:./media/service-fabric-cluster-resource-manager-cluster-description/cluster-nodes-and-capacity.png
