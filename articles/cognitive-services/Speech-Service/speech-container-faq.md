---
title: Часто задаваемые вопросы о контейнерах речевых служб
titleSuffix: Azure Cognitive Services
description: Установите и запустите речевые контейнеры. Озвучивание речи в текст расшифровывает звуковые потоки в режиме реального времени, которые могут использоваться приложениями, инструментами или устройствами. Преобразование текста в речь преобразует вводимый текст в синтезированную речь, похожую на человеческую.
services: cognitive-services
author: aahill
manager: nitinme
ms.service: cognitive-services
ms.subservice: speech-service
ms.topic: conceptual
ms.date: 07/24/2020
ms.author: aahi
ms.custom: devx-track-csharp
ms.openlocfilehash: b13a6944290f58f5ede239dee60610d67fff8b1c
ms.sourcegitcommit: 829d951d5c90442a38012daaf77e86046018e5b9
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 10/09/2020
ms.locfileid: "88918474"
---
# <a name="speech-service-containers-frequently-asked-questions-faq"></a>Часто задаваемые вопросы о контейнерах речевых служб

При использовании службы "речь" с контейнерами следует полагаться на эту коллекцию часто задаваемых вопросов, чтобы получить поддержку. В этой статье рассматриваются вопросы различной степени, от общих до технических. Чтобы развернуть ответ, щелкните вопрос.

## <a name="general-questions"></a>Общие вопросы

<details>
<summary>
<b>Как работают контейнеры речи и как настроить их?</b>
</summary>

**Ответ.** При настройке рабочего кластера необходимо учитывать несколько моментов. Во-первых, настройка одного языка, нескольких контейнеров на одном компьютере не должна быть большой проблемой. При возникновении проблем может возникнуть проблема, связанная с оборудованием, поэтому мы сначала рассмотрим ресурс, т. е. Спецификации ЦП и памяти.

Рассмотрим в течение некоторого времени `ja-JP` контейнер и последнюю модель. Акустическая модель является наиболее ресурсоемким ПРОЦЕССОРом, а языковая модель требует наибольшей памяти. При тестировании использования необходимо около 0,6 ядер ЦП для обработки одного запроса речи в текст, когда звук передается в режиме реального времени (например, с микрофона). Если вы пойдете на звук быстрее, чем в реальном времени (например, в файле), использование может быть удвоенным (1,2 x). В то же время приведенная ниже память — это операционная память для декодирования речи. Он *не* учитывает фактический полный размер языковой модели, который будет находиться в кэше файлов. Для `ja-JP` этого еще 2 ГБ; для `en-US` может быть больше (6-7 ГБ).

Если у вас есть компьютер, на котором не хватает памяти, и вы пытаетесь развернуть на нем несколько языков, возможно, файловый кэш заполнен, а операционная система принудительно помещает в нее модели. Для выполнения транскрипции это может быть катастрофно, что может привести к снижению производительности и другим последствиям, которые могут снизить производительность.

Кроме того, мы предварительно упакованы исполняемые файлы для компьютеров с набором инструкций [расширенного векторного расширения (AVX2)](speech-container-howto.md#advanced-vector-extension-support) . Для компьютера с набором инструкций AVX512 потребуется создание кода для этого целевого объекта, и запуск 10 контейнеров для 10 языков может временно привести к нехватке ресурсов ЦП. Сообщение, подобное этому, появится в журналах docker:

```console
2020-01-16 16:46:54.981118943 
[W:onnxruntime:Default, tvm_utils.cc:276 LoadTVMPackedFuncFromCache]
Cannot find Scan4_llvm__mcpu_skylake_avx512 in cache, using JIT...
```

Наконец, можно задать нужное число декодеров внутри *одного* контейнера с помощью `DECODER MAX_COUNT` переменной. Так что, по сути, мы должны начать с номера SKU (ЦП или памяти), и мы можем предложить, как получить лучшее решение. Отличная начальная точка ссылается на Рекомендуемые спецификации ресурсов хост-компьютера.

<br>
</details>

<details>
<summary>
<b>Вы могли бы помочь в планировании ресурсов и оценке затрат на локальные контейнеры преобразования речи в текст?</b>
</summary>

**Ответ.** Для емкости контейнеров в режиме пакетной обработки каждый декодер может обрабатывать 2-3 раза в режиме реального времени с двумя ядрами ЦП для единого распознавания. Не рекомендуется хранить более двух одновременных распознавать на каждый экземпляр контейнера, но рекомендуется запускать больше экземпляров контейнеров для обеспечения надежности и доступности за счет подсистемы балансировки нагрузки.

Несмотря на то, что каждый экземпляр контейнера может работать с несколькими декодерами. Например, мы можем настроить 7 декодеров для каждого экземпляра контейнера на восьми ядре компьютера (в более чем 2x), выдавая пропускную способность 15X. Существует параметр, `DECODER_MAX_COUNT` который следует учитывать. В экстремальных случаях возникают проблемы с надежностью и задержкой, что значительно увеличила пропускную способность. Для микрофона он будет иметь значение 1x в режиме реального времени. Общее использование должно быть одним ядром для одного распознавания.

В случае обработки 1 часа в день в режиме пакетной обработки, 3 виртуальные машины могут обработать его в течение 24 часов, но не гарантированно. Чтобы справиться с пиками, отработкой отказа, обновлением и предоставлением минимальной резервной копии или BCP, мы рекомендуем 4-5 компьютеров вместо 3 для каждого кластера и с 2 + кластерами.

Для оборудования мы используем стандартную виртуальную машину Azure в `DS13_v2` качестве справочника (каждая из ядер должна быть 2,6 ГГц или выше, с включенным набором инструкций AVX2).

| Экземпляр  | Виртуальных ЦП | ОЗУ    | Хранилище Temp | Оплата по мере использования с помощью АХБ | годовой резерв с АХБ (% экономии) | 3-летняя зарезервировано с АХБ (% экономии) |
|-----------|---------|--------|--------------|------------------------|-------------------------------------|--------------------------------------|
| `DS13 v2` | 8       | 56 гиб | 112 ГиБ      | $0.598/час            | $0.3528/час (~ 41%)                 | $0.2333/час (~ 61%)                  |

На основе справочника по проектированию (два кластера из 5 виртуальных машин для обработки аудио пакетной передачи в 1 КБ/день), стоимость оборудования в 1 год будет:

> 2 (кластеры) * 5 (виртуальных машин на кластер) * $0.3528/час * 365 (дн.) * 24 (ч) = $31K/year

При сопоставлении с физическим компьютером общая оценка — 1 виртуальных ЦП = 1 физическое ядро ЦП. В реальности 1vCPU является более мощным, чем единое ядро.

В локальной системе все эти дополнительные факторы играют следующие преимущества:

- Как тип физического ЦП и сколько ядер на нем
- Количество процессоров, работающих на одном поле или компьютере
- Настройка виртуальных машин
- Как используется технология Hyper-Threading/многопоточность
- Общий доступ к памяти
- ОС и т. д.

Обычно она не настроена в качестве среды Azure. Учитывая другие издержки, я бы сказал, что в качестве надежной оценки будет 10 физических ядер ЦП = 8 Azure виртуальных ЦП. Хотя популярные процессоры имеют только восемь ядер. При развертывании в локальной среде стоимость будет выше, чем при использовании виртуальных машин Azure. Кроме того, рассмотрим ставку амортизации.

Стоимость службы та же, что и в веб-службе: $1/час для преобразования речи в текст. Стоимость службы речи:

> $1 * 1000 * 365 = $365K

Стоимость обслуживания, выплачиваемая в корпорацию Майкрософт, зависит от уровня обслуживания и содержимого службы. Он имеет разное значение от $29,99/month для уровня "базовый" до сотен тысяч, если участвует в обслуживании. Приблизительное число составляет $300 в час для службы или обслуживания. Стоимость сотрудников не включена. Другие затраты на инфраструктуру (например, хранилище, сети и подсистемы балансировки нагрузки) не включены.

<br>
</details>

<details>
<summary>
<b>Почему пунктуация в транскрипции отсутствует?</b>
</summary>

**Ответ.** `speech_recognition_language=<YOUR_LANGUAGE>` Должен быть явно настроен в запросе, если они используют клиента.

Пример:

```python
if not recognize_once(
    speechsdk.SpeechRecognizer(
        speech_config=speechsdk.SpeechConfig(
            endpoint=template.format("interactive"),
            speech_recognition_language="ja-JP"),
            audio_config=audio_config)):

    print("Failed interactive endpoint")
    exit(1)
```
Результат выглядит так:

```cmd
RECOGNIZED: SpeechRecognitionResult(
    result_id=2111117c8700404a84f521b7b805c4e7, 
    text="まだ早いまだ早いは猫である名前はまだないどこで生まれたかとんと見当を検討をなつかぬ。
    何でも薄暗いじめじめした所でながら泣いていた事だけは記憶している。
    まだは今ここで初めて人間と言うものを見た。
    しかも後で聞くと、それは書生という人間中で一番同額同額。",
    reason=ResultReason.RecognizedSpeech)
```

<br>
</details>

<details>
<summary>
<b>Можно ли использовать пользовательскую акустическую модель и модель языка с контейнером речи?</b>
</summary>

Сейчас мы можем передать только один идентификатор модели, настраиваемую языковую модель или настраиваемую акустическую модель.

**Ответ.** Было принято решение о одновременной *поддержке как* акустических, так и языковых моделей. Это остается действительным до тех пор, пока не будет создан унифицированный идентификатор для сокращения количества разрывов API. К сожалению, это не поддерживается сейчас.

<br>
</details>

<details>
<summary>
<b>Могли бы объяснить эти ошибки из пользовательского контейнера преобразования речи в текст?</b>
</summary>

**Ошибка 1:**

```cmd
Failed to fetch manifest: Status: 400 Bad Request Body:
{
    "code": "InvalidModel",
    "message": "The specified model is not supported for endpoint manifests."
}
```

**Ответ 1.** Если вы выполняете обучение с последней пользовательской моделью, мы сейчас не поддерживаем ее. Если вы обучились с помощью более старой версии, то можете использовать. Мы по-прежнему работаем над поддержкой последних версий.

По сути, пользовательские контейнеры не поддерживают акустические модели на основе Халиде или ONNX (по умолчанию на настраиваемом портале обучения). Это обусловлено тем, что пользовательские модели не шифруются и мы не хотим предоставлять модели ONNX. языковые модели прекрасно работают. Клиент должен явно выбрать старую модель, не ONNX, для пользовательского обучения. Точность не будет затронуто. Размер модели может быть больше (на 100 МБ).

> Поддержка модели > 20190220 (унифицированная версия 4.5)

**Ошибка 2:**

```cmd
HTTPAPI result code = HTTPAPI_OK.
HTTP status code = 400.
Reason:  Synthesis failed.
StatusCode: InvalidArgument,
Details: Voice does not match.
```

**Ответ 2.** Необходимо указать правильное имя голоса в запросе, в котором учитывается регистр. См. полное сопоставление имени службы. Вы должны использовать `en-US-JessaRUS` , так как `en-US-JessaNeural` сейчас недоступно в версии контейнера преобразования текста в речь.

**Ошибка 3:**

```json
{
    "code": "InvalidProductId",
    "message": "The subscription SKU \"CognitiveServices.S0\" is not supported in this service instance."
}
```

**Ответ 3.** Вы Reed создать речевой ресурс, а не ресурс Cognitive Services.


<br>
</details>

<details>
<summary>
<b>Какие протоколы API поддерживаются, функции RESTFUL или WS?</b>
</summary>

**Ответ.** В настоящее время для преобразования речи в текст и пользовательских текстовых контейнеров поддерживается только протокол на основе WebSocket. Пакет SDK поддерживает вызов только в отношении WS, но не для остальных. Существует план добавления поддержки RESTFUL, но не слишком много времени. Всегда используйте официальную документацию, см. раздел [прогнозирующие конечные точки запросов](speech-container-howto.md#query-the-containers-prediction-endpoint).

<br>
</details>

<details>
<summary>
<b>Поддерживается ли CentOS для речевых контейнеров?</b>
</summary>

**Ответ.** CentOS 7 еще не поддерживается пакетом SDK для Python, также Ubuntu 19,04 не поддерживается.

Пакет SDK службы "Речь" для Python доступен для таких операционных систем:
- **Windows** — x64 и x86
- **Mac** -macOS X версии 10,12 или более поздней
- **Linux** -Ubuntu 16,04, Ubuntu 18,04, Debian 9 на x64

Дополнительные сведения о настройке среды см. в разделе [Установка платформы Python](quickstarts/setup-platform.md?pivots=programming-language-python). Сейчас Ubuntu 18,04 является рекомендуемой версией.

<br>
</details>

<details>
<summary>
<b>Почему при попытке вызова конечных точек прогнозирования LUIS возникают ошибки?</b>
</summary>

Я использую контейнер LUIS в развертывании IoT Edge и пытаюсь вызвать конечную точку прогнозирования LUIS из другого контейнера. Контейнер LUIS прослушивает порт 5001, и URL-адрес, который я использую:

```csharp
var luisEndpoint =
    $"ws://192.168.1.91:5001/luis/prediction/v3.0/apps/{luisAppId}/slots/production/predict";
var config = SpeechConfig.FromEndpoint(new Uri(luisEndpoint));
```

Полученная ошибка:

```cmd
WebSocket Upgrade failed with HTTP status code: 404 SessionId: 3cfe2509ef4e49919e594abf639ccfeb
```

Я вижу запрос в журналах контейнеров LUIS, и сообщение говорит:

```cmd
The request path /luis//predict" does not match a supported file type.
```

Что это значит? Что отсутствует? Я уже использовал пример пакета SDK для распознавания речи, отсюда [.](https://github.com/Azure-Samples/cognitive-services-speech-sdk) Сценарий заключается в том, что мы обнаруживаем звук непосредственно с микрофона компьютера и пытаемся определить намерение, основываясь на LUIS приложении, о котором мы обучены. В примере, который я связали с, выполняется именно это. И хорошо работает с облачной службой LUIS. При использовании речевого пакета SDK пришлось делать отдельный явный вызов API преобразования речи в текст, а затем второй вызов LUIS.

Поэтому я пытаюсь переключаться из сценария использования LUIS в облаке для использования контейнера LUIS. Я не могу представить, работает ли речевой пакет SDK для одного, он не будет работать в другом.

**Ответ.** Пакет SDK для распознавания речи не должен использоваться для контейнера LUIS. Для использования контейнера LUIS следует использовать пакет SDK LUIS или LUIS REST API. Речевой пакет SDK следует использовать для контейнера речи.

Облако отличается от контейнера. Облако может состоять из нескольких обобщенных контейнеров (иногда называемых микрослужбами). Итак, существует контейнер LUIS, а затем есть контейнер речи — два отдельных контейнера. Контейнер речи выполняет только речь. Контейнер LUIS только LUIS. В облаке, поскольку предполагается, что оба контейнера развернуты и неплохая производительность удаленного клиента для перехода в облако, перевода речи, возврата, перехода в облако и выполнения LUIS, мы предоставляем функцию, позволяющую клиенту перейти в речь, остаться в облаке, перейти к LUIS и вернуться к клиенту. Таким же даже в этом сценарии пакет SDK для распознавания речи переходит в контейнер речевого облака с аудио, а затем контейнер речевого облака обращается к контейнеру LUIS Cloud с текстом. Контейнер LUIS не имеет концепции приема звука (для контейнера LUIS не было смысла принимать потоковые аудио-LUIS — это служба, основанная на тексте). В локальной среде у нас нет уверенности в том, что клиент развернул оба контейнера, мы не планируем управлять контейнерами в нашей Организации, и если оба контейнера развернуты локально, учитывая, что они являются более локальными для клиента, то не стоит сначала перейти на клиентскую систему, вернуться к клиенту и перейти к LUIS.

<br>
</details>

<details>
<summary>
<b>Почему мы получаем ошибки с помощью macOS, контейнера речи и пакета SDK для Python?</b>
</summary>

Когда мы отправляем *WAV* -файл расшифрованной, результат возвращается следующим образом:

```cmd
recognition is running....
Speech Recognition canceled: CancellationReason.Error
Error details: Timeout: no recognition result received.
When creating a websocket connection from the browser a test, we get:
wb = new WebSocket("ws://localhost:5000/speech/recognition/dictation/cognitiveservices/v1")
WebSocket
{
    url: "ws://localhost:5000/speech/recognition/dictation/cognitiveservices/v1",
    readyState: 0,
    bufferedAmount: 0,
    onopen: null,
    onerror: null,
    ...
}
```

Мы понимаем, что WebSocket настроен правильно.

**Ответ.** Если это так, см. [эту ошибку в GitHub](https://github.com/Azure-Samples/cognitive-services-speech-sdk/issues/310). Мы предлагаем вам решить [эту](https://github.com/Azure-Samples/cognitive-services-speech-sdk/issues/310#issuecomment-527542722)проблемы.

Эта проблема исправлена в версии 1,8.


<br>
</details>

<details>
<summary>
<b>Каковы различия в конечных точках контейнера речи?</b>
</summary>

Можно ли заполнять следующие метрики тестирования, в том числе какие функции тестировать, а также тестировать пакет SDK и интерфейсы API для других приложений? В частности, различия в "интерактивном" и "разговоре", которые я не вижу из существующего документа или примера.

| Конечная точка                                                | Функциональный тест                                                   | SDK | REST API |
|---------------------------------------------------------|-------------------------------------------------------------------|-----|----------|
| `/speech/synthesize/cognitiveservices/v1`               | Синтезирование текста (преобразование текста в речь)                                  |     | Да      |
| `/speech/recognition/dictation/cognitiveservices/v1`    | Cognitive Services конечную точку WebSocket для локальной диктовки v1        | Да | Нет       |
| `/speech/recognition/interactive/cognitiveservices/v1`  | Cognitive Services локальную конечную точку WebSocket локального узла v1  |     |          |
| `/speech/recognition/conversation/cognitiveservices/v1` | Конечная точка для диалогового окна "перекрывающий службы" в локальной службе WebSocket v1 |     |          |

**Ответ.** Это Fusion:
- Люди, пытающиеся сделать конечную точку диктовки для контейнеров, (я не уверен, как они получили этот URL-адрес)
- Одна конечная точка<sup>St</sup> -a находится в контейнере.
- 1 конечная точка<sup>St</sup> , возвращающая речь. фрагменты сообщения вместо `speech.hypothesis` сообщений. 3 конечные точки компонента<sup>удаленных рабочих столов</sup> возвращаются для конечной точки диктофона.
- Все краткие руководства по использованию всех шаблонов `RecognizeOnce` (интерактивный режим)
- У углерода есть утверждение о том, что для `speech.fragment` сообщений, которым они не возвращаются в интерактивном режиме.
- Углекислые утверждения срабатывают в сборках выпуска (завершение процесса).

Чтобы решить проблему, переключитесь на использование непрерывного распознавания в коде или (быстрее) подключитесь либо к интерактивным, либо к непрерывным конечным точкам в контейнере.
Для кода задайте для конечной точки значение `host:port` /Speech/Recognition/Interactive/cognitiveservices/v1.

Различные режимы см. в разделе речевые режимы. см. ниже:

[!INCLUDE [speech-modes](includes/speech-modes.md)]

Исправление исправляется с помощью пакета SDK 1,8, который имеет локальную поддержку (выберет правильную конечную точку, поэтому мы будем работать не так, как веб-служба). В то же время есть пример для непрерывного распознавания, почему мы не будем указывать на него?

https://github.com/Azure-Samples/cognitive-services-speech-sdk/blob/6805d96bf69d9e95c9137fe129bc5d81e35f6309/samples/python/console/speech_sample.py#L196

<br>
</details>

<details>
<summary>
<b>Какой режим следует использовать для различных звуковых файлов?</b>
</summary>

**Ответ.** Ниже приведен [краткий пример использования Python](quickstarts/speech-to-text-from-microphone.md?pivots=programming-language-python). Другие языки, связанные с документами, можно найти на сайте документации.

Только для уточнения интерактивного, диалогового окна и диктовки; Это расширенный способ указания конкретного способа, которым наша служба будет обслуживать речевой запрос. К сожалению, для локальных контейнеров необходимо указать полный URI (так как он включает локальный компьютер), чтобы эта информация была утечка из абстракции. Мы работаем над командой пакета SDK, чтобы сделать это более удобным в будущем.

<br>
</details>

<details>
<summary>
<b>Как можно протестировать приблизительную меру транзакций в секунду на ядро?</b>
</summary>

**Ответ.** Ниже приведены некоторые из приблизительных чисел, которые следует рассчитывать на основе существующей модели (будет изменено для лучшего использования в общедоступной версии):

- Для файлов регулирование будет находиться в пакете SDK для распознавания речи, в 2x. Первые пять секунд звука не регулируется. Декодер способен выполнять 3 раза в режиме реального времени. Для этого общая загрузка ЦП будет близка к 2 ядрам для одного распознавания.
- Для MIC он будет находиться в режиме реального времени. Общее использование должно быть около 1 ядра для одного распознавания.

Все это можно проверить в журналах DOCKER. Мы фактически выведите дамп строки с статистикой сеанса и фразой/utterance, которая включает номера RTF.


<br>
</details>

<details>
<summary>
<b>Можно ли разделить звуковые файлы на чуккс для использования контейнера речи?</b>
</summary>

Текущий план — взять существующий звуковой файл и разделить его на 10 секунд и отправить их через контейнер. Является ли допустимым сценарий?  Существует ли лучший способ обработки больших звуковых файлов с помощью контейнера?

**Ответ.** Просто используйте пакет SDK для распознавания речи и присвойте ему файл, он сделает это правильно. Зачем нужно выполнять фрагментацию файла?


<br>
</details>

<details>
<summary>
<b>Разделы справки сделать несколько контейнеров запущенными на одном узле?</b>
</summary>

В документе говорится, как предоставить другой порт, который я делаю, но контейнер LUIS по-прежнему прослушивает порт 5000?

**Ответ.** Попробуйте `-p <outside_unique_port>:5000` . Например, `-p 5001:5000`.


<br>
</details>

## <a name="technical-questions"></a>Технические вопросы

<details>
<summary>
<b>Как можно получить непакетные API для обработки звука на &lt; 15 секунд?</b>
</summary>

**Ответ.** `RecognizeOnce()` в интерактивном режиме обрабатываются только до 15 секунд звукового сигнала, так как режим предназначен для речевых команд, где фразы продолжительностьюы должны быть короткими. Если вы используете `StartContinuousRecognition()` для диктовки или диалога, ограничение в 15 секунд отсутствует.


<br>
</details>

<details>
<summary>
<b>Каковы рекомендуемые ресурсы, ЦП и ОЗУ; для 50 параллельных запросов?</b>
</summary>

Сколько одновременных запросов будет 4-ядерным размером 4 ГБ ОЗУ? Если бы нужно было бы использовать, например, 50 параллельных запросов, сколько ядер и ОЗУ?

**Ответ.** В реальном времени 8 с нашим последним `en-US` , поэтому мы рекомендуем использовать больше контейнеров DOCKER за пределами 6 одновременных запросов. Он получает смешнее за пределы 16 ядер и становится чувствительным к неоднородному доступу к памяти (NUMA). В следующей таблице описаны минимальное и рекомендуемое выделение ресурсов для каждого контейнера речи.

# <a name="speech-to-text"></a>[Преобразование речи в текст](#tab/stt)

| Контейнер      | Минимальные             | Рекомендуется         |
|----------------|---------------------|---------------------|
| Преобразование речи в текст | 2 ядра, 2 ГБ памяти | 4 ядра, 4 ГБ памяти |

# <a name="custom-speech-to-text"></a>[Пользовательское распознавание речи к тексту](#tab/cstt)

| Контейнер             | Минимальные             | Рекомендуется         |
|-----------------------|---------------------|---------------------|
| Пользовательское распознавание речи к тексту | 2 ядра, 2 ГБ памяти | 4 ядра, 4 ГБ памяти |

# <a name="text-to-speech"></a>[Преобразование текста в речь](#tab/tts)

| Контейнер      | Минимальные             | Рекомендуется         |
|----------------|---------------------|---------------------|
| Преобразование текста в речь | 1 ядро, 2 ГБ памяти | 2 ядра, 3 ГБ памяти |

# <a name="custom-text-to-speech"></a>[Пользовательский текст в речь](#tab/ctts)

| Контейнер             | Минимальные             | Рекомендуется         |
|-----------------------|---------------------|---------------------|
| Пользовательский текст в речь | 1 ядро, 2 ГБ памяти | 2 ядра, 3 ГБ памяти |

***

- Каждое ядро должно иметь частоту не менее 2,6 ГГц или быстрее.
- Для файлов регулирование будет находиться в пакете SDK для распознавания речи, в 2x (первые 5 секунд звука не регулируется).
- Декодер способен выполнять 3 раза в режиме реального времени. Для этого общая загрузка ЦП будет близка к двум ядрам для одного распознавания. Поэтому мы не рекомендуем хранить более двух активных соединений в каждом экземпляре контейнера. Крайний стороной может быть около 10 декодеров в режиме реального времени на восьми компьютерах, например `DS13_V2` . Для контейнера версии 1,3 и более поздней существует параметр, который можно попытаться установить `DECODER_MAX_COUNT=20` .
- Для микрофона это будет в 1x в режиме реального времени. Общее использование должно быть одним ядром для одного распознавания.

Оцените общее количество часов у звукового сигнала. Если это число велико, для повышения надежности и доступности мы рекомендуем запускать больше экземпляров контейнеров — как на одном, так и на нескольких полях, расположенных за подсистемой балансировки нагрузки. Согласование можно выполнить с помощью Kubernetes (K8S) и Helm или с помощью создания DOCKER.

Например, для обработки 1000 часов/24 часов мы попытались настроить виртуальные машины 3-4 с 10 экземплярами и декодерами на виртуальную машину.

<br>
</details>

<details>
<summary>
<b>Поддерживает ли контейнер речи пунктуацию?</b>
</summary>

**Ответ.** В локальном контейнере доступны прописные буквы (восстановление). Пунктуация зависит от языка и не поддерживается для некоторых языков, включая китайский и японский.

У *нас есть* неявная и базовая поддержка пунктуации для существующих контейнеров, но она по `off` умолчанию. Это означает, что можно получить `.` символ в примере, но не `。` символ. Чтобы включить эту неявную логику, ниже приведен пример того, как это сделать в Python с помощью нашего пакета SDK для распознавания речи (он будет похож на другие языки):

```python
speech_config.set_service_property(
    name='punctuation',
    value='implicit',
    channel=speechsdk.ServicePropertyChannel.UriQueryParameter
)
```

<br>
</details>

<details>
<summary>
<b>Почему я получаю ошибки 404 при попытке отправить данные в контейнер преобразования речи в текст?</b>
</summary>

Ниже приведен пример HTTP-запроса POST:

```http
POST /speech/recognition/conversation/cognitiveservices/v1?language=en-US&format=detailed HTTP/1.1
Accept: application/json;text/xml
Content-Type: audio/wav; codecs=audio/pcm; samplerate=16000
Transfer-Encoding: chunked
User-Agent: PostmanRuntime/7.18.0
Cache-Control: no-cache
Postman-Token: xxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx
Host: 10.0.75.2:5000
Accept-Encoding: gzip, deflate
Content-Length: 360044
Connection: keep-alive
HTTP/1.1 404 Not Found
Date: Tue, 22 Oct 2019 15:42:56 GMT
Server: Kestrel
Content-Length: 0
```

**Ответ.** Мы не поддерживаем REST API в контейнере преобразования речи в текст. Мы поддерживаем только WebSocket через пакет SDK для распознавания речи. Всегда используйте официальную документацию, см. раздел [прогнозирующие конечные точки запросов](speech-container-howto.md#query-the-containers-prediction-endpoint).

<br>
</details>

<details>
<summary>
<b>Почему я получаю сообщение об ошибке при использовании службы преобразования речи в текст?</b>
</summary>

```cmd
Error in STT call for file 9136835610040002161_413008000252496:
{
    "reason": "ResultReason.Canceled",
    "error_details": "Due to service inactivity the client buffer size exceeded. Resetting the buffer. SessionId: xxxxx..."
}
```

**Ответ.** Обычно это происходит при ускорении передачи звука по сравнению с тем, что его может использовать контейнер распознавания речи. Клиентские буферы заполняются и происходит отмена. Необходимо управлять параллелизмом и RTF, в котором вы отправляете звук.

<br>
</details>

<details>
<summary>
<b>Можно ли объяснить эти ошибки контейнеров преобразования текста в речь из примеров C++?</b>
</summary>

**Ответ.** Если версия контейнера старше 1,3, следует использовать следующий код:

```cpp
const auto endpoint = "http://localhost:5000/speech/synthesize/cognitiveservices/v1";
auto config = SpeechConfig::FromEndpoint(endpoint);
auto synthesizer = SpeechSynthesizer::FromConfig(config);
auto result = synthesizer->SpeakTextAsync("{{{text1}}}").get();
```

Более старые контейнеры не имеют необходимой конечной точки для работы с `FromHost` API. Если контейнеры используются для версии 1,3, этот код следует использовать:

```cpp
const auto host = "http://localhost:5000";
auto config = SpeechConfig::FromHost(host);
config->SetSpeechSynthesisVoiceName(
    "Microsoft Server Speech Text to Speech Voice (en-US, AriaRUS)");
auto synthesizer = SpeechSynthesizer::FromConfig(config);
auto result = synthesizer->SpeakTextAsync("{{{text1}}}").get();
```

Ниже приведен пример использования `FromEndpoint` API:

```cpp
const auto endpoint = "http://localhost:5000/cognitiveservices/v1";
auto config = SpeechConfig::FromEndpoint(endpoint);
config->SetSpeechSynthesisVoiceName(
    "Microsoft Server Speech Text to Speech Voice (en-US, AriaRUS)");
auto synthesizer = SpeechSynthesizer::FromConfig(config);
auto result = synthesizer->SpeakTextAsync("{{{text2}}}").get();
```

 `SetSpeechSynthesisVoiceName`Функция вызывается, так как контейнеры с обновленным модулем преобразования текста в речь нуждаются в имени голоса.

<br>
</details>

<details>
<summary>
<b>Как можно использовать версию 1.7 пакета SDK для распознавания речи с контейнером речи?</b>
</summary>

**Ответ.** Существует три конечных точки в контейнере речи для различных применений, они определены как речевые режимы, см. ниже:

[!INCLUDE [speech-modes](includes/speech-modes.md)]

Они предназначены для разных целей и используются по-разному.

[Примеры](https://github.com/Azure-Samples/cognitive-services-speech-sdk/blob/master/samples/python/console/speech_sample.py)для Python:
- Сведения об одном распознавании (интерактивном режиме) с пользовательской конечной точкой (то есть `SpeechConfig` с параметром конечной точки) см. в разделе `speech_recognize_once_from_file_with_custom_endpoint_parameters()` .
- Для непрерывного распознавания (режим диалога) и просто изменения для использования пользовательской конечной точки, как описано выше, см `speech_recognize_continuous_from_file()` . раздел.
- Чтобы включить диктовку в образцах, подобных приведенным выше (только если это действительно необходимо), сразу после создания `speech_config` добавьте код `speech_config.enable_dictation()` .

В C# для включения диктовки вызовите `SpeechConfig.EnableDictation()` функцию.

### <a name="fromendpoint-apis"></a>`FromEndpoint` Интерфейсы
| Язык | Сведения об API |
|----------|:------------|
| C++ | <a href="https://docs.microsoft.com/en-us/cpp/cognitive-services/speech/speechconfig#fromendpoint" target="_blank">`SpeechConfig::FromEndpoint` <span class="docon docon-navigate-external x-hidden-focus"></span></a> |
| C# | <a href="https://docs.microsoft.com/dotnet/api/microsoft.cognitiveservices.speech.speechconfig.fromendpoint?view=azure-dotnet" target="_blank">`SpeechConfig.FromEndpoint` <span class="docon docon-navigate-external x-hidden-focus"></span></a> |
| Java | <a href="https://docs.microsoft.com/java/api/com.microsoft.cognitiveservices.speech.speechconfig.fromendpoint?view=azure-java-stable" target="_blank">`SpeechConfig.fromendpoint` <span class="docon docon-navigate-external x-hidden-focus"></span></a> |
| Objective-C | <a href="https://docs.microsoft.com/en-us/objectivec/cognitive-services/speech/spxspeechconfiguration#initwithendpoint" target="_blank">`SPXSpeechConfiguration:initWithEndpoint;` <span class="docon docon-navigate-external x-hidden-focus"></span></a> |
| Python | <a href="https://docs.microsoft.com/python/api/azure-cognitiveservices-speech/azure.cognitiveservices.speech.speechconfig?view=azure-python" target="_blank">`SpeechConfig;` <span class="docon docon-navigate-external x-hidden-focus"></span></a> |
| JavaScript | В настоящее время не поддерживается и не планируется. |

<br>
</details>

<details>
<summary>
<b>Как использовать v 1.8 речевого пакета SDK с контейнером речи?</b>
</summary>

**Ответ.** Есть новый `FromHost` API. Это не приводит к замене или изменению существующих интерфейсов API. Он просто добавляет альтернативный способ создания конфигурации речи с помощью пользовательского узла.

### <a name="fromhost-apis"></a>`FromHost` Интерфейсы

| Язык | Сведения об API |
|--|:-|
| C# | <a href="https://docs.microsoft.com/dotnet/api/microsoft.cognitiveservices.speech.speechconfig.fromhost?view=azure-dotnet" target="_blank">`SpeechConfig.FromHost` <span class="docon docon-navigate-external x-hidden-focus"></span></a> |
| C++ | <a href="https://docs.microsoft.com/en-us/cpp/cognitive-services/speech/speechconfig#fromhost" target="_blank">`SpeechConfig::FromHost` <span class="docon docon-navigate-external x-hidden-focus"></span></a> |
| Java | <a href="https://docs.microsoft.com/java/api/com.microsoft.cognitiveservices.speech.speechconfig.fromhost?view=azure-java-stable" target="_blank">`SpeechConfig.fromHost` <span class="docon docon-navigate-external x-hidden-focus"></span></a> |
| Objective-C | <a href="https://docs.microsoft.com/en-us/objectivec/cognitive-services/speech/spxspeechconfiguration#initwithhost" target="_blank">`SPXSpeechConfiguration:initWithHost;` <span class="docon docon-navigate-external x-hidden-focus"></span></a> |
| Python | <a href="https://docs.microsoft.com/python/api/azure-cognitiveservices-speech/azure.cognitiveservices.speech.speechconfig?view=azure-python" target="_blank">`SpeechConfig;` <span class="docon docon-navigate-external x-hidden-focus"></span></a> |
| JavaScript | Сейчас не поддерживается |

> Параметры: узел (обязательный), ключ подписки (необязательно, если вы можете использовать службу без нее).

Формат для Host является `protocol://hostname:port` `:port` необязательным (см. ниже):
- Если контейнер запущен локально, имя узла — `localhost` .
- Если контейнер работает на удаленном сервере, используйте имя узла или IPv4-адрес этого сервера.

Примеры параметров узла для преобразования речи в текст:
- `ws://localhost:5000` — небезопасное подключение к локальному контейнеру через порт 5000
- `ws://some.host.com:5000` — небезопасное подключение к контейнеру, выполняющемуся на удаленном сервере;

Примеры Python, приведенные выше, но используйте `host` параметр вместо `endpoint` :

```python
speech_config = speechsdk.SpeechConfig(host="ws://localhost:5000")
```

<br>
</details>

## <a name="next-steps"></a>Дальнейшие действия

> [!div class="nextstepaction"]
> [Контейнеры Cognitive Services](speech-container-howto.md)
