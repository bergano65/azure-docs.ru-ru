---
title: Оценка вознаграждения (Персонализатор)
titleSuffix: Azure Cognitive Services
description: Оценка вознаграждения указывает, насколько хорошо подходит пользователю выбор персонализации RewardActionID. Значение оценки вознаграждения определяется бизнес-логикой, основанной на наблюдении за поведением пользователей. Персонализатор обучает свои модели машинного обучения, оценивая вознаграждения.
services: cognitive-services
author: diberry
manager: nitinme
ms.service: cognitive-services
ms.subservice: personalizer
ms.topic: conceptual
ms.date: 10/24/2019
ms.author: diberry
ms.openlocfilehash: a47d6014e51dce81c9caf82f8624896c439f050d
ms.sourcegitcommit: c22327552d62f88aeaa321189f9b9a631525027c
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 11/04/2019
ms.locfileid: "73490883"
---
# <a name="reward-scores-indicate-success-of-personalization"></a>Оценки вознаграждения демонстрируют успешность персонализации

Оценка вознаграждения указывает, насколько хорошо подходит пользователю выбор персонализации [RewardActionID](https://docs.microsoft.com/rest/api/cognitiveservices/personalizer/rank/rank#response). Значение оценки вознаграждения определяется бизнес-логикой, основанной на наблюдении за поведением пользователей.

Персонализатор обучает свои модели машинного обучения, оценивая вознаграждения. 

## <a name="use-reward-api-to-send-reward-score-to-personalizer"></a>Передача оценки вознаграждения в Персонализатор через API вознаграждения

Вознаграждения отправляются в Персонализатор через [API вознаграждения](https://docs.microsoft.com/rest/api/cognitiveservices/personalizer/events/reward). Как правило, вознаграждение является числом от 0 до 1. Отрицательное вознаграждение со значением-1 возможно в определенных сценариях, и его следует использовать только при работе с подкреплением Learning (RL). Персонализатор обучает модель для достижения наивысшей возможной суммы вознаграждения за период времени.

Вознаграждения отправляются после выполнения пользователем действий, что может произойти через несколько дней после этого. В параметре [Reward Wait Time](#reward-wait-time) (Время ожидания вознаграждения) на портале Azure можно указать максимальное количество времени, в течение которого Персонализатор ожидает вознаграждения. По истечении этого времени считается, что вознаграждения нет или применяется настроенное по умолчанию вознаграждение.

Если оценка вознаграждения для события не получена в течение **времени ожидания вознаграждения**, применяется **Default Reward** (Вознаграждение по умолчанию). Обычно **[вознаграждение по умолчанию](how-to-settings.md#configure-reward-settings-for-the-feedback-loop-based-on-use-case)** равно нулю.


## <a name="behaviors-and-data-to-consider-for-rewards"></a>Поведение и данные, которые следует учитывать для вознаграждения

Рассмотрим эти сигналы и поведение для контекста оценки вознаграждения:

* Прямой ввод пользователя для получения предложений, если присутствуют параметры ("Вы имеете в виду X?").
* Продолжительность сеанса.
* Время между сеансами.
* Анализ тональности взаимодействий с пользователем.
* Прямые вопросы и мини-опросы, во время которых бот запрашивает у пользователя отзыв о полезности и точности.
* Ответ на оповещения или задержка такого ответа.

## <a name="composing-reward-scores"></a>Расчет оценок вознаграждения

Оценка вознаграждения должна вычисляться в бизнес-логике. Эта оценка может быть выражена в следующих форматах:

* одно число, отправляемое один раз; 
* немедленно отправляемая оценка (например 0,8) и отправляемая позже дополнительная оценка (обычно 0,2).

## <a name="default-rewards"></a>Вознаграждение по умолчанию

Если после вызова события Rank вознаграждение не получено в течение времени, указанного в параметре [Reward Wait Time](#reward-wait-time) (Время ожидания вознаграждения), Персонализатор неявно применяет для этого события **вознаграждение по умолчанию**.

## <a name="building-up-rewards-with-multiple-factors"></a>Вычисление оценок с несколькими факторами  

Для эффективной персонализации можно создать оценку вознаграждений на основе нескольких факторов. 

Например, для персонализации списка видео, можно применить такие правила.

|Действия пользователя|Значение компонента оценки|
|--|--|
|Пользователь щелкнул верхний элемент.|Оценка +0,5|
|Пользователь открыл фактическое содержимое этого элемента.|Оценка +0,3|
|Пользователь просматривал содержимое не менее 5 минут или просмотрел не менее 30% содержимого.|Оценка +0,2|
|||

Итоговую оценку можно отправить в API.

## <a name="calling-the-reward-api-multiple-times"></a>Многократный вызов API вознаграждения

Вы можете вызывать API вознаграждения с одним идентификатором события несколько раз, отправляя разные оценки вознаграждения. Когда функция персонализации получает эти вознаграждения, она определяет окончательное вознаграждение для этого события, выполнив статистическую обработку, как указано в конфигурации персонализации.

Значения статистической обработки:

*  **Первый**: принимает первый полученный результат для события и отменяет остальные.
* **Sum**: принимает все оценки наград, собранные для EventID, и добавляет их вместе.

Все вознаграждения за событие, полученные после истечения **времени ожидания вознаграждения**, отбрасываются и не учитываются в обучении моделей.

Добавив оценки вознаграждений, вы сможете получить итоговые результаты за пределами ожидаемого диапазона оценок. Это не приводит к сбою службы.

## <a name="best-practices-for-calculating-reward-score"></a>Советы и рекомендации для вычисления оценки вознаграждения

* **Оцените истинные признаки успешной персонализации**: легко Взгляните на щелчки, но хороший вознаграждение зависит от того, что вы хотите *достичь* , а не от того, что нужно *делать*пользователям.  Например, вознаграждение на основе числа щелчков может привести к отбору содержимого с заголовками-наживками.

* **Использование оценки награды для того, насколько хорошо работает Персонализация**: Персонализация предложения фильма, скорее всего, приведет к тому, что пользователь просмотрит фильм и предоставит ему высокую оценку. Но рейтинг фильма часто зависит от многих параметров (качества игры актеров, настроения пользователя), поэтому он не очень точно отражает эффективность *персонализации*. Если пользователь просмотрел несколько первых минут фильма, это может лучше свидетельствовать об эффективности персонализации. Таким образом, неплохим сигналом будет оценка 1 за 5 минут просмотра.

* **Вознаграждения применяются только к ревардактионид**: Персонализация применяет вознаграждения для понимания эффективность действия, указанного в ревардактионид. Если вы решите отображать другие действия, и пользователь выполняет их, вознаграждение должно быть равно нулю.

* Рассмотрите непредвиденные **последствия**: Создайте функции поощрений, которые ведут к полученным результатам с помощью [этического и ответственного за использование](ethics-responsible-use.md).

* **Использовать добавочные вознаграждения**: Добавление частичных вознаграждений для меньших вариантов поведения пользователей помогает персонализации достичь лучших вознаграждений. Такое частичное вознаграждение сообщает алгоритму о том, что он приближает пользователя к требуемому поведению.
    * Если вы отображаете список кинофильмов, то можно учитывать частичную заинтересованность пользователя, выражающуюся в наведении им указателя мыши на первый элемент списка для получения дополнительных сведений. За такое поведение можно присуждать оценку вознаграждения 0,1. 
    * Если пользователь откроет страницу, а затем покинет ее, оценку вознаграждения можно повысить до 0,2. 

## <a name="reward-wait-time"></a>Время ожидания результата

Персонализатор будет сопоставлять сведения о вызове Rank с вознаграждениями, отправленными в вызовах Reward, для обучения модели. Они могут поступать в разное время. Персонализатор ожидает в течение определенного времени, начиная с момента вызова Rank, даже если этот вызов Rank выполнялся как неактивное событие и активировался позже.

Если **время ожидания вознаграждения** истекает, когда еще не получено никакой информации о вознаграждении, к такому событию применяется значение вознаграждения по умолчанию в целях обучения. Максимальная длительность ожидания составляет 6 дней.

## <a name="best-practices-for-reward-wait-time"></a>Рекомендации по времени ожидания наград

Следуйте этим рекомендациям, чтобы получить оптимальные результаты.

* Время ожидания вознаграждения должно быть максимально коротким, но достаточным для получения отклика пользователя. 

* Время ожидания не должно меньшим, чем требуется для получения отклика. Например, если некоторая часть вознаграждения присуждается за 1 минуту просмотра, длительность эксперимента должна быть по меньшей мере вдвое дольше.

## <a name="next-steps"></a>Дальнейшие действия

* [Обучение с подкреплением](concepts-reinforcement-learning.md) 
* [Работа с API ранжирования](https://westus2.dev.cognitive.microsoft.com/docs/services/personalizer-api/operations/Rank/console)
* [Работа с API вознаграждения](https://westus2.dev.cognitive.microsoft.com/docs/services/personalizer-api/operations/Reward)
