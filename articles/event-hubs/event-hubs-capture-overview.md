---
title: Сбор событий потоковой передачи — Центры событий Azure | Документация Майкрософт
description: В этой статье описывается функция сбора, которая позволяет записывать события потоковой передачи из Центров событий Azure
services: event-hubs
documentationcenter: ''
author: ShubhaVijayasarathy
manager: timlt
editor: ''
ms.assetid: e53cdeea-8a6a-474e-9f96-59d43c0e8562
ms.service: event-hubs
ms.workload: na
ms.custom: seodec18
ms.tgt_pltfrm: na
ms.devlang: na
ms.topic: article
ms.date: 12/06/2018
ms.author: shvija
ms.openlocfilehash: 442ceab68851dc108d327cdf212dcf58d5b31084
ms.sourcegitcommit: a819209a7c293078ff5377dee266fa76fd20902c
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 09/16/2019
ms.locfileid: "71008568"
---
# <a name="capture-events-through-azure-event-hubs-in-azure-blob-storage-or-azure-data-lake-storage"></a>Сбор событий из Центров событий Azure в хранилище BLOB-объектов Azure или Azure Data Lake Storage
Центры событий Azure позволяют автоматически записывать определенный объем потоковых данных из Центров событий в учетную запись [хранилища BLOB-объектов Azure](https://azure.microsoft.com/services/storage/blobs/) или [Azure Data Lake Storage](https://azure.microsoft.com/services/data-lake-store/) с указанным интервалом времени и размера. Настройка функции "Сбор" выполняется быстро, ее использование не влечет дополнительных административных расходов, а масштабирование осуществляется автоматически на основе [единиц пропускной способности](event-hubs-scalability.md#throughput-units) Центров событий. Функция "Сбор" в Центрах событий — это самый удобный способ передачи потоковых данных в Azure. Он позволяет сосредоточиться на обработке данных, а не на их записи.

Кроме того, функция "Сбор" в Центрах событий обеспечивает обработку конвейеров в режиме реального времени и на основе пакетов в одном потоке, благодаря чему вы можете создавать решения, масштабируемые по мере необходимости. Независимо от того, что вам нужно (создать системы на основе пакетов с учетом будущих потребностей в обработке данных в режиме реального времени или добавить эффективный холодный путь к имеющемуся решению для обработки в режиме реального времени), функция "Сбор" в Центрах событий упрощает работу с потоковыми данными.

## <a name="how-event-hubs-capture-works"></a>Как работает функция "Сбор" в Центрах событий

Центры событий — это устойчивый буфер для хранения входящих данных телеметрии в течение определенного времени, подобный распределенному журналу. Масштабирование в Центрах событий выполняется в рамках [модели секционированных потребителей](event-hubs-scalability.md#partitions). Каждая секция — это независимый сегмент данных, потребление которого осуществляется отдельно. По истечении настроенного срока хранения эти данные устаревают, поэтому определенный концентратор событий никогда не заполняется полностью.

Функция "Сбор" в Центрах событий позволяет указать собственную учетную запись хранилища BLOB-объектов Azure и контейнер, или учетную запись Azure Data Lake Store, используемые для хранения собранных данных. Эти учетные записи могут находиться в том же регионе, что и концентратор событий, или в другом. Это дает гибкие возможности использования функции "Сбор" в Центрах событий.

Записанные данные записываются в формате [Apache Avro][Apache Avro] : компактный и быстрый двоичный формат, обеспечивающий широкие возможности структуры данных со встроенной схемой. Этот формат широко используется в экосистеме Hadoop, Stream Analytics и фабрике данных Azure. Работа с Avro более подробно описана далее в этой статье.

### <a name="capture-windowing"></a>Управление окнами в записи

В функции "Сбор" в Центрах событий можно настроить окно управления сбором. Это окно с минимальным размером и продолжительностью, для которого предусмотрена политика "побеждает первый". Это означает, что первый обнаруженный триггер активирует операцию записи. При наличии окна записи размером в 100 МБ и продолжительностью 15 минут для отправки данных со скоростью 1 МБ/с сначала используется окно размера, а затем — окно времени. Запись каждой секции выполняется отдельно, а запись выполненного блочного BLOB-объекта осуществляется в процессе записи. Имя блочного BLOB-объекта зависит от времени создания записи. Соглашение об именовании хранилища выглядит следующим образом:

```
{Namespace}/{EventHub}/{PartitionId}/{Year}/{Month}/{Day}/{Hour}/{Minute}/{Second}
```

Обратите внимание, что значения даты дополняются нулями. Пример имени файла может выглядеть так:

```
https://mystorageaccount.blob.core.windows.net/mycontainer/mynamespace/myeventhub/0/2017/12/08/03/03/17.avro
```

В случае, если большой двоичный объект службы хранилища Azure временно недоступен, запись концентраторов событий сохраняет данные в течение срока хранения данных, настроенного в концентраторе событий, и заполняет данные, когда учетная запись хранения снова станет доступной.

### <a name="scaling-to-throughput-units"></a>Масштабирование единиц пропускной способности

Трафик Центров событий контролируется с помощью [единиц пропускной способности](event-hubs-scalability.md#throughput-units). Одна единица пропускной способности разрешает передачу до 1 МБ/с или 1000 событий/с для входящих данных или до 2 МБ/с или 2000 событий/с для исходящих данных. Для стандартных концентраторов событий можно настроить 1-20 единиц пропускной способности, и вы можете приобрести дополнительные [запросы в службу поддержки][support request]по увеличению квоты. Использование единиц пропускной способности свыше приобретенного количества регулируется. Функция "Сбор" в Центрах событий копирует данные непосредственно из внутреннего хранилища Центров событий. При этом выполняется обход квоты на единицы пропускной способности для исходящего трафика, а этот трафик сохраняется для других средств обработки, например Stream Analytics или Spark.

После настройки функция "Сбор" в Центрах событий автоматически запускается при отправке первого события и продолжает работать. Чтобы позволить операции последующей обработки установить, что процесс выполняется, при отсутствии данных Центры событий записывают пустые файлы. Этот процесс обеспечивает прогнозируемую периодичность и позволяет получить маркер, необходимый для пакетных обработчиков.

## <a name="setting-up-event-hubs-capture"></a>Настройка функции "Сбор" в Центрах событий

Запись можно настроить при создании концентратора событий с помощью [портала Azure](https://portal.azure.com) или с помощью шаблонов Azure Resource Manager. Дополнительные сведения см. в следующих статьях:

- [Включение функции "Сбор" в Центрах событий с помощью портала Azure](event-hubs-capture-enable-through-portal.md)
- [Создание пространства имен Центров событий с концентратором событий и включение записи с помощью шаблона Azure Resource Manager](event-hubs-resource-manager-namespace-event-hub-enable-capture.md)


## <a name="exploring-the-captured-files-and-working-with-avro"></a>Просмотр собранных файлов и работа с Avro

Функция "Сбор" в Центрах событий создает файлы в формате Avro, как указано в настроенном окне времени. Эти файлы можно просмотреть в любом средстве, например в [обозревателе хранилищ Azure][Azure Storage Explorer]. Чтобы выполнить определенные действия с этими файлами, их можно скачать локально.

Файлы, созданные записью с помощью функции "Сбор" в Центрах событий, имеют следующую схему Avro.

![Схема Avro][3]

Файлы Avro можно легко просмотреть с помощью [средств Avro][Avro Tools] (JAR-файл) из Apache. Можно также использовать [Apache][Apache Drill] для упрощения работы на основе SQL или [Apache Spark][Apache Spark] для выполнения сложной распределенной обработки принимаемых данных. 

### <a name="use-apache-drill"></a>Использование Apache Drill

[Apache детализируется][Apache Drill] «обработчиком SQL-запросов с открытым кодом для просмотра больших данных», который может запрашивать структурированные и частично структурированные данные в любом месте. Этот механизм может работать как отдельный узел или как огромный кластер для высокой производительности.

Доступна встроенная поддержка хранилища BLOB-объектов Azure, которая упрощает запрос данных в файле Avro, как описано в документации:

[Apache Drill. Подключаемый модуль хранилища BLOB-объектов Azure][Apache Drill: Azure Blob Storage Plugin]

Чтобы облегчить запрос захваченных файлов, вы можете создать и запустить виртуальную машину с включенной Apache Drill через контейнер для доступа к хранилищу BLOB-объектов Azure.

https://github.com/yorek/apache-drill-azure-blob

Полный законченный пример доступен в потоковой передаче масштабируемого репозитория.

[Streaming at Scale with Event Hubs Capture] (Потоковая передача в нужном масштабе с помощью функции "Сбор" в Центрах событий)

### <a name="use-apache-spark"></a>Использование Apache Spark

[Apache Spark][Apache Spark] — это «единый модуль аналитики для обработки крупномасштабных данных». Он поддерживает разные языки, включая SQL, и может легко связываться с хранилищем BLOB-объектов Azure. Существует два варианта для запуска Apache Spark в Azure, и оба обеспечивают легкий доступ к хранилищу BLOB-объектов Azure.

- [HDInsight. Адресные файлы в службе хранилища Azure][HDInsight: Address files in Azure storage]
- [Azure Databricks. Хранилище BLOB-объектов Azure][Azure Databricks: Azure Blob Storage]

### <a name="use-avro-tools"></a>Использование средств Avro

[Средства Avro][Avro Tools] доступны в виде пакета JAR. После того как вы загрузили этот JAR-файл, чтобы просмотреть схему определенного файла Avro, выполните следующую команду:

```shell
java -jar avro-tools-1.9.1.jar getschema <name of capture file>
```

Эта команда возвращает следующее:

```json
{

    "type":"record",
    "name":"EventData",
    "namespace":"Microsoft.ServiceBus.Messaging",
    "fields":[
                 {"name":"SequenceNumber","type":"long"},
                 {"name":"Offset","type":"string"},
                 {"name":"EnqueuedTimeUtc","type":"string"},
                 {"name":"SystemProperties","type":{"type":"map","values":["long","double","string","bytes"]}},
                 {"name":"Properties","type":{"type":"map","values":["long","double","string","bytes"]}},
                 {"name":"Body","type":["null","bytes"]}
             ]
}
```

Средства Avro можно также использовать для преобразования файлов в формат JSON и выполнения других задач обработки.

Чтобы выполнить более расширенную обработку, скачайте и установите Avro для определенной платформы. На момент написания статьи средства Avro доступны для следующих платформ: C, C++, C\#, Java, NodeJS, Perl, PHP, Python и Ruby.

Apache Avro содержит полный начало работы руководства по [Java][Java] и [Python][Python]. Дополнительные сведения см. в статье [Пошаговое руководство. Использование функции "Сбор" в Центрах событий с Python](event-hubs-capture-python.md).

## <a name="how-event-hubs-capture-is-charged"></a>Выставление счета за использование функции "Сбор" в Центрах событий

Выставление счета за использование функции "Сбор" в Центрах событий осуществляется подобно тарификации за единицы пропускной способности, то есть каждый час. Размер платы прямо пропорционален количеству единиц пропускной способности, приобретенных для пространства имен. Так же как и с единицами пропускной способности, единицы измерения при использовании функции "Сбор" в Центрах событий можно регулировать, чтобы обеспечить соответствующую производительность. Единицы измерения действуют совместно. Дополнительные сведения о ценах см. на странице цен на [Центры событий](https://azure.microsoft.com/pricing/details/event-hubs/). 

Обратите внимание, что захват не потребляет квоту исходящего трафика, так как счет выставляется отдельно. 

## <a name="integration-with-event-grid"></a>Интеграция со службой "Сетка событий" 

Можно создать подписку на Сетку событий Azure с пространством имен Центров событий в качестве источника. Сведения о том, как создать подписку на Сетку событий с помощью концентратора событий в качестве источника и приложения Функций Azure в качестве приемника, см. в руководстве [Обработка и перемещение записанных данных из концентраторов событий в Хранилище данных SQL с помощью служб "Сетка событий" и "Функции Azure"](store-captured-data-data-warehouse.md).

## <a name="next-steps"></a>Следующие шаги

Функция "Сбор" в Центрах событий — это самый быстрый способ передать данные в Azure. С помощью знакомых средств и платформ (Azure Data Lake, фабрики данных Azure и Azure HDInsight) можно выполнять необходимую пакетную обработку и другие операции анализа в любом масштабе.

Дополнительные сведения о Центрах событий см. в следующих источниках:

* [Отправка событий в концентраторы событий Azure с помощью платформы .NET Framework](event-hubs-dotnet-framework-getstarted-send.md)
* [Общие сведения о Центрах событий][Event Hubs overview]

[Apache Avro]: https://avro.apache.org/
[Apache Drill]: https://drill.apache.org/
[Apache Spark]: https://spark.apache.org/
[support request]: https://portal.azure.com/?#blade/Microsoft_Azure_Support/HelpAndSupportBlade
[Azure Storage Explorer]: https://azurestorageexplorer.codeplex.com/
[3]: ./media/event-hubs-capture-overview/event-hubs-capture3.png
[Avro Tools]: https://www.apache.org/dist/avro/stable/java/avro-tools-1.9.1.jar
[Java]: https://avro.apache.org/docs/current/gettingstartedjava.html
[Python]: https://avro.apache.org/docs/current/gettingstartedpython.html
[Event Hubs overview]: event-hubs-what-is-event-hubs.md
[HDInsight: Address files in Azure storage]:https://docs.microsoft.com/azure/hdinsight/hdinsight-hadoop-use-blob-storage#address-files-in-azure-storage
[Azure Databricks: Azure Blob Storage]:https://docs.databricks.com/spark/latest/data-sources/azure/azure-storage.html
[Apache Drill: Azure Blob Storage Plugin]:https://drill.apache.org/docs/azure-blob-storage-plugin/
[Streaming at Scale with Event Hubs Capture]: https://github.com/yorek/streaming-at-scale/tree/master/event-hubs-capture (Потоковая передача в нужном масштабе с помощью функции "Сбор" в Центрах событий)
