---
title: Как работает Azure Dev Spaces и настроена
services: azure-dev-spaces
ms.date: 03/04/2019
ms.topic: conceptual
description: Описание процессов, Azure Dev Spaces и их настройки в файле конфигурации аздс. YAML
keywords: аздс. YAML, Azure Dev Spaces, пространства разработки, Docker, Kubernetes, Azure, AKS, служба Azure Kubernetes, контейнеры
ms.openlocfilehash: 9efae0e9d6bc53e08dce604fa79aa29e158ecabd
ms.sourcegitcommit: 653e9f61b24940561061bd65b2486e232e41ead4
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 11/21/2019
ms.locfileid: "74280138"
---
# <a name="how-azure-dev-spaces-works-and-is-configured"></a>Как работает Azure Dev Spaces и настроена

Разработка приложения Kubernetes может оказаться сложной задачей. Требуются файлы конфигурации DOCKER и Kubernetes. Необходимо выяснить, как протестировать приложение локально и взаимодействовать с другими зависимыми службами. Может потребоваться выполнить разработку и тестирование нескольких служб одновременно и с группой разработчиков.

Azure Dev Spaces помогает разрабатывать, развертывать и отлаживать приложения Kubernetes непосредственно в службе Kubernetes Azure (AKS). Azure Dev Spaces также позволяет группе совместно использовать пространство разработки. Совместное использование пространства разработки в группе позволяет отдельным членам группы разрабатывать изолированные решения без необходимости репликации или макетирования зависимостей или других приложений в кластере.

Azure Dev Spaces создает и использует файл конфигурации для развертывания, запуска и отладки приложений Kubernetes в AKS. Этот файл конфигурации находится в коде приложения и может быть добавлен в систему управления версиями.

В этой статье описываются процессы, Azure Dev Spaces и способы настройки этих процессов в файле конфигурации Azure Dev Spaces. Чтобы быстро запустить Azure Dev Spaces и увидеть его на практике, выполните одно из кратких руководств.

* [Java с интерфейсом командной строки и Visual Studio Code](quickstart-java.md)
* [.NET Core с интерфейсом командной строки и Visual Studio Code](quickstart-netcore.md)
* [.NET Core с Visual Studio](quickstart-netcore-visualstudio.md)
* [Node. js с интерфейсом командной строки и Visual Studio Code](quickstart-nodejs.md)

## <a name="how-azure-dev-spaces-works"></a>Принцип работы Azure Dev Spaces

Azure Dev Spaces содержит два отдельных компонента, с которыми вы взаимодействуете: контроллер и клиентские средства.

![Компоненты Azure Dev Spaces](media/how-dev-spaces-works/components.svg)

Контроллер выполняет следующие действия:

* Управляет созданием и выбором пространства разработки.
* Устанавливает диаграмму Helm приложения и создает объекты Kubernetes.
* Создает образ контейнера приложения.
* Развертывает приложение в AKS.
* Выполняет добавочные сборки и перезапускается при изменении исходного кода.
* Управляет журналами и трассировками HTTP.
* Перенаправляет stdout и stderr на клиентские средства.
* Позволяет членам команды создавать дочерние пространства разработки, производные от родительского пространства разработки.
* Настраивает маршрутизацию для приложений в пределах пространства, а также для родительских и дочерних пространств.

Контроллер находится за пределами AKS. Он управляет поведением и связью между клиентскими инструментами и кластером AKS. Контроллер включается с помощью Azure CLI при подготовке кластера к использованию Azure Dev Spaces. После его включения можно взаимодействовать с ним с помощью клиентских средств.

Средства на стороне клиента позволяют пользователю:
* Создайте Dockerfile, Helm диаграмму и файл конфигурации Azure Dev Spaces для приложения.
* Создайте родительские и дочерние пространства разработки.
* Сообщите контроллеру о необходимости создания и запуска приложения.

Пока приложение работает, клиентские средства также:
* Получает и отображает stdout и stderr из приложения, работающего в AKS.
* Использует [переадресацию через порт](https://kubernetes.io/docs/tasks/access-application-cluster/port-forward-access-application-cluster/) , чтобы разрешить веб-доступ к приложению с помощью http:\//ЛОКАЛХОСТ.
* Присоединяет отладчик к работающему приложению в AKS.
* Синхронизирует исходный код с пространством разработки при обнаружении изменений для добавочных сборок, что позволяет быстро выполнить итерацию.

Клиентские средства можно использовать из командной строки в составе команды `azds`. Клиентские средства также можно использовать с:

* Visual Studio Code с помощью [расширения Azure dev Spaces](https://marketplace.visualstudio.com/items?itemName=azuredevspaces.azds).
* Visual Studio с [средства Visual Studio для Kubernetes](https://aka.ms/get-vsk8stools).

Ниже приведен базовый поток для настройки и использования Azure Dev Spaces.
1. Подготовка кластера AKS для Azure Dev Spaces
1. Подготовка кода к запуску на Azure Dev Spaces
1. Выполнение кода в пространстве разработки
1. Отладка кода в пространстве разработки
1. Совместное использование пространства разработки

В каждом из приведенных ниже разделов будут рассмотрены дополнительные сведения о работе Azure Dev Spaces.

## <a name="prepare-your-aks-cluster"></a>Подготовка кластера AKS

Подготовка кластера AKS включает в себя:
* Проверка того, что кластер AKS находится в регионе [, поддерживаемом Azure dev Spaces][supported-regions].
* Проверка того, что вы используете Kubernetes 1.10.3 или более поздней версии.
* Включение Azure Dev Spaces в кластере с помощью `az aks use-dev-spaces`

Дополнительные сведения о создании и настройке кластера AKS для Azure Dev Spaces см. в одном из руководств по началу работы:
* [Начало работы с Azure Dev Spaces с помощью Java](get-started-java.md)
* [Приступая к работе с Azure Dev Spaces с помощью .NET Core и Visual Studio](get-started-netcore-visualstudio.md)
* [Приступая к работе с Azure Dev Spaces .NET Core](get-started-netcore.md)
* [Приступая к работе с Azure Dev Spaces Node. js](get-started-nodejs.md)

Если Azure Dev Spaces включен в кластере AKS, он устанавливает контроллер для кластера. Контроллер является отдельным ресурсом Azure за пределами кластера и выполняет следующие действия для ресурсов в кластере:

* Создает или назначает пространство имен Kubernetes для использования в качестве пространства разработки.
* Удаляет любое пространство имен Kubernetes с именем *аздс*, если оно существует, и создает новое.
* Развертывает конфигурацию веб-перехватчика Kubernetes.
* Развертывает сервер допуска веб-перехватчика.
    

Он также использует тот же субъект-службу, который используется кластером AKS для выполнения вызовов служб к другим компонентам Azure Dev Spaces.

![Azure Dev Spaces подготовить кластер](media/how-dev-spaces-works/prepare-cluster.svg)

Чтобы использовать Azure Dev Spaces, необходимо иметь по крайней мере одно пространство разработки. Azure Dev Spaces использует пространства имен Kubernetes в кластере AKS для пространств разработки. При установке контроллера будет предложено создать новое пространство имен Kubernetes или выбрать существующее пространство имен для использования в качестве первого пространства разработки. Если пространство имен обозначено как пространство разработки, контроллер добавляет метку *azds.IO/Space=true* в это пространство имен, чтобы обозначить ее как пространство разработки. Начальное пространство разработки, которое вы создаете или назначаете, выбирается по умолчанию после подготовки кластера. Если выбрано место, оно используется Azure Dev Spaces для создания новых рабочих нагрузок.

По умолчанию контроллер создает пространство разработки с именем *Default* , обновляя существующее пространство имен Kubernetes *по умолчанию* . С помощью клиентских средств можно создавать новые пространства для разработки и удалять существующие пространства разработки. Из-за ограничения в Kubernetes не удается удалить пространство разработки *по умолчанию* . Контроллер также удаляет все существующие пространства имен Kubernetes с именем *аздс* , чтобы избежать конфликтов с командой `azds`, используемой клиентскими средствами.

Сервер допуска Kubernetes веб-перехватчика используется для вставки модулей Pod с тремя контейнерами во время развертывания для инструментирования: девспацес-proxy, контейнер девспацес-proxy-init и контейнер девспацес-Build. **Все три из этих контейнеров работают с корневым доступом к кластеру AKS.** Они также используют тот же субъект-службу, который используется кластером AKS для выполнения вызовов служб к другим компонентам Azure Dev Spaces.

![Azure Dev Spaces сервера допуска веб-перехватчика Kubernetes](media/how-dev-spaces-works/kubernetes-webhook-admission-server.svg)

Контейнер девспацес-Proxy — это контейнер расширения, который обрабатывает весь входящий и исходящий TCP-трафик контейнера приложения и помогает выполнять маршрутизацию. Контейнер девспацес-proxy перенаправляет сообщения HTTP, если используются определенные пробелы. Например, он может помочь маршрутизировать сообщения HTTP между приложениями в родительских и дочерних пространствах. Весь трафик, отличный от HTTP, проходит через девспацес-proxy без изменений. Контейнер девспацес-proxy также регистрирует все входящие и исходящие HTTP-сообщения и отправляет их в средства на стороне клиента в виде трассировок. Затем эти трассировки могут быть просмотрены разработчиком для проверки поведения приложения.

Контейнер девспацес-proxy-init — это [контейнер инициализации](https://kubernetes.io/docs/concepts/workloads/pods/init-containers/) , который добавляет дополнительные правила маршрутизации на основе иерархии пространства в контейнер приложения. Он добавляет правила маршрутизации, обновив файл */etc/resolv.conf* контейнера приложений и конфигурацию Iptables перед запуском. Обновления */etc/resolv.conf* РАЗРЕШАЮТ DNS-разрешение служб в родительских пространствах. Обновления конфигурации iptables гарантируют, что весь входящий и исходящий TCP-трафик в контейнер приложения направляется через девспацес прокси. Все обновления из девспацес-proxy-init происходят в дополнение к правилам, которые Kubernetes добавляет.

Контейнер девспацес-Build — это контейнер инициализации, который подключен к исходному коду проекта и сокету DOCKER. Исходный код проекта и доступ к DOCKER позволяют создавать контейнер приложения непосредственно модулем Pod.

> [!NOTE]
> Azure Dev Spaces использует тот же узел для создания контейнера приложения и его запуска. В результате Azure Dev Spaces не требуется внешний реестр контейнеров для сборки и запуска приложения.

Сервер допуска веб-перехватчиков Kubernetes прослушивает все новые Pod, созданные в кластере AKS. Если этот модуль будет развернут в любом пространстве имен с меткой *azds.IO/Space=true* , он внедряет этот модуль с дополнительными контейнерами. Контейнер девспацес-Build вставляется только в том случае, если контейнер приложения запускается с помощью клиентских средств на стороне клиента.

После подготовки кластера AKS можно использовать средства на стороне клиента для подготовки и выполнения кода в пространстве разработки.

## <a name="prepare-your-code"></a>Подготовка кода

Чтобы запустить приложение в пространстве разработки, оно должно быть контейнерным и необходимо определить, как оно должно быть развернуто в Kubernetes. Чтобы контейнеризовать приложение, вам потребуется Dockerfile. Чтобы определить, как приложение развертывается в Kubernetes, требуется [Диаграмма Helm](https://docs.helm.sh/). Чтобы упростить создание диаграммы Dockerfile и Helm для приложения, клиентские средства предоставляют команду `prep`:

```cmd
azds prep --public
```

Команда `prep` выполнит просмотр файлов в проекте и попытается создать диаграмму Dockerfile и Helm для запуска приложения в Kubernetes. В настоящее время команда `prep` создаст диаграмму Dockerfile и Helm со следующими языками:

* Java:
* Node.js
* .NET Core

*Необходимо* выполнить команду `prep` из каталога, содержащего исходный код. Выполнение команды `prep` из правильного каталога позволяет клиентским инструментам обнаруживать язык и создавать соответствующие Dockerfile для контейнеризовать приложения. Можно также выполнить команду `prep` из каталога, содержащего файл *POM. XML* для проектов Java.

При выполнении команды `prep` из каталога, который не содержит исходный код, средства на стороне клиента не будут создавать Dockerfile. Кроме того, будет выведено сообщение об ошибке: *Dockerfile не удалось создать из-за неподдерживаемого языка*. Эта ошибка также возникает, если клиентские средства не распознают тип проекта.

При выполнении команды `prep` можно указать флаг `--public`. Этот флаг указывает контроллеру создать конечную точку, доступную в Интернете, для этой службы. Если этот флаг не указан, служба доступна только в пределах кластера или с помощью туннеля localhost, созданного клиентскими средствами. Это поведение можно включить или отключить после выполнения команды `prep`, обновив созданную диаграмму Helm.

Команда `prep` не заменит существующие диаграммы файлы dockerfile или Helm, имеющиеся в проекте. Если существующая Dockerfile или Helm диаграмма использует те же соглашения об именовании, что и файлы, созданные командой `prep`, команда `prep` пропустит создание этих файлов. В противном случае команда `prep` создаст собственную диаграмму Dockerfile или Helm вместе с существующими файлами.

Команда `prep` также создаст файл `azds.yaml` в корне проекта. Azure Dev Spaces использует этот файл для создания, установки, настройки и запуска приложения. Этот файл конфигурации содержит сведения о расположении диаграммы Dockerfile и Helm, а также дополнительную настройку на основе этих артефактов.

Ниже приведен пример файла аздс. YAML, созданного с помощью [примера приложения .NET Core](https://github.com/Azure/dev-spaces/tree/master/samples/dotnetcore/getting-started/webfrontend).

```yaml
kind: helm-release
apiVersion: 1.1
build:
  context: .
  dockerfile: Dockerfile
install:
  chart: charts/webfrontend
  values:
  - values.dev.yaml?
  - secrets.dev.yaml?
  set:
    replicaCount: 1
    image:
      repository: webfrontend
      tag: $(tag)
      pullPolicy: Never
    ingress:
      annotations:
        kubernetes.io/ingress.class: traefik-azds
      hosts:
        # This expands to [space.s.][rootSpace.]webfrontend.<random suffix>.<region>.azds.io
        # Customize the public URL by changing the 'webfrontend' text between the $(rootSpacePrefix) and $(hostSuffix) tokens
        # For more information see https://aka.ms/devspaces/routing
        - $(spacePrefix)$(rootSpacePrefix)webfrontend$(hostSuffix)
configurations:
  develop:
    build:
      dockerfile: Dockerfile.develop
      useGitIgnore: true
      args:
        BUILD_CONFIGURATION: ${BUILD_CONFIGURATION:-Debug}
    container:
      sync:
      - "**/Pages/**"
      - "**/Views/**"
      - "**/wwwroot/**"
      - "!**/*.{sln,csproj}"
      command: [dotnet, run, --no-restore, --no-build, --no-launch-profile, -c, "${BUILD_CONFIGURATION:-Debug}"]
      iterate:
        processesToKill: [dotnet, vsdbg]
        buildCommands:
        - [dotnet, build, --no-restore, -c, "${BUILD_CONFIGURATION:-Debug}"]
```

Файл `azds.yaml`, созданный командой `prep`, должен прекрасно работать для простого сценария разработки с одним проектом. Если в конкретном проекте увеличилась сложность, может потребоваться обновить этот файл после выполнения команды `prep`. Например, в проекте может потребоваться внести некоторые изменения в процесс сборки или запуска в зависимости от потребностей разработки или отладки. Кроме того, в проекте может быть несколько приложений, для которых требуется несколько процессов сборки или другое содержимое сборки.

## <a name="run-your-code"></a>Выполнение кода

Чтобы запустить код в пространстве разработки, выполните команду `up` в том же каталоге, где находится файл `azds.yaml`:

```cmd
azds up
```

Команда `up` отправляет исходные файлы приложения и другие артефакты, необходимые для сборки и запуска проекта в пространстве разработки. В этом случае контроллер в сфере разработки:

1. Создает объекты Kubernetes для развертывания приложения.
1. Создает контейнер для приложения.
1. Развертывает приложение в пространстве разработки.
1. Создает общедоступное DNS-имя для конечной точки приложения, если оно настроено.
1. Использует *переадресацию через порт* для предоставления доступа к конечной точке приложения с помощью http://localhost.
1. Перенаправляет stdout и stderr на клиентские средства.


### <a name="starting-a-service"></a>Запуск службы

При запуске службы в пространстве разработки клиентские средства и контроллер работают в координации для синхронизации исходных файлов, создания контейнеров и объектов Kubernetes и запуска приложения.

На более детальном уровне ниже показано, что происходит при запуске `azds up`:

1. Файлы синхронизируются с компьютера пользователя в хранилище файлов Azure, которое является уникальным для кластера AKS пользователя. Передаются исходный код, диаграмма Helm и файлы конфигурации. Дополнительные сведения о процессе синхронизации можно найти в следующем разделе.
1. Контроллер создает запрос на запуск нового сеанса. Этот запрос содержит несколько свойств, включая уникальный идентификатор, имя пространства, путь к исходному коду и флаг отладки.
1. Контроллер заменяет заполнитель *$ (Tag)* в диаграмме Helm уникальным идентификатором сеанса и устанавливает диаграмму Helm для службы. Добавление ссылки на уникальный идентификатор сеанса в диаграмму Helm позволяет связать контейнер, развернутый в кластере AKS, с этим конкретным сеансом, чтобы он был связан с запросом сеанса и связанной информацией.
1. Во время установки диаграммы Helm сервер допуска веб-перехватчика Kubernetes добавляет дополнительные контейнеры в модуль приложения для инструментирования и доступа к исходному коду проекта. Добавляются контейнеры девспацес-proxy и девспацес-proxy-init, чтобы обеспечить трассировку HTTP и маршрутизацию пространства. Контейнер девспацес-Build добавляется, чтобы предоставить Pod доступ к экземпляру DOCKER и исходному коду проекта для создания контейнера приложения.
1. Когда запускается модуль приложения, контейнер девспацес-Build и девспацес-proxy-init используются для построения контейнера приложения. Затем будут запущены контейнеры приложений и контейнеры девспацес-proxy.
1. После запуска контейнера приложений функция на стороне клиента использует функцию *переадресации порта* Kubernetes, чтобы предоставить доступ к приложению по протоколу HTTP http://localhost. Этот порт перенаправляет подключение компьютера разработчика к службе в пространстве разработки.
1. Когда все контейнеры в модуле Pod запущены, служба выполняется. На этом этапе Клиентская функция начинает потоковую передачу трассировок HTTP, stdout и stderr. Эти сведения отображаются клиентской функциональностью для разработчика.

### <a name="updating-a-running-service"></a>Обновление работающей службы

Во время работы службы Azure Dev Spaces может обновить эту службу, если какой-либо из исходных файлов проекта изменится. Пространства разработки также обрабатывают обновление службы по-разному в зависимости от типа изменяемого файла. Существует три способа, с помощью которых пространства разработки могут обновлять работающую службу:

* Непосредственное обновление файла
* Перестроение и перезапуск процесса приложения в контейнере выполняющегося приложения
* Перестроение и повторное развертывание контейнера приложения

![Azure Dev Spaces синхронизации файлов](media/how-dev-spaces-works/file-sync.svg)

Некоторые файлы проектов, являющиеся статическими ресурсами, такие как файлы HTML, CSS и CSHTML, можно обновлять непосредственно в контейнере приложения, не перезапуская ничего. При изменении статического ресурса новый файл синхронизируется с пространством разработки, а затем используется работающим контейнером.

Изменения в файлах, таких как исходный код или файлы конфигурации приложения, могут быть применены путем перезапуска процесса приложения в работающем контейнере. После синхронизации этих файлов процесс приложения перезапускается в запущенном контейнере с помощью процесса *девхостажент* . При первоначальном создании контейнера приложения контроллер заменяет команду запуска приложения другим процессом, именуемым *девхостажент*. Фактический процесс приложения запускается как дочерний процесс в разделе *девхостажент*, и его выходные данные исправляются с помощью выходных данных *девхостажент*. Процесс *девхостажент* также является частью пространств разработки и может выполнять команды в работающем контейнере от имени пространств разработки. При выполнении перезапуска *девхостажент*:

* Останавливает текущий процесс или процессы, связанные с приложением
* Перестраивает приложение
* Перезапускает процесс или процессы, связанные с приложением

Способ, который *девхостажент* выполняет предыдущие шаги, настраивается в файле конфигурации `azds.yaml`. Эта конфигурация подробно описана в следующем разделе.

Для обновления файлов проекта, таких как файлы dockerfile, CSPROJ или любая часть диаграммы Helm, требуется перестроение и повторное развертывание контейнера приложения. Когда один из этих файлов синхронизируется с пространством разработки, контроллер выполняет команду [обновления Helm](https://helm.sh/docs/intro/using_helm/#helm-upgrade-and-helm-rollback-upgrading-a-release-and-recovering-on-failure) , а контейнер приложения перестраивается и развертывается повторно.

### <a name="file-synchronization"></a>Синхронизация файлов

При первом запуске приложения в пространстве разработки передаются все исходные файлы приложения. Пока приложение выполняется, а затем перезапускается, передаются только измененные файлы. Для координации этого процесса используются два файла: файл на стороне клиента и файл на стороне контроллера.

Файл на стороне клиента хранится во временном каталоге и именуется на основе хэша каталога проекта, который выполняется в пространствах разработки. Например, в Windows для вашего проекта будет иметься такой файл, как *Users\USERNAME\AppData\Local\Temp\1234567890abcdef1234567890abcdef1234567890abcdef1234567890abcdef.synclog* . В Linux файл на стороне клиента хранится в каталоге */tmp* . Каталог в macOS можно найти, выполнив команду `echo $TMPDIR`.

Этот файл имеет формат JSON и содержит:

* Запись для каждого файла проекта, который синхронизируется с пространством разработки
* ИДЕНТИФИКАТОР синхронизации
* Метка времени последней операции синхронизации

Каждая запись файла проекта содержит путь к файлу и его метку времени.

Файл на стороне контроллера хранится в кластере AKS. Он содержит идентификатор синхронизации и метку времени последней синхронизации.

Синхронизация происходит, когда метки времени синхронизации между файлами на стороне клиента и на стороне контроллера не совпадают. Во время синхронизации средства клиентской стороны перебирает записи файлов в файле на стороне клиента. Если метка времени файла находится после метки времени синхронизации, этот файл синхронизируется с пространством разработки. После завершения синхронизации отметка времени синхронизации обновляется как на стороне клиента, так и на стороне контроллера.

Если файл на стороне клиента отсутствует, все файлы проекта синхронизируются. Такое поведение позволяет принудительно выполнить полную синхронизацию, удалив клиентский файл.

### <a name="how-routing-works"></a>Принцип работы маршрутизации

Пространство разработки построено на основе AKS и использует те же [Основные понятия сети](../aks/concepts-network.md). Azure Dev Spaces также имеет централизованную службу *ингрессманажер* и развертывает свой собственный контроллер входящего трафика в кластере AKS. Служба *ингрессманажер* отслеживает кластеры AKS с помощью пространств разработки и дополняет контроллер Azure dev Spaces входящего трафика в кластере с помощью входящих объектов для маршрутизации модулей приложения. Контейнер девспацес-proxy в каждом модуле Pod добавляет заголовок HTTP `azds-route-as` для трафика HTTP в пространство разработчика, основанное на URL-адресе. Например, запрос к URL-адресу *http://azureuser.s.default.serviceA.fedcba09...azds.io* бы получил заголовок HTTP с `azds-route-as: azureuser`. Контейнер девспацес-proxy не добавляет заголовок `azds-route-as`, если он уже существует.

При выполнении HTTP-запроса к службе извне кластера запрос переходит на входной контроллер. Входной контроллер направляет запрос непосредственно в соответствующий Pod на основе входящих в него объектов и правил. Контейнер девспацес-proxy в Pod получает запрос, добавляет заголовок `azds-route-as` на основе URL-адреса, а затем направляет запрос в контейнер приложения.

При выполнении HTTP-запроса к службе из другой службы в кластере запрос сначала проходит через контейнер девспацес-proxy вызывающей службы. Контейнер девспацес-proxy просматривает HTTP-запрос и проверяет заголовок `azds-route-as`. На основе заголовка контейнер девспацес-proxy выполнит поиск IP-адреса службы, связанной со значением заголовка. Если IP-адрес найден, контейнер девспацес-proxy перенаправит запрос на этот IP-адрес. Если IP-адрес не найден, контейнер девспацес-proxy направляет запрос в родительский контейнер приложения.

Например, приложения *Service* a и *serviceB* развертываются в родительском пространстве разработки, именуемом *по умолчанию*. *Служба* a использует *serviceB* и выполняет HTTP-вызовы. Пользователь Azure создает Дочернее пространство разработки на основе пространства *по умолчанию* с именем *azureuser*. Пользователь Azure также развертывает собственную версию *Service* a в своем дочернем пространстве. Когда выполняется запрос к *http://azureuser.s.default.serviceA.fedcba09...azds.io* :

![Маршрутизация Azure Dev Spaces](media/how-dev-spaces-works/routing.svg)

1. Контроллер входящего трафика находит IP-адрес модуля Pod, связанного с URL-адресом *Service azureuser*.
1. Входной контроллер находит IP-адрес для Pod в пространстве разработки пользователя Azure и направляет запрос в модуль *Service a. azureuser* .
1. Контейнер девспацес-proxy в модуле *Service a. azureuser* получает запрос и добавляет `azds-route-as: azureuser` в качестве заголовка HTTP.
1. Контейнер девспацес-proxy в модуле *Service a. azureuser* выполняет маршрутизацию запроса в контейнер приложения *Service* a в модуль Service a *. azureuser* .
1. Приложение *Service* a в модуле *Service a. Azureuser* выполняет вызов *serviceB*. Приложение *Service* a также содержит код для сохранения существующего заголовка `azds-route-as`, который в данном случае `azds-route-as: azureuser`.
1. Контейнер девспацес-proxy в модуле *Service a. azureuser* получает запрос и выполняет поиск IP-адреса *serviceB* на основе значения заголовка `azds-route-as`.
1. Контейнер девспацес-proxy в модуле *Service a. azureuser* Pod не находит IP-адрес для *serviceB. azureuser*.
1. Контейнер девспацес-proxy в модуле *Service a. azureuser* ищет IP-адрес *serviceB* в родительском пространстве, то есть *serviceB. по умолчанию*.
1. Контейнер девспацес-proxy в модуле *Service a. azureuser* находит IP-адрес для *serviceB. Default* и направляет запрос в модуль *serviceB. Default* .
1. Контейнер девспацес-proxy в модуле *serviceB. Default* получает запрос и направляет запрос в контейнер приложения *ServiceB* в Pod *serviceB. Default* .
1. Приложение *serviceB* в модуле *serviceB. Default* возвращает ответ на модуль *Service a. azureuser* .
1. Контейнер девспацес-proxy в модуле *Service a. azureuser* получает ответ и направляет ответ в контейнер приложения *Service* a в модуле *Service a. azureuser* .
1. Приложение *Service* a получает ответ и возвращает собственный ответ.
1. Контейнер девспацес-proxy в модуле *Service a. azureuser* получает ответ от контейнера приложения *Service* a и направляет ответ исходному вызывающему объекту за пределами кластера.

Весь остальной TCP-трафик, который не проходит через контроллер входящего трафика, и девспацес контейнеры прокси без изменений.

### <a name="how-running-your-code-is-configured"></a>Как выполняется настройка кода

Azure Dev Spaces использует файл `azds.yaml` для установки и настройки службы. Контроллер использует свойство `install` в файле `azds.yaml` для установки диаграммы Helm и создания объектов Kubernetes:

```yaml
...
install:
  chart: charts/webfrontend
  values:
  - values.dev.yaml?
  - secrets.dev.yaml?
  set:
    replicaCount: 1
    image:
      repository: webfrontend
      tag: $(tag)
      pullPolicy: Never
    ingress:
      annotations:
        kubernetes.io/ingress.class: traefik-azds
      hosts:
      # This expands to [space.s.][rootSpace.]webfrontend.<random suffix>.<region>.azds.io
      # Customize the public URL by changing the 'webfrontend' text between the $(rootSpacePrefix) and $(hostSuffix) tokens
      # For more information see https://aka.ms/devspaces/routing
      - $(spacePrefix)$(rootSpacePrefix)webfrontend$(hostSuffix)
...
```

По умолчанию команда `prep` создаст диаграмму Helm. Он также устанавливает свойство *install. Chart* в каталог диаграммы Helm. Если вы хотите использовать диаграмму Helm в другом расположении, можно обновить это свойство, чтобы использовать это расположение.

При установке диаграмм Helm Azure Dev Spaces предоставляет способ переопределения значений в диаграмме Helm. Значения по умолчанию для диаграммы Helm находятся в `charts/APP_NAME/values.yaml`.

С помощью свойства *install. Values* можно перечислить один или несколько файлов, определяющих значения, которые необходимо заменить на диаграмме Helm. Например, если требуется настроить имя узла или базы данных, особенно при запуске приложения в пространстве разработки, можно использовать эту функцию переопределения. Вы также можете добавить *?* в конце любого имени файла, чтобы задать его как необязательный.

Свойство *install. Set* позволяет настроить одно или несколько значений, которые необходимо заменить на диаграмме Helm. Любые значения, настроенные в *install. Set* , переопределяют значения, настроенные в файлах, перечисленных в файле *install. Values*. Свойства в *install. Set* зависят от значений в Helm диаграмме и могут отличаться в зависимости от созданной диаграммы Helm.

В приведенном выше примере свойство *install. Set. replicaCount* указывает контроллеру, сколько экземпляров приложения нужно запустить в пространстве разработки. В зависимости от сценария вы можете увеличить это значение, но оно повлияет на подключение отладчика к модулю приложения. Дополнительные сведения см. в [статье Устранение неполадок](troubleshooting.md).

В созданной диаграмме Helm образ контейнера имеет значение *{{. Values. Image. репозиторий}}: {{. Значения. Image. Tag}}* . Файл `azds.yaml` определяет свойство *install. Set. Image. Tag* как *$ (Tag)* по умолчанию, которое используется как значение для *{{. Значения. Image. Tag}}* . Если задать свойство *install. Set. Image. Tag* таким образом, то при запуске Azure dev Spaces образ контейнера для приложения можно помечать по отдельности. В этом конкретном случае образ помечается как *\<значение из Image. репозиторий >: $ (Tag)* . Необходимо использовать переменную *$ (Tag)* в качестве значения *install. Set. Image. Tag* , чтобы пространства разработки распознавать и найти контейнер в кластере AKS.

В приведенном выше примере `azds.yaml` определяет *install. Set. входной. Hosts*. Свойство *install. Set. входящий. Hosts* определяет формат имени узла для общедоступных конечных точек. В этом свойстве также используются *$ (спацепрефикс)* , *$ (рутспацепрефикс)* и *$ (хостсуффикс)* , которые являются значениями, предоставленными контроллером. 

*$ (Спацепрефикс)* — это имя дочернего пространства разработки, которое принимает форму *спаценаме. s*. *$ (Рутспацепрефикс)* — это имя родительского пространства. Например, если *azureuser* является дочерним пространством *по умолчанию*, значение параметра *$ (рутспацепрефикс)* *по умолчанию* , а значение *$ (спацепрефикс)* — *azureuser. s*. Если пространство не является дочерним, *$ (спацепрефикс)* пусто. Например, если пространство *по умолчанию* не имеет родительского пространства, значение *$ (рутспацепрефикс)* будет *по умолчанию* , а значение *$ (спацепрефикс)* — пустым. *$ (Хостсуффикс)* — это DNS-суффикс, указывающий на Azure dev Spaces контроллер входящего трафика, который выполняется в кластере AKS. Этот DNS-суффикс соответствует DNS-записи с подстановочными знаками, например *\*. RANDOM_VALUE. ЕУС. аздс. IO*, который был создан при добавлении контроллера Azure dev Spaces в кластер AKS.

В приведенном выше `azds.yaml` файле можно также обновить *install. Set. входной. Hosts* , чтобы изменить имя узла приложения. Например, если требуется упростить имя узла приложения из *$ (спацепрефикс) $ (рутспацепрефикс) веб-интерфейс $ (хостсуффикс)* в *$ (спацепрефикс) $ (рутспацепрефикс) Web $ (хостсуффикс)* .

Чтобы создать контейнер для приложения, контроллер использует следующие разделы файла конфигурации `azds.yaml`:

```yaml
build:
  context: .
  dockerfile: Dockerfile
...
configurations:
  develop:
    build:
      dockerfile: Dockerfile.develop
      useGitIgnore: true
      args:
        BUILD_CONFIGURATION: ${BUILD_CONFIGURATION:-Debug}
...
```

Контроллер использует Dockerfile для сборки и запуска приложения.

Свойство *Build. Context* содержит каталог, в котором существует файлы dockerfile. Свойство *Build. dockerfile* определяет имя dockerfile для создания рабочей версии приложения. Свойство *Configurations. разработать. Build. dockerfile* настраивает имя dockerfile для версии разработки приложения.

Наличие различных файлы dockerfile для разработки и рабочей среды позволяет включать определенные вещи во время разработки и отключать эти элементы для рабочих развертываний. Например, можно включить отладку или более подробное ведение журнала во время разработки и отключить в рабочей среде. Вы также можете обновить эти свойства, если файлы dockerfile имеют разные имена или находятся в другом расположении.

Для ускорения итераций во время разработки Azure Dev Spaces синхронизирует изменения из локального проекта и постепенно обновляет приложение. Следующий раздел в файле конфигурации `azds.yaml` используется для настройки синхронизации и обновления.

```yaml
...
configurations:
  develop:
    ...
    container:
      sync:
      - "**/Pages/**"
      - "**/Views/**"
      - "**/wwwroot/**"
      - "!**/*.{sln,csproj}"
      command: [dotnet, run, --no-restore, --no-build, --no-launch-profile, -c, "${BUILD_CONFIGURATION:-Debug}"]
      iterate:
        processesToKill: [dotnet, vsdbg]
        buildCommands:
        - [dotnet, build, --no-restore, -c, "${BUILD_CONFIGURATION:-Debug}"]
...
```

Файлы и каталоги, которые будут синхронизировать изменения, перечислены в свойстве *Configurations. Разработка. контейнер. Синхронизация* . Эти каталоги изначально синхронизируются при выполнении команды `up`, а также при обнаружении изменений. Если имеются дополнительные или разные каталоги, которые вы хотите синхронизировать с пространством разработки, это свойство можно изменить.

Свойство *Configurations. Development. Container. REBUILD. буилдкоммандс* определяет способ сборки приложения в сценарии разработки. Свойство *Configurations. разработать. Container. Command* предоставляет команду для запуска приложения в сценарии разработки. Может потребоваться обновить любое из этих свойств, если имеются дополнительные флаги сборки или времени выполнения, которые вы хотите использовать во время разработки.

В параметре *Configurations. разработать. Container. reитерация. процессестокилл* перечислены процессы, которые нужно уничтожить, чтобы остановить приложение. Если вы хотите изменить поведение приложения во время разработки, может потребоваться обновить это свойство. Например, если вы обновили параметры *. Разработка. Container* . restart. буилдкоммандс или *Configurations. Разработка. контейнер. команда* для изменения способа построения или запуска приложения, может потребоваться изменить процессы, которые останавливаются.

При подготовке кода с помощью команды `azds prep` можно добавить флаг `--public`. Добавление флага `--public` создает общедоступный URL-адрес приложения. Если опустить этот флаг, приложение будет доступно только в пределах кластера или с помощью туннеля localhost. После выполнения команды `azds prep` можно изменить этот параметр, изменив свойство входящее *. Enabled* в `charts/APPNAME/values.yaml`:

```yaml
ingress:
  enabled: true
```

## <a name="debug-your-code"></a>Отладка кода

Для приложений Java, .NET и Node. js можно выполнить отладку приложения, работающего непосредственно в пространстве разработки, с помощью Visual Studio Code или Visual Studio. Visual Studio Code и Visual Studio предоставляют средства для подключения к сфере разработки, запуска приложения и подключения отладчика. После выполнения `azds prep`можно открыть проект в Visual Studio Code или Visual Studio. Visual Studio Code или Visual Studio создаст собственные файлы конфигурации для подключения, которые отделены от запуска `azds prep`. В Visual Studio Code или Visual Studio можно задать точки останова и запустить приложение в пространстве разработки.

![Отладка кода](media/get-started-node/debug-configuration-nodejs2.png)

При запуске приложения с помощью Visual Studio Code или Visual Studio для отладки они управляют запуском и подключением к пространству разработки таким же образом, как и работающие `azds up`. Средства на стороне клиента в Visual Studio Code и Visual Studio также предоставляют дополнительный параметр с определенной информацией для отладки. Параметр содержит имя образа отладчика, расположение отладчика в образе отладчика и конечное расположение в контейнере приложения для подключения папки отладчика.

Изображение отладчика автоматически определяется средствами на стороне клиента. В нем используется метод, аналогичный тому, который используется во время Dockerfile и Helm диаграммы при выполнении `azds prep`. После подключения отладчика к образу приложения он выполняется с помощью `azds exec`.

## <a name="sharing-a-dev-space"></a>Совместное использование пространства разработки

Работая с командой, вы можете [поделиться пространством разработки во всей группе](how-to/share-dev-spaces.md) и создать производные пространства разработки. Пространство разработки может использоваться любым сотрудником, у которого есть доступ к группе ресурсов пространства разработки.

Можно также создать новое пространство разработки, полученное из другого пространства разработки. При создании производного пространства разработки метка *azds.IO/Parent-Space=Parent-Space-Name* добавляется в пространство имен производного пространства разработки. Кроме того, все приложения из родительского пространства разработки используются совместно с производным пространством разработки. При развертывании обновленной версии приложения в производном пространстве разработки оно будет существовать только в производном пространстве разработки, а родительское пространство разработки останется без изменений. Можно использовать не более трех уровней производных пространств разработки или *бабушкх* .

Производное пространство разработки также будет Intelligent маршрутизировать запросы между собственными приложениями и приложениями, которые совместно используются от его родителя. Маршрутизация работает, пытаясь направить запрос к приложению в производном пространстве разработки и вернуться к общему приложению из родительского пространства разработки. Маршрутизация вернется к общему приложению в области «бабушке», если приложение не находится в родительском пространстве.

Например,
* Пространство разработки *по умолчанию* имеет приложения *Service* a и *serviceB* .
* *Azureuser* пространства разработки является производным от *значения по умолчанию*.
* Обновленная версия *Service* a развертывается в *azureuser*.

При использовании *azureuser*все запросы к *Service* a будут направляться в обновленную версию в *azureuser*. Запрос к *serviceB* сначала попытается направляться в *azureuser* версию *serviceB*. Так как она не существует, она будет направляться в версию *serviceB* *по умолчанию* . При удалении версии *azureuser* *Service* a все запросы к *Service* a будут возвращаться к использованию версии *Service*a *по умолчанию* .

## <a name="next-steps"></a>Дополнительная информация

Чтобы приступить к работе с Azure Dev Spaces, ознакомьтесь со следующими краткими руководствами:

* [Java с интерфейсом командной строки и Visual Studio Code](quickstart-java.md)
* [.NET Core с интерфейсом командной строки и Visual Studio Code](quickstart-netcore.md)
* [.NET Core с Visual Studio](quickstart-netcore-visualstudio.md)
* [Node. js с интерфейсом командной строки и Visual Studio Code](quickstart-nodejs.md)

Чтобы приступить к работе с Team Development, ознакомьтесь со следующими статьями:

* [Коллективная разработка — Java с интерфейсом командной строки и Visual Studio Code](team-development-java.md)
* [Коллективная разработка — .NET Core с интерфейсом командной строки и Visual Studio Code](team-development-netcore.md)
* [Коллективная разработка — .NET Core с Visual Studio](team-development-netcore-visualstudio.md)
* [Коллективная разработка — Node. js с интерфейсом командной строки и Visual Studio Code](team-development-nodejs.md)



[supported-regions]: about.md#supported-regions-and-configurations
