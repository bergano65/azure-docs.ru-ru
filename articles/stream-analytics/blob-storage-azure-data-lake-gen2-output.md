---
title: Хранилище BLOB-объектов и Azure Data Lake выходных данных Gen2 из Azure Stream Analytics
description: В этой статье описывается хранилище BLOB-объектов и Azure Data Lake Gen 2 в качестве выходных данных для Azure Stream Analytics.
author: mamccrea
ms.author: mamccrea
ms.reviewer: mamccrea
ms.service: stream-analytics
ms.topic: conceptual
ms.date: 08/25/2020
ms.openlocfilehash: 2ab45f4c64e6993f70f08f04ee413211abb0307d
ms.sourcegitcommit: 927dd0e3d44d48b413b446384214f4661f33db04
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 08/26/2020
ms.locfileid: "88875906"
---
# <a name="blob-storage-and-azure-data-lake-gen2-output-from-azure-stream-analytics"></a>Хранилище BLOB-объектов и Azure Data Lake выходных данных Gen2 из Azure Stream Analytics

Data Lake Storage 2-го поколения использует службу хранилища Azure в качестве основы для создания корпоративных хранилищ данных в Azure. Разработанное для обслуживания нескольких петабайт информации, поддержки пропускной способности сети на уровне нескольких сотен гигабит, Data Lake Storage 2-го поколения позволяет легко управлять большими объемами данных. Основной частью Data Lake Storage 2-го поколения является добавление иерархического пространства имен в хранилище BLOB-объектов.

Хранилище BLOB-объектов Azure предоставляет экономичное и масштабируемое решение для хранения в облаке больших объемов неструктурированных данных. Общие сведения о хранилище BLOB-объектов и его использовании см. в разделе [Передача, скачивание и составление списка больших двоичных объектов с помощью портала Azure](../storage/blobs/storage-quickstart-blobs-portal.md).

## <a name="output-configuration"></a>Конфигурация выходных данных

В таблице ниже приведены имена и описание свойств для создания выходных данных большого двоичного объекта или ADLS 2-го поколения.

| Имя свойства       | Описание                                                                      |
| ------------------- | ---------------------------------------------------------------------------------|
| Псевдоним выходных данных        | Понятное имя, которое используется в запросах для направления выходных данных запроса в хранилище BLOB-объектов. |
| Учетная запись хранения     | Имя учетной записи хранения, в которую отправляются выходные данные.               |
| Ключ учетной записи хранения | Секретный ключ, связанный с учетной записью хранения.                              |
| Контейнер хранилища   | Логическая группировка больших двоичных объектов, хранящихся в службе BLOB-объектов Azure. При передаче BLOB-объекта в службу BLOB-объектов для него необходимо указать контейнер. |
| Шаблон пути | Необязательный параметр. Шаблон пути к файлу, используемый для записи BLOB-объектов в указанном контейнере. <br /><br /> Чтобы указать периодичность записи больших двоичных объектов, в шаблоне пути можно использовать один или несколько экземпляров переменных даты и времени: <br /> {date}, {time} <br /><br />Пользовательское секционирование большого двоичного объекта можно использовать для предварительной версии, указав одно пользовательское имя поля {field} из данных события. Имя поля может содержать буквы, цифры, дефисы и символы подчеркивания. Существуют следующие ограничения для пользовательских полей. <ul><li>Имена полей нечувствительны к регистру. Например, служба не может различить столбец "ID" и столбец "id".</li><li>Вложенные поля недопустимы. Вместо этого используйте псевдоним в запросе задания для "сведения" поля.</li><li>В имени поля запрещено использовать выражения.</li></ul> <br />Этот компонент допускается использовать в пути конфигурации описателей пользовательских форматов даты и времени. Пользовательские форматы даты и времени должны быть указаны по одному за раз, заключенным в \<specifier> ключевое слово {DateTime:}. Допустимые входные данные для \<specifier> : гггг, mm, M, дд, d, чч, H, mm, M, SS или s. \<specifier>Ключевое слово {DateTime:} можно использовать несколько раз в пути для формирования настраиваемых конфигураций даты и времени. <br /><br />Примеры: <ul><li>Пример 1: cluster1/logs/{date}/{time}</li><li>Пример 2: cluster1/logs/{date}</li><li>Пример 3: cluster1/{client_id}/{date}/{time}</li><li>Пример 4: cluster1/{datetime:ss}/{myField}, где запрос имеет следующий вид: SELECT data.myField AS myField FROM Input;</li><li>Пример 5: cluster1/year={datetime:yyyy}/month={datetime:MM}/day={datetime:dd}</ul><br />Метка времени создаваемой структуры папок соответствует времени UTC, а не местному времени.<br /><br />При именовании файлов используется следующее соглашение: <br /><br />{Шаблон префикса пути}/schemaHashcode_Guid_Number.extension<br /><br />Выходные файлы примера:<ul><li>Myoutput/20170901/00/45434_gguid_1.csv</li>  <li>Myoutput/20170901/01/45434_gguid_1.csv</li></ul> <br />Дополнительные сведения об этом компоненте см. в статье [Пользовательские шаблоны даты и времени в пути для выходных данных хранилища BLOB-объектов Azure Stream Analytics (предварительная версия)](stream-analytics-custom-path-patterns-blob-storage-output.md). |
| Формат даты | Необязательный параметр. Если в префиксе пути используется маркер даты, вы можете выбрать формат даты для упорядочивания своих файлов. Пример ГГГГ/ММ/ДД |
| Формат времени | Необязательный параметр. Если в префиксе пути используется маркер времени, укажите формат времени для упорядочивания своих файлов. В настоящее время поддерживается только один формат — ЧЧ. |
| Формат сериализации событий | Формат сериализации для выходных данных. Поддерживаются форматы JSON, CSV, Avro и Parquet. |
|Минимальное число строк |Количество минимальных строк на пакет. Для Parquet каждый пакет создаст файл. Текущее значение по умолчанию — 2000 строк, а допустимое максимальное — 10 000 строк.|
|Максимальное время |Максимальное время ожидания на пакет. По истечении этого времени пакет будет записан в выходные данные, даже если не будет выполнено требование к минимальному числу строк. Текущее значение по умолчанию — 1 минута, а максимально допустимое — 2 часа. Если у выходных данных большого двоичного объекта есть частота шаблона пути, время ожидания не может превышать диапазон времени раздела.|
| Кодирование    | Если используется формат CSV или JSON, необходимо указать формат кодирования. В настоящее время единственным поддерживаемым форматом кодировки является UTF-8. |
| Разделитель   | Применяется только для сериализации в формате CSV. Служба Stream Analytics позволяет использовать ряд распространенных разделителей для сериализации данных в формате CSV. Поддерживаются такие разделители: запятая, точка с запятой, пробел, табуляция и вертикальная черта. |
| Формат      | Применяется только для сериализации в формате JSON. Вариант **строки-разделители** предусматривает форматирование выходных данных таким образом, что каждый объект JSON будет отделен новой строкой. Если выбрать вариант **строки-разделители**, то JSON считывает по одному объекту за раз. Все содержимое само по себе не будет допустимым форматом JSON. Вариант **массив** означает, что выходные данные будут отформатированы как массив объектов JSON. Этот массив будет закрыт только в том случае, если выполнение задания будет остановлено или Stream Analytics перейдет в следующее временное окно. В общем рекомендуется использовать JSON-файл со строками-разделителями, так как для него не требуется никакой специальной обработки. При этом по-прежнему выполняется запись в выходной файл. |

## <a name="blob-output-files"></a>Выходные файлы BLOB-объектов

При использовании хранилища BLOB-объектов для выходных данных в большом двоичном объекте создается файл в следующих случаях:

* Если размер файла превышает максимально допустимое количество блоков (в настоящее время 50 000). Максимальное количество блоков может быть достигнуто без превышения максимально допустимого размера большого двоичного объекта. Например, при высокой скорости вывода данных в блоке будет большее число байтов, значит, и размер файла будет большим. В случае низкой скорости вывода данных в каждом блоке будет меньше данных, а значит, и размер файла будет меньшим.
* Если схема изменена в выходных данных, а для формата выходных данных требуется фиксированная схема (CSV или Avro).
* При перезапуске задания извне пользователем, который останавливает и затем запускает его, или изнутри для обслуживания системы или восстановления после сбоя.
* Если запрос полностью разделен, для каждого раздела выходных данных создается файл.
* Если пользователь удаляет файл или контейнер учетной записи хранения.
* Если выходные данные разделены по времени с использованием шаблона префикса пути, новый блок применяется, когда запрос переходит к следующему часу.
* Если выходные данные разделяются по пользовательскому полю, то для каждого ключа раздела создается большой двоичный объект, если он не существует.
* Если выходные данные разделяются по пользовательскому полю и кратность ключа раздела превышает 8000, то для каждого ключа раздела может быть создан большой двоичный объект.

## <a name="partitioning"></a>Секционирование

Для ключа секции Используйте токены {Date} и {Time} из полей событий в шаблоне пути. Выберите формат даты, например ГГГГ-ММ-ДД, ДД-ММ-ГГГГ, ММ-ДД-ГГГГ. Для времени используется формат ЧЧ. Выходные данные большого двоичного объекта можно секционировать по одному настраиваемому атрибуту события {FieldName} или {DateTime: \<specifier> }. Количество модулей записи вывода соответствует входному секционированию для [полностью параллелизуемые запросов](stream-analytics-scale-jobs.md).

## <a name="output-batch-size"></a>Размер выходного пакета

Максимальный размер сообщения см. в статье [ограничения хранилища Azure](../azure-resource-manager/management/azure-subscription-service-limits.md#storage-limits). Максимальный размер блока BLOB-объектов составляет 4 МБ, а максимальное число Bock больших двоичных объектов — 50 000. |

## <a name="next-steps"></a>Дальнейшие действия

* [Краткое руководство. по созданию задания Stream Analytics с помощью портала Azure](stream-analytics-quick-create-portal.md)
* [Краткое руководство. Создание задания Azure Stream Analytics с помощью Azure CLI](quick-create-azure-cli.md)
* [Краткое руководство. Создание задания Azure Stream Analytics с помощью шаблона ARM](quick-create-azure-resource-manager.md)
* [Краткое руководство. Создание задания Stream Analytics с помощью Azure PowerShell](stream-analytics-quick-create-powershell.md)
* [Краткое руководство. Создание задания Azure Stream Analytics с помощью Visual Studio](stream-analytics-quick-create-vs.md)
* [Краткое руководство. Создание задания Azure Stream Analytics в Visual Studio Code](quick-create-vs-code.md)
