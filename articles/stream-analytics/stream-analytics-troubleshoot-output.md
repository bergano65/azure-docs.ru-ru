---
title: Устранение неполадок с выходными данными в Azure Stream Analytics
description: В этой статье описаны методы устранения неполадок с выходными подключениями в заданиях Azure Stream Analytics.
author: sidram
ms.author: sidram
ms.reviewer: mamccrea
ms.service: stream-analytics
ms.topic: troubleshooting
ms.date: 10/05/2020
ms.custom: seodec18
ms.openlocfilehash: bc630fc5ea9407c284e2e2e879c349a83302cd9f
ms.sourcegitcommit: 857859267e0820d0c555f5438dc415fc861d9a6b
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 10/30/2020
ms.locfileid: "93122629"
---
# <a name="troubleshoot-azure-stream-analytics-outputs"></a>Устранение неполадок с выходными данными в Azure Stream Analytics

В этой статье описаны распространенные проблемы с выходными подключениями Azure Stream Analytics и способы их устранения. Для многих действий по устранению неполадок должны быть включены журналы ресурсов и другие журналы диагностики для задания Stream Analytics. Если журналы ресурсов еще не включены, воспользуйтесь статьей [Устранение неполадок в Azure Stream Analytics с помощью журналов ресурсов](stream-analytics-job-diagnostic-logs.md).

## <a name="the-job-doesnt-produce-output"></a>Задание не создает выходные данные

1. Проверьте подключение к портам вывода с помощью кнопки **Проверить подключение** для всех выходных данных.
1. Проверьте [метрики мониторинга](stream-analytics-monitoring.md) на вкладке **Монитор** . Так как значения агрегированы, метрики задерживаются на несколько минут.

   * Если значение в поле **Входные события** больше нуля, то задание может считывать входные данные. Если значение в поле **Входные события** не больше нуля, то это указывает на ошибку со входными данными задания. Дополнительные сведения см. в разделе [Устранение неполадок с входными подключениями](stream-analytics-troubleshoot-input.md). Если в задании введены ссылочные данные, примените разделение по логическому имени при просмотре метрики **событий ввода** . Если нет никаких входных событий из эталонных данных, то, скорее всего, это означает, что этот источник ввода не настроен должным образом для выборки правильного эталонного набора данных.
   * Если значение в поле **Ошибки преобразования данных** больше нуля и увеличивается, обратитесь к статье [Ошибки преобразования данных Azure Stream Analytics](data-errors.md) для получения дополнительных сведений об ошибках преобразования данных.
   * Если значение в поле **Ошибки среды выполнения** больше нуля, это означает, что задание получает данные, но при обработке запроса выдает ошибки. Чтобы найти ошибки, перейдите к [журналам аудита](../azure-resource-manager/management/view-activity-logs.md) и выполните фильтрацию по состоянию **Сбой** .
   * Если значение в поле **Входные события** больше нуля, а значение в поле **Выходные события** равно нулю, то верно одно из следующих утверждений:
      * В результате обработки запроса исходящие события не получены.
      * События или его поля могут быть повреждены, и выходные события после обработки запроса не получены.
      * Заданию не удалось передать данные в приемник выходных данных из-за проблем с подключением или аутентификацией.

   Сведения об ошибках можно получить в сообщениях журнала операций (включая сведения о самом событии), за исключением случаев, когда логика запроса отфильтровала все события. Если обработка нескольких событий приводит к возникновению ошибок, то ошибки вычисляются каждые 10 минут.

## <a name="the-first-output-is-delayed"></a>Первый фрагмент выходных данных задерживается

При запуске задания Stream Analytics считываются входные события. Однако в некоторых обстоятельствах может возникнуть задержка с предоставлением выходных данных.

Большие значения времени в элементах темпоральных запросов могут приводить к задержке выходных данных. Чтобы сформировать правильные выходные данные за большие периоды времени, задание потоковой передачи считывает данные за самое последнее время для охвата временного окна (до семи дней назад). Выходные данные не создаются до тех пор, пока не будут считаны ожидающие входные события. Эта проблема может возникнуть, когда система обновляет задания потоковой передачи. При обновлении задание перезапускается. Такие обновления обычно происходят один раз каждые несколько месяцев.

Следует проявлять осторожность при проектировании запроса Stream Analytics. Если вы используете большое временное окно для темпоральных элементов в синтаксисе запроса задания, это может привести к задержке первого фрагмента выходных данных при запуске или перезапуске задания. Большим временным окно считается окно от нескольких часов до семи дней.

Одним из способов устранения этой задержки первых выходных данных является использование методов параллельной обработки запросов, таких как секционирование данных. Также можно добавить дополнительные единицы потоковой передачи, чтобы повысить пропускную способность до тех пор, пока задание не будет срабатывать.  Дополнительные сведения см. в статье [Концепции контрольных точек и воспроизведения в Azure Stream Analytics](stream-analytics-concepts-checkpoint-replay.md).

Эти факторы влияют на временные шкалы первого вывода:

* Использование агрегатов данных на основе периодов, таких как предложение группирования GROUP BY по "переворачивающимся", "прыгающим" и "скользящим" окнам.

  * Для агрегатов "переворачивающихся" или "прыгающих" окон результаты создаются в конце оконного временного интервала.
  * Для "скользящего" окна результаты создаются, когда событие входит в это окно или выходит из него.
  * Если вы планируете использовать окно большого размера, например более одного часа, лучше выбрать "прыгающее" или "скользящее" окно. Эти типы окон позволяют чаще видеть выходные данные.

* Использование темпоральных соединений, таких как JOIN с DATEDIFF.
  * Соответствия генерируются, как только поступают оба экземпляра сопоставляемых событий.
  * Данные без соответствия, такие как LEFT OUTER JOIN, создаются в конце окна DATEDIFF относительно каждого события с левой стороны.

* Использование темпоральных аналитических функций, таких как ISFIRST, LAST и LAG с LIMIT DURATION.
  * Для аналитических функций выходные данные создаются для каждого события без задержки.

## <a name="the-output-falls-behind"></a>Выходные данные запаздывают

Во время нормальной работы задания выходные данные могут иметь все большие и большие периоды задержки. Если выходные данные запаздывают, можно выявить основные причины, изучив следующие факторы:

* регулируется ли подчиненный приемник:
* регулируется ли вышестоящий источник данных;
* потребляет ли логика обработки в запросе много вычислительных ресурсов.

Чтобы просмотреть сведения о выходных данных, на портале Azure выберите задание потоковой передачи и нажмите **Схема заданий** . Для каждого набора входных данных есть метрика событий невыполненной работы секции. Если метрика увеличивается, это сигнализирует об ограниченности ресурсов системы. Такое увеличение может быть связано с регулированием приемника выходных данных или высокой загрузкой ЦП. Дополнительные сведения см. в статье [Отладка на основе данных с помощью схемы заданий](stream-analytics-job-diagram-with-metrics.md).

## <a name="key-violation-warning-with-azure-sql-database-output"></a>Предупреждение о нарушении ключа с выходными данными службы "База данных SQL Azure"

При настройке службы "База данных SQL Azure" для выходных данных задания Stream Analytics выполняется массовая вставка записей в целевую таблицу. Как правило, Azure Stream Analytics гарантирует [не менее одной доставки](/stream-analytics-query/event-delivery-guarantees-azure-stream-analytics) в приемник выходных данных. Вы по-прежнему можете [выполнить однократную доставку]( https://blogs.msdn.microsoft.com/streamanalytics/2017/01/13/how-to-achieve-exactly-once-delivery-for-sql-output/) в выходные данные SQL, если для таблицы SQL определено уникальное ограничение.

При настройке уникальных ограничений для ключа в таблице SQL служба Azure Stream Analytics удаляет повторяющиеся записи. Данные разделяются на пакеты, которые рекурсивно вставляются, пока не будет обнаружена повторяющаяся запись. Процесс разбиения и вставки пропускает дубликаты по одному за раз. Для задания потоковой передачи,в котором имеется много повторяющихся строк, процесс является неэффективным и занимает много времени. Если в журнале действий в течение последнего часа отображается несколько предупреждающих сообщений о нарушении ключа, вполне вероятно, что обработка выходных данных SQL замедляет выполнение всего задания.

Чтобы устранить эту проблему, нужно [настроить индекс]( https://docs.microsoft.com/sql/t-sql/statements/create-index-transact-sql), который вызывает нарушение ключа, включив параметр IGNORE_DUP_KEY. Этот параметр позволяет SQL игнорировать дублирующиеся значения во время операций вставки. База данных SQL Azure просто выдает предупреждающее сообщение, а не ошибку. В результате Azure Stream Analytics больше не создает ошибки нарушения первичного ключа.

Если IGNORE_DUP_KEY настраивается для нескольких типов индексов, обратите внимание на следующее:

* IGNORE_DUP_KEY нельзя установить для первичного ключа или уникального ограничения, в котором используется ALTER INDEX. Индекс нужно удалить и создать повторно.  
* IGNORE_DUP_KEY можно задать с помощью инструкции ALTER INDEX для уникального индекса. Этот экземпляр отличается от ограничения PRIMARY KEY/UNIQUE и создается с помощью определения CREATE INDEX или INDEX.  
* Параметр IGNORE_DUP_KEY не применяется к индексам хранилища столбцов, так как нельзя обеспечить уникальность таких индексов.

## <a name="sql-output-retry-logic"></a>Логика повторных попыток вывода SQL

Когда Stream Analytics задание с выходными данными SQL получает первый пакет событий, выполняются следующие действия.

1. Задание пытается подключиться к SQL.
2. Задание извлекает схему целевой таблицы.
3. Задание проверяет имена и типы столбцов в схеме целевой таблицы.
4. Задание подготавливает таблицу данных в памяти из выходных записей в пакете.
5. Задание записывает таблицу данных в SQL с помощью [API](/dotnet/api/system.data.sqlclient.sqlbulkcopy.writetoserver?view=dotnet-plat-ext-3.1)BulkCopy.

Во время выполнения этих действий в выходных данных SQL могут возникать ошибки следующих типов:

* Временные [ошибки](../azure-sql/database/troubleshoot-common-errors-issues.md#transient-fault-error-messages-40197-40613-and-others) , повторные попытки которых выполняются с помощью стратегии повтора с экспоненциальной задержкой. Минимальный интервал повторных попыток зависит от конкретного кода ошибки, но интервалы обычно менее 60 секунд. Верхний предел может составлять не более пяти минут. 

   [Ошибки входа](../azure-sql/database/troubleshoot-common-errors-issues.md#unable-to-log-in-to-the-server-errors-18456-40531) и [брандмауэр](../azure-sql/database/troubleshoot-common-errors-issues.md#cannot-connect-to-server-due-to-firewall-issues) повторяются по крайней мере через 5 минут после предыдущей попытки и повторяются до тех пор, пока они не будут выполнены.

* Ошибки данных, такие как ошибки приведения и нарушения ограничений схемы, обрабатываются с помощью политики исходящих ошибок. Эти ошибки обрабатываются повторными пакетами двоичного разделения до тех пор, пока отдельные записи, вызвавшие ошибку, не будут обрабатываться функцией Skip или Retry. Нарушение ограничения первичного уникального ключа [всегда обрабатывается](./stream-analytics-troubleshoot-output.md#key-violation-warning-with-azure-sql-database-output).

* При возникновении проблем службы SQL или внутренних дефектов кода могут возникать невременные ошибки. Например, если такие ошибки, как (код 1132), Эластичный пул достижении предельного размера хранилища, повторные попытки не разрешают ошибку. В этих сценариях Stream Analyticsное задание приводит к [ухудшению производительности](job-states.md).
* `BulkCopy` время ожидания может быть выполнено `BulkCopy` на шаге 5. `BulkCopy` Иногда может возникать время ожидания операций. Минимальное заданное по умолчанию время ожидания составляет пять минут и удваивается при последовательном обращении.
Если время ожидания превышает 15 минут, указание максимального размера пакета `BulkCopy` уменьшается до половины до тех пор, пока не будет оставлено 100 событий на пакет.

## <a name="column-names-are-lowercase-in-azure-stream-analytics-10"></a>Имена столбцов в Azure Stream Analytics (1.0) указаны строчными буквами

При использовании исходного уровня совместимости (1.0) Azure Stream Analytics преобразует символы имен столбцов в нижний регистр. На более поздних уровнях совместимости это поведение было исправлено. Чтобы сохранить регистр, перейдите на уровень совместимости 1.1 или более поздний. Дополнительные сведения см. в разделе [Уровень совместимости для заданий Stream Analytics](./stream-analytics-compatibility-level.md).

## <a name="get-help"></a>Получить справку

Для получения дополнительной помощи воспользуйтесь [страницей вопросов и ответов по Azure Stream Analytics на сайте Майкрософт](/answers/topics/azure-stream-analytics.html).

## <a name="next-steps"></a>Дальнейшие действия

* [Введение в Azure Stream Analytics](stream-analytics-introduction.md)
* [Приступая к работе с Azure Stream Analytics](stream-analytics-real-time-fraud-detection.md)
* [Масштабирование заданий в службе Azure Stream Analytics](stream-analytics-scale-jobs.md)
* [Справочник по языку запросов Azure Stream Analytics](/stream-analytics-query/stream-analytics-query-language-reference)
* [Azure Stream Analytics management REST API reference](/rest/api/streamanalytics/) (Справочник по API-интерфейсу REST для управления Stream Analytics).