---
title: Потоковые данные в качестве входных данных Azure Stream Analytics
description: Узнайте больше о настройке подключения данных в Azure Stream Analytics. К входным данным относятся поток данных из событий, а также справочные данные.
author: enkrumah
ms.author: ebnkruma
ms.service: stream-analytics
ms.topic: conceptual
ms.date: 10/28/2020
ms.openlocfilehash: 5f10fed66475cda8fd700a4737727101e2465870
ms.sourcegitcommit: 42a4d0e8fa84609bec0f6c241abe1c20036b9575
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 01/08/2021
ms.locfileid: "98019369"
---
# <a name="stream-data-as-input-into-stream-analytics"></a>Потоковые данные в качестве входных данных Stream Analytics

Stream Analytics полностью интегрируется с потоками данных Azure в качестве входных данных трех типов ресурсов.

- [Центры событий Azure](https://azure.microsoft.com/services/event-hubs/)
- [Центр Интернета вещей Azure](https://azure.microsoft.com/services/iot-hub/) 
- [Хранилище BLOB-объектов Azure](https://azure.microsoft.com/services/storage/blobs/) 
- [Azure Data Lake Storage 2-го поколения](../storage/blobs/data-lake-storage-introduction.md) 

Эти ресурсы входных данных могут существовать в той же подписке Azure, что и задание Stream Analytics, или другой подписке.

### <a name="compression"></a>Сжатие

Stream Analytics поддерживает функцию сжатия во всех источниках входных потоковых данных. Поддерживаются следующие типы сжатия. без сжатия, форматы GZIP и DEFLATE. Для ссылочных данных сжатие не поддерживается. Если у входных данных формат AVRO, они сжимаются и выполняется их прозрачная обработка. Для сериализации Avro не требуется указывать тип сжатия. 

## <a name="create-edit-or-test-inputs"></a>Создание, изменение или проверка входных данных

С помощью [портала Azure](stream-analytics-quick-create-portal.md), [Visual Studio](stream-analytics-quick-create-vs.md) и [Visual Studio Code](quick-create-visual-studio-code.md) можно добавлять входные данные и просматривать или изменять их в задании потоковой передачи. Можно также проверять входящие подключения и тестировать запросы из демонстрационных данных на портале Azure, в [Visual Studio](stream-analytics-vs-tools-local-run.md) и [Visual Studio Code](visual-studio-code-local-run.md). При написании запроса входные данные указываются в предложении FROM. Список доступных входных данных можно получить на странице **Запрос** на портале. Чтобы использовать несколько источников входных данных, объедините их с помощью параметра `JOIN` или напишите несколько запросов `SELECT`.


## <a name="stream-data-from-event-hubs"></a>Потоковая передача данных из Центров событий

Концентраторы событий Azure обеспечивают высокую масштабируемость событий публикации и подписки. Концентратор событий может обрабатывать миллионы событий в секунду, позволяя вам обрабатывать и анализировать огромное количество данных, создаваемых подключенными устройствами и приложениями. Используя Центры событий со службой Stream Analytics, вы получаете комплексное решение для анализа данных в режиме реального времени. Центры событий позволяют передавать события в Azure в режиме реального времени, где задания Stream Analytics также обрабатывают эти события в режиме реального времени. Например, в Центры событий можно отправлять сведения о щелчках, показания датчиков или журналы сетевых событий. Затем можно создать задания Stream Analytics, которые используют Центры событий в качестве входных потоков данных для фильтрации в режиме реального времени, выполнения агрегации и корреляции.

`EventEnqueuedUtcTime` — это метка времени поступления события в концентратор, которая также является меткой времени по умолчанию для событий, поступающих из Центров событий в Stream Analytics. Для обработки данных как потока с помощью метки времени в полезных данных события необходимо использовать ключевое слово [TIMESTAMP BY](/stream-analytics-query/timestamp-by-azure-stream-analytics).

### <a name="event-hubs-consumer-groups"></a>Группы потребителей Центров событий

Каждый концентратор событий Stream Analytics нужно настроить таким образом, чтобы у него была собственная группа получателей. Если задание содержит самосоединение или несколько источников входных данных, некоторые входные данные могут последовательно считываться несколькими модулями чтения. Эта ситуация влияет на количество модулей чтения в группе получателей. Чтобы не превысить лимит на количество модулей чтения для Центров событий (5 на каждую группу получателей в разделе), рекомендуется назначить группу получателей для каждого задания Stream Analytics. У каждого концентратора событий уровня "Стандартный" должно быть не более 20 групп получателей. Дополнительные сведения см. в разделе об [устранении неполадок входных данных Azure Stream Analytics](stream-analytics-troubleshoot-input.md).

### <a name="create-an-input-from-event-hubs"></a>Создание входных данных из Центров событий

В следующей таблице описываются все параметры на странице **Новые входные данные** на портале Azure для передачи потока входных данных из концентратора событий.

| Свойство | Описание |
| --- | --- |
| **Псевдоним входных данных** |Понятное имя, используемое в запросах задания для ссылки на эти входные данные. |
| **Подписка** | Выберите подписку, в которой существует ресурс концентратора событий. | 
| **Пространство имен концентратора событий** | Пространство имен концентратора событий — это контейнер для набора сущностей обмена сообщениями. При создании нового концентратора событий также создается пространство имен. |
| **Имя концентратора событий** | Имя концентратора событий для использования в качестве источника входных данных. |
| **Имя политики концентратора событий** | Политика общего доступа, которая предоставляет доступ к концентратору событий. Каждой политике общего доступа присваивается имя, а также для нее задаются разрешения и ключи доступа. Этот параметр автоматически заполняется, если только не указан параметр "Указать настройки концентратора событий вручную".|
| **Группа получателей концентратора событий** (рекомендуется) | Для каждого задания Stream Analytics настоятельно рекомендуется использовать отдельную группу получателей. Эта строка указывает группу получателей, принимающих данные из концентратора событий. Если группа получателей не указана, задание Stream Analytics использует группу получателей "$Default".  |
| **Ключ секции** | Это необязательное поле, которое доступно только в том случае, если для задания настроено использование [уровня совместимости](./stream-analytics-compatibility-level.md) 1,2 или выше. Если входные данные секционированы по свойству, можно добавить здесь имя этого свойства. Он используется для повышения производительности запроса, если он включает предложение PARTITION BY или GROUP BY для этого свойства. Если для этого задания используется уровень совместимости 1,2 или выше, по умолчанию для этого поля используется значение "PartitionId". |
| **Формат сериализации событий** | Формат сериализации (JSON, CSV, Avro или [другой [Protobuf, XML, собственный формат...]](custom-deserializer.md)) входящего потока данных.  Убедитесь, что формат JSON совпадает со спецификацией и не содержит ведущий "0" в десятичных числах. |
| **Кодирование** | Сейчас UTF-8 — единственный поддерживаемый формат кодировки. |
| **Тип сжатия событий** | Тип сжатия используется для чтения таких входящих потоков данных, как None (по умолчанию), GZip или Deflate. |

При поступлении входных данных из потока данных концентратора событий запрос Stream Analytics может получить доступ к следующим полям метаданных:

| Свойство | Описание |
| --- | --- |
| **EventProcessedUtcTime** |Дата и время обработки события службой Stream Analytics. |
| **EventEnqueuedUtcTime** |Дата и время получения события Центрами событий. |
| **PartitionId** |Идентификатор секции для входного адаптера (нумерация идет от нуля). |

Например, используя эти поля, можно писать запросы, как в следующем примере:

```sql
SELECT
    EventProcessedUtcTime,
    EventEnqueuedUtcTime,
    PartitionId
FROM Input
```

> [!NOTE]
> При использовании концентратора событий в качестве конечной точки для маршрутов Центра Интернета вещей вы можете получить доступ к метаданным Центра Интернета вещей с помощью [функции GetMetadataPropertyValue](/stream-analytics-query/getmetadatapropertyvalue).
> 

## <a name="stream-data-from-iot-hub"></a>Потоковая передача данных из Центра Интернета вещей

Центр Интернета вещей — это высокомасштабируемая служба приема данных о событиях публикации и подписки, оптимизированная под сценарии "Интернет вещей".

По умолчанию метка времени событий, поступающих из Центра Интернета вещей в Stream Analytics, — это метка времени поступления события в концентратор Центра Интернета вещей, то есть `EventEnqueuedUtcTime`. Для обработки данных как потока с помощью метки времени в полезных данных события необходимо использовать ключевое слово [TIMESTAMP BY](/stream-analytics-query/timestamp-by-azure-stream-analytics).

### <a name="iot-hub-consumer-groups"></a>Группа потребителей Центра Интернета вещей

Каждый Центр Интернета вещей Stream Analytics нужно настроить таким образом, чтобы у него была собственная группа получателей. Если задание включает самосоединение или несколько источников входных данных, некоторые входные данные могут последовательно считываться несколькими модулями чтения. Эта ситуация влияет на количество модулей чтения в группе получателей. Чтобы не превысить лимит на количество модулей чтения для Центра Интернета вещей (5 на каждую группу получателей в разделе), рекомендуется назначить группу получателей для каждого задания Stream Analytics.

### <a name="configure-an-iot-hub-as-a-data-stream-input"></a>Настройка Центра Интернета вещей в качестве входного потока данных

В следующей таблице описываются все параметры на странице **Новые входные данные** на портале Azure при настройке Центра Интернета вещей в качестве потокового входа.

| Свойство | Описание |
| --- | --- |
| **Псевдоним входных данных** | Понятное имя, используемое в запросах задания для ссылки на эти входные данные.|
| **Подписка** | Выберите подписку, в которой существуют ресурсы Центра Интернета вещей. | 
| **Центр Интернета вещей** | Имя Центра Интернета вещей для использования в качестве источника входных данных. |
| **Конечная точка** | Конечная точка для Центра Интернета вещей.|
| **Имя политики общего доступа** | Политика общего доступа, которая предоставляет доступ к Центру Интернета вещей. Каждой политике общего доступа присваивается имя, а также для нее задаются разрешения и ключи доступа. |
| **Ключ политики общего доступа** | Ключ общего доступа, используемый для авторизации доступа к Центру Интернета вещей.  Этот параметр автоматически заполняется, если только не будет указан вариант ручной настройки параметров Центра Интернета вещей. |
| **Группа потребителей** | Для каждого задания Stream Analytics настоятельно рекомендуется использовать отдельную группу получателей. Группа получателей используется для принимающих данных из Центра Интернета вещей. Stream Analytics использует группу получателей "$Default", если не указано иное.  |
| **Ключ секции** | Это необязательное поле, которое доступно только в том случае, если для задания настроено использование [уровня совместимости](./stream-analytics-compatibility-level.md) 1,2 или выше. Если входные данные секционированы по свойству, можно добавить здесь имя этого свойства. Он используется для повышения производительности запроса, если он включает предложение PARTITION BY или GROUP BY для этого свойства. Если для этого задания используется уровень совместимости 1,2 или выше, по умолчанию для этого поля используется значение "PartitionId". |
| **Формат сериализации событий** | Формат сериализации (JSON, CSV, Avro или [другой [Protobuf, XML, собственный формат...]](custom-deserializer.md)) входящего потока данных.  Убедитесь, что формат JSON совпадает со спецификацией и не содержит ведущий "0" в десятичных числах. |
| **Кодирование** | Сейчас UTF-8 — единственный поддерживаемый формат кодировки. |
| **Тип сжатия событий** | Тип сжатия используется для чтения таких входящих потоков данных, как None (по умолчанию), GZip или Deflate. |


При использовании потоковой передачи данных из Центра Интернета вещей запрос Stream Analytics может получить доступ к следующим полям метаданных:

| Свойство | Описание |
| --- | --- |
| **EventProcessedUtcTime** | Дата и время обработки события. |
| **EventEnqueuedUtcTime** | Дата и время получения события Центром Интернета вещей. |
| **PartitionId** | Идентификатор секции для входного адаптера (нумерация идет от нуля). |
| **IoTHub.MessageId** | Идентификатор, используемый для корреляции двустороннего обмена данными в Центре Интернета вещей. |
| **IoTHub.CorrelationId** | Идентификатор, используемый в ответах на сообщение и отзывах в Центре Интернета вещей. |
| **IoTHub.ConnectionDeviceId** | Идентификатор проверки подлинности, используемый для отправки этого сообщения. Это значение помещается в сообщения, связанные со службой, Центром Интернета вещей. |
| **IoTHub.ConnectionDeviceGenerationId** | Идентификатор создания устройства, прошедшего проверку подлинности, используемый для отправки этого сообщения. Это значение помещается в сообщения, связанные со службой, Центром Интернета вещей. |
| **IoTHub.EnqueuedTime** | Время, когда Центр Интернета вещей получил сообщение. |


## <a name="stream-data-from-blob-storage-or-data-lake-storage-gen2"></a>Потоковая передача данных из хранилища BLOB-объектов или Data Lake Storage 2-го поколения
Для сценариев с большим количеством неструктурированных данных для хранения в облаке хранилище BLOB-объектов Azure или Azure Data Lake Storage 2-го поколения (ADLS 2-го поколения) предлагает экономичное и масштабируемое решение. Данные в хранилище BLOB-объектов или ADLS 2-го поколения обычно считаются неактивныхи. Однако эти данные могут обрабатываться в виде потока данных Stream Analytics. 

Обработка журналов является часто используемым сценарием для использования таких входных данных с Stream Analytics. В этом сценарии файлы данных телеметрии, полученные из системы, необходимо проанализировать и обработать, чтобы извлечь значимые данные.

Метка времени по умолчанию для хранилища BLOB-объектов или события ADLS 2-го поколения в Stream Analytics — это отметка времени последнего изменения, то есть `BlobLastModifiedUtcTime` . Если большой двоичный объект передается в учетную запись хранения по адресу 13:00, а задание Azure Stream Analytics запускается с параметром *сейчас* в 13:01, оно не будет выбрано, так как время его изменения не превышает период выполнения задания.

Если большой двоичный объект передается в контейнер учетной записи хранения в 13:00, а задание Azure Stream Analytics запускается с параметром *Другое время* в 13:00 или раньше, то большой двоичный объект будет выбран, поскольку время его изменения приходится на период выполнения задания.

Если задание Azure Stream Analytics запускается с параметром *Сейчас* в 13:00, а большой двоичный объект передается в контейнер учетной записи хранения в 13:01, то Azure Stream Analytics выберет большой двоичный объект. Метка времени, назначенная каждому большому двоичному объекту, основана только на `BlobLastModifiedTime`. Папка, в которой находится большой двоичный объект, не имеет отношения к назначенной метке времени. Например, если имеется большой двоичный объект *2019/10-01/00/b1.txt* с `BlobLastModifiedTime` 2019-11-11, то метка времени, назначенная этому большому двоичному объекту, — 2019-11-11.

Для обработки данных как потока с помощью метки времени в полезных данных события необходимо использовать ключевое слово [TIMESTAMP BY](/stream-analytics-query/stream-analytics-query-language-reference). Stream Analytics задание извлекает данные из хранилища BLOB-объектов Azure или ADLS 2-го поколения входных данных каждую секунду, если файл большого двоичного объекта доступен. В случае, если этот файл недоступен, применяется экспоненциально увеличивающаяся задержка с максимальным значением, равным 90 секундам.

Для входных данных в формате CSV необходимо, чтобы строка заголовка определяла поля для набора данных и все поля строк заголовка были уникальными.

> [!NOTE]
> Stream Analytics не поддерживает добавление содержимого в существующий файл большого двоичного объекта. Stream Analytics просматривает каждый файл только один раз, и любые изменения, которые произойдут в нем после того, как задание прочитает данные, не обрабатываются. Мы рекомендуем отправлять все данные для файла большого двоичного объекта за один раз, а затем добавлять более новые события в другой новый файл большого двоичного объекта.

В сценариях, где много больших двоичных объектов постоянно добавляются и Stream Analytics обрабатывает большие двоичные объекты по мере их добавления, некоторые большие двоичные объекты могут быть пропущены в редких случаях из-за гранулярности `BlobLastModifiedTime` . Это можно устранить, организовав отправку больших двоичных объектов по крайней мере через две секунды. Если это не представляется возможным, можно использовать Центры событий для потоковой передачи больших объемов событий.

### <a name="configure-blob-storage-as-a-stream-input"></a>Настройка хранилища BLOB-объектов в качестве потокового входа 

В следующей таблице описываются все параметры на странице **Новые входные данные** на портале Azure при настройке хранилища больших двоичных объектов в качестве потокового входа.

| Свойство | Описание |
| --- | --- |
| **Псевдоним входных данных** | Понятное имя, используемое в запросах задания для ссылки на эти входные данные. |
| **Подписка** | Выберите подписку, в которой находится ресурс хранилища. | 
| **Учетная запись хранения** | Имя учетной записи хранения, в которой находятся файлы больших двоичных объектов. |
| **Ключ учетной записи хранения** | Секретный ключ, связанный с учетной записью хранения. Этот параметр заполняется автоматически в, если не выбран параметр для указания параметров вручную. |
| **Контейнер** | Контейнеры обеспечивают логическую группировку для больших двоичных объектов. Чтобы создать новый контейнер, вы можете выбрать параметр **Use existing** (Использование имеющихся) контейнеров или **Создать**.|
| **Шаблон пути** (необязательно) | Путь к файлу, используемый для поиска больших двоичных объектов в указанном контейнере. Если требуется считать большие двоичные объекты из корня контейнера, не задавайте шаблон пути. В пути можно указать один или более экземпляров следующих трех переменных: `{date}`, `{time}` или `{partition}`.<br/><br/>Пример 1: `cluster1/logs/{date}/{time}/{partition}`<br/><br/>Пример 2: `cluster1/logs/{date}`<br/><br/>Символ `*` является недопустимым значением для префикса пути. Допустимыми являются только <a HREF="/rest/api/storageservices/Naming-and-Referencing-Containers--Blobs--and-Metadata">символы больших двоичных объектов Azure</a>. Это не касается имен контейнеров и имен файлов. |
| **Формат даты** (необязательное свойство) | При использовании переменной даты в пути это формат даты, по которому упорядочены файлы. Например, `YYYY/MM/DD`. <br/><br/> Если входные данные большого двоичного объекта имеют `{date}` или `{time}` в пути, папки проверяются в порядке возрастания времени.|
| **Формат времени** (необязательное свойство) |  При использовании переменной времени в пути это формат времени, в котором размещаются файлы. В настоящее время единственным поддерживаемым значением в течение нескольких часов является `HH`. |
| **Ключ секции** | Это необязательное поле, которое доступно только в том случае, если для задания настроено использование [уровня совместимости](./stream-analytics-compatibility-level.md) 1,2 или выше. Если входные данные секционированы по свойству, можно добавить здесь имя этого свойства. Он используется для повышения производительности запроса, если он включает предложение PARTITION BY или GROUP BY для этого свойства. Если для этого задания используется уровень совместимости 1,2 или выше, по умолчанию для этого поля используется значение "PartitionId". |
| **Число входных секций** | Это поле доступно только в том случае, если в шаблоне пути есть {Partition}. Значением этого свойства является целое число >= 1. Где {Partition} отображается в pathPattern, будет использоваться число от 0 до значения этого поля-1. |
| **Формат сериализации событий** | Формат сериализации (JSON, CSV, Avro или [другой [Protobuf, XML, собственный формат...]](custom-deserializer.md)) входящего потока данных.  Убедитесь, что формат JSON совпадает со спецификацией и не содержит ведущий "0" в десятичных числах. |
| **Кодирование** | В настоящее время единственным поддерживаемым форматом кодирования файлов CSV и JSON является UTF-8. |
| **Сжатие** | Тип сжатия используется для чтения таких входящих потоков данных, как None (по умолчанию), GZip или Deflate. |

При поступлении данных из хранилища больших двоичных объектов запрос Stream Analytics может получить доступ к следующим полям метаданных:

| Свойство | Описание |
| --- | --- |
| **BlobName** |Имя входного большого двоичного объекта, от которого поступило событие. |
| **EventProcessedUtcTime** |Дата и время обработки события службой Stream Analytics. |
| **BlobLastModifiedUtcTime** |Дата и время последнего изменения большого двоичного объекта. |
| **PartitionId** |Идентификатор секции для входного адаптера (нумерация идет от нуля). |

Например, используя эти поля, можно писать запросы, как в следующем примере:

```sql
SELECT
    BlobName,
    EventProcessedUtcTime,
    BlobLastModifiedUtcTime
FROM Input
```

## <a name="next-steps"></a>Дальнейшие действия
> [!div class="nextstepaction"]
> [Краткое руководство. по созданию задания Stream Analytics с помощью портала Azure](stream-analytics-quick-create-portal.md)

<!--Link references-->
[stream.analytics.developer.guide]: ../stream-analytics-developer-guide.md
[stream.analytics.scale.jobs]: stream-analytics-scale-jobs.md
[stream.analytics.introduction]: stream-analytics-introduction.md
[stream.analytics.get.started]: stream-analytics-real-time-fraud-detection.md
[stream.analytics.query.language.reference]: /stream-analytics-query/stream-analytics-query-language-reference
[stream.analytics.rest.api.reference]: /rest/api/streamanalytics/