---
title: Потоковая передача данных из Stream Analytics в Data Lake Storage 1-го поколения Azure
description: Используйте Azure Stream Analytics для потоковой передачи данных в Azure Data Lake Storage 1-го поколения.
author: twooley
ms.service: data-lake-store
ms.topic: how-to
ms.date: 05/30/2018
ms.author: twooley
ms.openlocfilehash: 42c7894c33fe0f09748beee20508e7670545c0ed
ms.sourcegitcommit: 877491bd46921c11dd478bd25fc718ceee2dcc08
ms.contentlocale: ru-RU
ms.lasthandoff: 07/02/2020
ms.locfileid: "85515172"
---
# <a name="stream-data-from-azure-storage-blob-into-azure-data-lake-storage-gen1-using-azure-stream-analytics"></a>Потоковая передача данных из большого двоичного объекта службы хранилища Azure в Azure Data Lake Storage 1-го поколения с помощью Azure Stream Analytics
В этой статье вы узнаете, как использовать Azure Data Lake Storage 1-го поколения в качестве выходных данных для задания Azure Stream Analytics. В этой статье показан простой сценарий, в котором данные считываются из большого двоичного объекта службы хранилища Azure (входные данные) и записываются в Data Lake Storage 1-го поколения (выходные данные).

## <a name="prerequisites"></a>Предварительные требования
Перед началом работы с этим учебником необходимо иметь следующее:

* **Подписка Azure**. См. страницу [бесплатной пробной версии Azure](https://azure.microsoft.com/pricing/free-trial/).

* **Учетная запись хранения Azure.** Контейнер больших двоичных объектов из этой учетной записи будет использоваться для ввода данных для задания Stream Analytics. Для работы с этим руководством предполагается, что у вас есть учетная запись хранилища с именем **storageforasa**, а в ней — контейнер с именем **storageforasacontainer**. После создания контейнера отправьте в него образец файла данных. 
  
* **Учетная запись Data Lake Storage 1-го поколения**. Следуйте инструкциям из статьи [Начало работы с Azure Data Lake Storage Gen1 с помощью портала Azure](data-lake-store-get-started-portal.md). Предположим, у вас есть учетная запись Data Lake Storage 1-го поколения **myadlsg1**. 

## <a name="create-a-stream-analytics-job"></a>Создание задания Stream Analytics
Для начала нужно создать задание Stream Analytics с источником входных данных и целевым объектом для выходных данных. В этом руководстве источником является контейнер больших двоичных объектов Azure, а целевым объектом — Data Lake Storage 1-го поколения.

1. Выполните вход на [портал Azure](https://portal.azure.com).

2. В области слева щелкните **Задания Stream Analytics**, а затем нажмите кнопку **Добавить**.

    ![Создание задания Stream Analytics](./media/data-lake-store-stream-analytics/create.job.png "Создание задания Stream Analytics")

    > [!NOTE]
    > Следите за тем, чтобы задание создавалось в той же области, в которой расположена учетная запись хранения. Иначе вам придется заплатить за перемещение данных между регионами.
    >

## <a name="create-a-blob-input-for-the-job"></a>Создание входных данных большого двоичного объекта для задания

1. Откройте страницу задания Stream Analytics, на панели слева перейдите на вкладку **Входные данные** и выберите команду **Добавить**.

    ![Добавление входных данных в задание](./media/data-lake-store-stream-analytics/create.input.1.png "Добавление входных данных в задание")

2. В колонке **Создание входных данных** введите следующие значения.

    ![Добавление входных данных в задание](./media/data-lake-store-stream-analytics/create.input.2.png "Добавление входных данных в задание")

   * **Входной псевдоним** — введите уникальное имя для этих входных данных задания.
   * **Тип источника** — выберите **Поток данных**.
   * **Источник** — выберите **Хранилище больших двоичных объектов**.
   * **Подписка** — выберите **Использовать хранилище BLOB-объектов из текущей подписки**.
   * **Учетная запись хранилища** — выберите учетную запись, которую вы создали при подготовке необходимых условий. 
   * **Контейнер** — выберите контейнер, который вы создали в этой учетной записи.
   * **Формат сериализации событий** — выберите **CSV**.
   * **Разделитель** — выберите **Табуляция**.
   * **Кодировка** — выберите **UTF-8**.

     Нажмите кнопку **Создать**. Портал добавит входные данные и проверит подключение к ним.


## <a name="create-a-data-lake-storage-gen1-output-for-the-job"></a>Создание выходных данных Data Lake Storage 1-го поколения для задания

1. Откройте страницу задания Stream Analytics, перейдите на вкладку **Выходные данные**, щелкните **Добавить** и выберите **Data Lake Storage 1-го поколения**.

    ![Добавление выходных данных в задание](./media/data-lake-store-stream-analytics/create.output.1.png "Добавление выходных данных в задание")

2. В колонке **Новые выходные данные** введите следующие значения.

    ![Добавление выходных данных в задание](./media/data-lake-store-stream-analytics/create.output.2.png "Добавление выходных данных в задание")

    * **Выходной псевдоним** — введите уникальное имя для этих выходных данных задания. Это понятное имя, которое используется в запросах для направления выходных данных запроса в соответствующую учетную запись Data Lake Storage 1-го поколения.
    * Появится запрос на авторизацию доступа к учетной записи Data Lake Storage 1-го поколения. Щелкните **Авторизовать**.

3. В колонке **Новые выходные данные** продолжайте ввод значений.

    ![Добавление выходных данных в задание](./media/data-lake-store-stream-analytics/create.output.3.png "Добавление выходных данных в задание")

   * **Имя учетной записи** — выберите учетную запись Data Lake Storage 1-го поколения, которую вы создали там, куда хотите отправлять результаты задания.
   * **Шаблон префикса в пути** — введите путь для сохранения файлов в указанной учетной записи Data Lake Storage 1-го поколения.
   * **Формат даты** — если в префиксе пути используется маркер даты, вы можете выбрать формат даты для упорядочивания своих файлов.
   * **Формат времени** — если в префиксе пути используется маркер времени, вы можете выбрать формат времени для упорядочивания своих файлов.
   * **Формат сериализации событий** — выберите **CSV**.
   * **Разделитель** — выберите **Табуляция**.
   * **Кодировка** — выберите **UTF-8**.
    
     Нажмите кнопку **Создать**. Портал добавит выходные данные и проверит подключение к ним.
    
## <a name="run-the-stream-analytics-job"></a>Выполнение задания Stream Analytics

1. Чтобы выполнить Stream Analytics задание, необходимо выполнить запрос на вкладке **запрос** . В этом руководстве можно выполнить пример запроса, заменив заполнители на входные и выходные псевдонимы задания, как показано на снимке экрана ниже.

    ![Выполнение запроса](./media/data-lake-store-stream-analytics/run.query.png "Выполнение запроса")

2. Щелкните **Сохранить** в верхней части экрана, а затем на вкладке **Обзор** щелкните **Запустить**. В диалоговом окне выберите **Настраиваемое время** и установите текущие дату и время.

    ![Настройка времени задания](./media/data-lake-store-stream-analytics/run.query.2.png "Настройка времени задания")

    Щелкните **Пуск**, чтобы начать задание. Для запуска задания может потребоваться несколько минут.

3. Чтобы запустить задание выбора данных из большого двоичного объекта, скопируйте образец файла данных в контейнер больших двоичных объектов. Его можно получить из [репозитория Git Azure Data Lake](https://github.com/Azure/usql/tree/master/Examples/Samples/Data/AmbulanceData/Drivers.txt). В нашем примере мы скопируем файл **vehicle1_09142014.csv**. Чтобы передать данные в контейнер больших двоичных объектов, можно использовать различные клиенты, например [обозреватель хранилищ Azure](https://storageexplorer.com/).

4. На вкладке **Обзор** в разделе **Мониторинг** можно наблюдать за ходом обработки данных.

    ![Отслеживание задания](./media/data-lake-store-stream-analytics/run.query.3.png "Отслеживание задания")

5. А теперь можно проверить, появились ли выходные данные задания в учетной записи Data Lake Storage 1-го поколения. 

    ![Проверка выходных данных](./media/data-lake-store-stream-analytics/run.query.4.png "Проверка выходных данных")

    В области обозревателя данных Azure Data Explorer можно увидеть, что выходные данные записаны в папку, указанную в параметрах выходных данных Data Lake Storage 1-го поколения (`streamanalytics/job/output/{date}/{time}`).  

## <a name="see-also"></a>См. также
* [Создание кластера HDInsight для работы с Data Lake Storage 1-го поколения](data-lake-store-hdinsight-hadoop-use-portal.md)
