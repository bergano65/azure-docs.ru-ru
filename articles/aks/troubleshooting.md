---
title: Устранение распространенных проблем со Службой Azure Kubernetes
description: Узнайте, как устранить распространенные проблемы при использовании Службы Azure Kubernetes (AKS).
services: container-service
ms.topic: troubleshooting
ms.date: 06/20/2020
ms.openlocfilehash: f334f501335e9e384cfcc35b356e61ab66efe7a8
ms.sourcegitcommit: dabd9eb9925308d3c2404c3957e5c921408089da
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 07/11/2020
ms.locfileid: "86243687"
---
# <a name="aks-troubleshooting"></a>Устранение неполадок с AKS

При создании кластеров Azure Kubernetes Service (AKS) или управлении ими иногда могут возникать проблемы. В этой статье описаны некоторые распространенные проблемы и действия по устранению неполадок.

## <a name="in-general-where-do-i-find-information-about-debugging-kubernetes-problems"></a>Где найти общие сведения об отладке проблем Kubernetes?

[Вот официальное руководство по устранению неполадок с кластерами Kubernetes](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/).
[Это руководство по устранению неполадок](https://github.com/feiskyer/kubernetes-handbook/blob/master/en/troubleshooting/index.md) модулей pod, узлов, кластеров и т. д., опубликованное инженером Майкрософт.

## <a name="im-getting-a-quota-exceeded-error-during-creation-or-upgrade-what-should-i-do"></a>Я получаю ошибку превышения квоты во время создания или обновления.   Что следует делать? 

 [Запросить еще ядер](../azure-portal/supportability/resource-manager-core-quotas-request.md).

## <a name="what-is-the-maximum-pods-per-node-setting-for-aks"></a>Каково максимальное число модулей pod на узел, установленное для AKS?

Если вы развертываете кластер AKS на портале Azure, максимальное число модулей pod на узел по умолчанию составляет 30.
Если вы развертываете кластер AKS в Azure CLI, максимальное число модулей pod на узел по умолчанию составляет 110. Убедитесь, что используется последняя версия Azure CLI. Этот параметр можно изменить с помощью флага `–-max-pods` в команде `az aks create`.

## <a name="im-getting-an-insufficientsubnetsize-error-while-deploying-an-aks-cluster-with-advanced-networking-what-should-i-do"></a>Я получаю ошибку insufficientSubnetSize при развертывании кластера AKS с использованием расширенного сетевого взаимодействия.   Что следует делать?

Эта ошибка указывает, что подсеть, используемая для кластера, больше не имеет доступных IP-адресов в пределах CIDR для успешного назначения ресурсов. Для кластеров Кубенет требуется достаточное пространство IP-адресов для каждого узла в кластере. Для кластеров Azure CNI требуется достаточное пространство IP-адресов для каждого узла и Pod в кластере.
Узнайте больше о [проектировании Azure CNI для назначения IP-адресов для модулей](configure-azure-cni.md#plan-ip-addressing-for-your-cluster)Pod.

Эти ошибки также отображаются в [AKS Diagnostics](./concepts-diagnostics.md) , которые заблаговременно выводят проблемы, например недостаточный размер подсети.

Следующие три варианта (3) вызывают ошибку недостаточного размера подсети:

1. Масштабирование AKS или AKS Нодепул
   1. Если используется Кубенет, это происходит, когда `number of free IPs in the subnet` значение **меньше** , чем `number of new nodes requested` .
   1. Если используется Azure CNI, это происходит, когда `number of free IPs in the subnet` значение **меньше** , чем `number of nodes requested times (*) the node pool's --max-pod value` .

1. Обновление AKS или AKS Нодепул
   1. Если используется Кубенет, это происходит, когда `number of free IPs in the subnet` значение **меньше** , чем `number of buffer nodes needed to upgrade` .
   1. Если используется Azure CNI, это происходит, когда `number of free IPs in the subnet` значение **меньше** , чем `number of buffer nodes needed to upgrade times (*) the node pool's --max-pod value` .
   
   По умолчанию кластеры AKS устанавливают значение максимального всплеска напряжения ("буфер обновления"), равное одному (1), но это поведение может быть настроено путем установки [максимального значения всплеска для пула узлов](upgrade-cluster.md#customize-node-surge-upgrade-preview) , что увеличит количество доступных IP-адресов, необходимых для завершения обновления.

1. AKS CREATE или AKS Нодепул Add
   1. Если используется Кубенет, это происходит, когда `number of free IPs in the subnet` значение **меньше** , чем `number of nodes requested for the node pool` .
   1. Если используется Azure CNI, это происходит, когда `number of free IPs in the subnet` значение **меньше** , чем `number of nodes requested times (*) the node pool's --max-pod value` .

Для создания новых подсетей можно использовать следующие способы устранения рисков. Разрешение на создание новой подсети требуется для устранения рисков из-за невозможности обновить диапазон CIDR существующей подсети.

1. Перестройте новую подсеть с более крупным диапазоном CIDR, достаточным для целей операций:
   1. Создайте новую подсеть с новым нужным диапазоном, отличным от перекрытия.
   1. Создайте новый нодепул в новой подсети.
   1. Очистка модулей Pod из старого нодепул, находящегося в старой подсети для замены.
   1. Удалите старую подсеть и старую нодепул.

## <a name="my-pod-is-stuck-in-crashloopbackoff-mode-what-should-i-do"></a>Мой модуль pod завис в режиме CrashLoopBackOff.   Что следует делать?

Модуль pod может зависнуть в этом режиме по различным причинам. Вы можете просмотреть:

* сведения о самом модуле pod с помощью команды `kubectl describe pod <pod-name>`;
* сведения в журналах с помощью команды `kubectl logs <pod-name>`.

Дополнительные сведения об устранении неполадок модуля pod см. в статье об [отладке приложений](https://kubernetes.io/docs/tasks/debug-application-cluster/debug-application/#debugging-pods).

## <a name="im-receiving-tcp-timeouts-when-using-kubectl-or-other-third-party-tools-connecting-to-the-api-server"></a>Я получаю `TCP timeouts` при использовании `kubectl` или других сторонних средств, подключающихся к серверу API
AKS содержит плоскости управления высокой доступности, которые масштабируются по вертикали в соответствии с количеством ядер, чтобы гарантировать его цели уровня обслуживания (SLO) и соглашения об уровне обслуживания (SLA). Если время ожидания подключений истекло, проверьте следующее:

- **Истечет ли время ожидания команд API или только некоторые из них?** Если это всего лишь несколько, то есть `tunnelfront` модуль или модуль `aks-link` , отвечающий за обмен данными с плоскостью управления узлами >, может быть не в состоянии выполнения. Убедитесь, что узлы, на которых размещается этот модуль, не чрезмерно загружены или перегружены. Рассмотрите возможность перемещения их в собственный [ `system` пул узлов](use-system-pools.md).
- **Вы открыли все необходимые порты, полные доменные имена и IP-адреса, указанные в [документах AKS restrict исходящего трафика](limit-egress-traffic.md)?** В противном случае вызов нескольких команд может завершиться ошибкой.
- **Находится ли текущий IP-адрес, охваченный [IP-адресами разрешенных диапазонов API](api-server-authorized-ip-ranges.md)?** Если вы используете эту функцию и ваш IP-адрес не включен в диапазоны, ваши вызовы будут заблокированы. 
- **Имеются ли у клиента или приложения утечки вызовов к серверу API?** Обязательно используйте контрольные значения вместо частых вызовов Get, а сторонние приложения не вызывают такие вызовы. Например, ошибка в Istio микшере приводит к тому, что новое подключение к серверу API будет создаваться каждый раз при внутреннем чтении секрета. Так как такое поведение происходит через обычный интервал, Просмотр подключений быстро накапливается и, в конечном итоге, приводит к перегрузке сервера API независимо от шаблона масштабирования. https://github.com/istio/istio/issues/19481
- **У вас есть много выпусков в развертываниях Helm?** Этот сценарий может привести к тому, что оба сервера используют слишком много памяти на узлах, а также большой объем `configmaps` , что может привести к ненужным пиковым нагрузкам на сервере API. Рекомендуется настроить `--history-max` в `helm init` и использовать новый Helm 3. Дополнительные сведения о следующих проблемах: 
    - https://github.com/helm/helm/issues/4821
    - https://github.com/helm/helm/issues/3500
    - https://github.com/helm/helm/issues/4543


## <a name="im-trying-to-enable-role-based-access-control-rbac-on-an-existing-cluster-how-can-i-do-that"></a>Я пытаюсь включить Управление доступом на основе ролей (RBAC) в существующем кластере. Как это сделать?

Включение Управления доступом на основе ролей (RBAC) в существующих кластерах в настоящее время не поддерживается. Его нужно включать при создании новых кластеров. RBAC включен по умолчанию при использовании CLI, портала или версии API, более поздней, чем `2020-03-01`.

## <a name="i-created-a-cluster-with-rbac-enabled-and-now-i-see-many-warnings-on-the-kubernetes-dashboard-the-dashboard-used-to-work-without-any-warnings-what-should-i-do"></a>Я создал кластер с включенным RBAC, и теперь я вижу много предупреждений на панели мониторинга Kubernetes. Ранее на панели мониторинга не возникало никаких предупреждений.   Что следует делать?

Причина предупреждений в том, что в кластере включен RBAC и доступ к панели мониторинга теперь ограничен по умолчанию. В целом этот подход является рекомендуемым, так как раскрытие панели мониторинга по умолчанию для всех пользователей кластера может привести к угрозам безопасности. Если вы по-прежнему хотите включить панель мониторинга, следуйте указаниям в этом [блоге](https://pascalnaber.wordpress.com/2018/06/17/access-dashboard-on-aks-with-rbac-enabled/).

## <a name="i-cant-get-logs-by-using-kubectl-logs-or-i-cant-connect-to-the-api-server-im-getting-error-from-server-error-dialing-backend-dial-tcp-what-should-i-do"></a>Мне не удается получить журналы kubectl, или подключение к серверу API завершается сбоем. Выводится ошибка "Error from server: error dialing backend: dial tcp…" (На сервере возникла ошибка: ошибка вызова серверной части: вызывается протокол tcp...). Что делать?

Убедитесь, что порты 22, 9000 и 1194 открыты для подключения к серверу API. Проверьте, выполняется ли модуль `tunnelfront` или `aks-link` в пространстве имен *kube-system* с помощью команды `kubectl get pods --namespace kube-system`. Если он не запущен, принудительно удалите его, и он перезапустится.

## <a name="im-trying-to-upgrade-or-scale-and-am-getting-a-changing-property-imagereference-is-not-allowed-error-how-do-i-fix-this-problem"></a>При попытке обновления или масштабирования я получаю ошибку `"Changing property 'imageReference' is not allowed"`. Как устранить эту проблему?

Возможно, вы получаете эту ошибку, потому что изменили теги в узлах агента внутри кластера AKS. Изменение и удаление тегов и других свойств ресурсов в группе ресурсов MC_* может привести к непредвиденным результатам. Изменение ресурсов в группе MC_* кластера AKS нарушает цель уровня обслуживания (SLO).

## <a name="im-receiving-errors-that-my-cluster-is-in-failed-state-and-upgrading-or-scaling-will-not-work-until-it-is-fixed"></a>Я получаю ошибки, указывающие, что кластер находится в состоянии сбоя и что обновление или масштабирование не состоятся, пока это не будет исправлено

*Эта справка по устранению неполадок берется из https://aka.ms/aks-cluster-failed*

Эта ошибка возникает, когда кластеры переходят в состояние сбоя по нескольким причинам. Выполните следующие действия, чтобы устранить сбой кластера перед повторным выполнением операции, которая ранее не удалась.

1. Пока кластер не выйдет из состояния `failed`, операции `upgrade` и `scale` не будут выполняться. Ниже приведены общие основные проблемы и способы их устранения.
    * Масштабирование с **недостаточной квотой вычислений (CRP)** . Чтобы устранить эту проблему, сначала выполните масштабирование кластера до стабильного целевого состояния в пределах квоты. Затем выполните следующие [действия, чтобы запросить увеличение квоты вычислений](../azure-portal/supportability/resource-manager-core-quotas-request.md) перед попыткой увеличения масштаба после превышения первоначальных пределов квоты.
    * Масштабирование кластера с расширенными сетевым взаимодействием и **недостаточными ресурсами подсети (сетевого взаимодействия)** . Чтобы устранить эту проблему, сначала выполните масштабирование кластера до стабильного целевого состояния в пределах квоты. Затем выполните следующие [действия, чтобы запросить увеличение квоты ресурсов](../azure-resource-manager/templates/error-resource-quota.md#solution) перед попыткой увеличения масштаба после превышения первоначальных пределов квоты.
2. После устранения корневой проблемы с обновлением кластер должен находиться в состоянии "успешно". После проверки состояния "успешно" повторите исходную операцию.

## <a name="im-receiving-errors-when-trying-to-upgrade-or-scale-that-state-my-cluster-is-being-upgraded-or-has-failed-upgrade"></a>При попытке обновления или масштабирования возникли ошибки, указывающие, что выполняется обновление кластера или произошел сбой обновления

*Эта справка по устранению неполадок берется из https://aka.ms/aks-pending-upgrade*

 Кластер или пул узлов нельзя обновлять и масштабировать одновременно. Вместо этого каждый тип операции должен быть завершен на целевом ресурсе перед следующим запросом к этому же ресурсу. В результате операции ограничены при выполнении активных операций обновления, масштабирования или попытках выполнения таких операций. 

Чтобы помочь в диагностике проблемы, запустите `az aks show -g myResourceGroup -n myAKSCluster -o table` для получения подробных сведений о состоянии кластера. В зависимости от результата:

* Если кластер активно обновляется, дождитесь завершения операции. В случае успеха обновления повторите операцию, которая была выполнена ранее.
* Если произошел сбой обновления кластера, выполните действия, описанные в предыдущем разделе.

## <a name="can-i-move-my-cluster-to-a-different-subscription-or-my-subscription-with-my-cluster-to-a-new-tenant"></a>Можно ли переместить кластер в другую подписку или подписку с кластером к новому клиенту?

Если вы переместили кластер AKS в другую подписку или подписку кластера к новому клиенту, кластер не будет работать из-за отсутствия разрешений на удостоверение кластера. **AKS не поддерживает перемещение кластеров между подписками или клиентами** из-за этого ограничения.

## <a name="im-receiving-errors-trying-to-use-features-that-require-virtual-machine-scale-sets"></a>При попытке использования функций, требующих масштабируемых наборов виртуальных машин, возникают ошибки

*Эта справка по устранению неполадок берется из aka.ms/aks-vmss-enablement*

Могут возникать ошибки, указывающие, что кластер AKS не находится в масштабируемом наборе виртуальных машин, как показано в следующем примере:

**AgentPool `<agentpoolname>` включил автомасштабирование, однако не в Масштабируемых наборах виртуальных машин**

Для таких функций, как автомасштабирование кластера или пулы с несколькими узлами, в качестве `vm-set-type` требуются масштабируемые наборы виртуальных машин.

Выполните действия *Перед началом работы* из соответствующего документа, чтобы правильно создать кластер AKS:

* [Использование средства автомасштабирования кластера](cluster-autoscaler.md)
* [Использование нескольких пулов узлов](use-multiple-node-pools.md)
 
## <a name="what-naming-restrictions-are-enforced-for-aks-resources-and-parameters"></a>Какие ограничения именования применяются к ресурсам и параметрам AKS?

*Эта справка по устранению неполадок берется из aka.ms/aks-naming-rules*

Ограничения именования реализуются как платформой Azure, так и AKS. Если имя или параметр ресурса нарушает одно из этих ограничений, возвращается сообщение об ошибке, предлагающее указать другие входные данные. Применяются следующие общие рекомендации по именованию.

* Имена кластеров должны содержать 1–63 символа. Допустимыми являются только буквы, цифры, тире и символ подчеркивания. Первый и последний символ должны быть буквами или цифрами.
* Имя группы ресурсов "Узел AKS/*MC_* " объединяет имя группы ресурсов и имя ресурса. Автоматически сформированный синтаксис `MC_resourceGroupName_resourceName_AzureRegion` должен быть не длиннее 80 символов. При необходимости сократите длину имени группы ресурсов или имени кластера AKS. Вы также можете [модифицировать имя группы ресурсов узла.](cluster-configuration.md#custom-resource-group-name)
* *dnsPrefix* должны начинаться и заканчиваться буквенно-цифровыми значениями и содержать 1–54 символа. Допустимые символы включают в себя буквы, цифры и дефисы (-). *dnsPrefix* не может содержать специальные символы, такие как точка (.).
* Имена пулов узлов AKS должны быть целиком в нижнем регистре и состоять из 1–11 символов для пулов узлов Linux либо 1–6 символов для пулов узлов Windows. Имя должно начинаться с буквы, а допустимыми символами являются буквы и цифры.
* Имя *администратора, которое*задает имя администратора для узлов Linux, должно начинаться с буквы, может содержать только буквы, цифры, дефисы и символы подчеркивания и имеет максимальную длину 64 символов.

## <a name="im-receiving-errors-when-trying-to-create-update-scale-delete-or-upgrade-cluster-that-operation-is-not-allowed-as-another-operation-is-in-progress"></a>При попытке создать, обновить, масштабировать, удалить или обновить кластер возникают ошибки, указывающие, что операция запрещена, так как выполняется другая операция.

*Эта справка по устранению неполадок берется из aka.ms/aks-pending-operation*

Операции с кластером ограничены, если предыдущая операция все еще выполняется. Чтобы получить подробные сведения о состоянии кластера, используйте команду `az aks show -g myResourceGroup -n myAKSCluster -o table`. При необходимости укажите собственную группу ресурсов и имя кластера AKS.

На основе выходных данных состояния кластера:

* Если кластер находится в любом состоянии подготовки, кроме *Успешно* или *Сбой*, дождитесь завершения операции (*Обновление версии/Обновление/Создание/Масштабирование/Удаление/Миграция*). После завершения предыдущей операции повторите свою последнюю операцию с кластером.

* Если имел место сбой обновления кластера, выполните действия, обрисованные как [Я получаю ошибки, указывающие, что кластер находится в состоянии сбоя, и обновление или масштабирование не состоятся, пока это не будет исправлено](#im-receiving-errors-that-my-cluster-is-in-failed-state-and-upgrading-or-scaling-will-not-work-until-it-is-fixed).

## <a name="received-an-error-saying-my-service-principal-wasnt-found-or-is-invalid-when-i-try-to-create-a-new-cluster"></a>При попытке создать новый кластер выдается сообщение об ошибке, сообщающее, что субъект-служба не найдена или недопустима.

При создании кластера AKS для создания ресурсов от вашего имени требуются субъект-служба или управляемое удостоверение. AKS может автоматически создать новый субъект-службу во время создания кластера или получить существующий. При использовании автоматического создания Azure Active Directory необходимо распространить его на каждый регион, чтобы создание прошло успешно. Если распространение занимает слишком много времени, кластер не сможет выполнить проверку при создании, так как не сможет найти доступный субъект-службу. 

Для решения этой проблемы используйте следующие обходные пути.
* Используйте существующий субъект-службу, который уже распространен между регионами и существует для передачи в AKS во время создания кластера.
* При использовании скриптов автоматизации добавьте временные задержки между созданием субъекта-службы и созданием кластера AKS.
* Если используется портал Azure, вернитесь к параметрам кластера во время создания и обновите страницу проверки через несколько минут.





## <a name="im-receiving-errors-after-restricting-egress-traffic"></a>После ограничения исходящего трафика поступают ошибки

При ограничении исходящего трафика из кластера AKS существуют [обязательные и необязательные рекомендованные](limit-egress-traffic.md) исходящие порты/правила сети и правила FQDN/приложений для AKS. Если ваши параметры конфликтуют с любым из этих правил, некоторые команды `kubectl` будут работать неправильно. При создании кластера AKS также могут появиться ошибки.

Убедитесь, что параметры не конфликтуют с какими-либо из обязательных или необязательных рекомендуемых правил исходящих портов/сети и правил полного доменного имени/приложения.

## <a name="azure-storage-and-aks-troubleshooting"></a>Устранение неполадок AKS и службы хранилища Azure

### <a name="what-are-the-recommended-stable-versions-of-kubernetes-for-azure-disk"></a>Какие стабильные версии Kubernetes рекомендуются для диска Azure? 

| Версия Kubernetes | Рекомендуемая версия |
|--|:--:|
| 1.12 | 1.12.9 и более поздние |
| 1.13 | 1.13.6 и более поздние |
| 1,14 | 1.14.2 и более поздние |


### <a name="waitforattach-failed-for-azure-disk-parsing-devdiskazurescsi1lun1-invalid-syntax"></a>Сбой WaitForAttach для диска Azure: синтаксический анализ "/dev/disk/Azure/SCSI1/lun1": недопустимый синтаксис

В Kubernetes версии 1.10 MountVolume.WaitForAttach может завершиться ошибкой с повторным подключением диска Azure.

В Linux может появиться сообщение об ошибке неверного формата DevicePath. Например:

```console
MountVolume.WaitForAttach failed for volume "pvc-f1562ecb-3e5f-11e8-ab6b-000d3af9f967" : azureDisk - Wait for attach expect device path as a lun number, instead got: /dev/disk/azure/scsi1/lun1 (strconv.Atoi: parsing "/dev/disk/azure/scsi1/lun1": invalid syntax)
  Warning  FailedMount             1m (x10 over 21m)   kubelet, k8s-agentpool-66825246-0  Unable to mount volumes for pod
```

В Windows может появиться ошибка неверного номера устройства DevicePath (LUN). Например:

```console
Warning  FailedMount             1m    kubelet, 15282k8s9010    MountVolume.WaitForAttach failed for volume "disk01" : azureDisk - WaitForAttach failed within timeout node (15282k8s9010) diskId:(andy-mghyb
1102-dynamic-pvc-6c526c51-4a18-11e8-ab5c-000d3af7b38e) lun:(4)
```

Эта проблема исправлена в следующих версиях Kubernetes:

| Версия Kubernetes | Исправленная версия |
|--|:--:|
| 1,10 | 1.10.2 и более поздние |
| 1.11 | 1.11.0 и более поздние |
| 1.12 и более поздние | Недоступно |


### <a name="failure-when-setting-uid-and-gid-in-mountoptions-for-azure-disk"></a>Сбой при настройке UID и GID в mountOptions для диска Azure

По умолчанию диск Azure использует ext4,xfs filesystem, а mountOptions, например uid=x,gid=x, не могут быть заданы во время подключения. Например, если вы попытались задать mountOptions uid=999,gid=999, отобразится следующее сообщение об ошибке:

```console
Warning  FailedMount             63s                  kubelet, aks-nodepool1-29460110-0  MountVolume.MountDevice failed for volume "pvc-d783d0e4-85a1-11e9-8a90-369885447933" : azureDisk - mountDevice:FormatAndMount failed with mount failed: exit status 32
Mounting command: systemd-run
Mounting arguments: --description=Kubernetes transient mount for /var/lib/kubelet/plugins/kubernetes.io/azure-disk/mounts/m436970985 --scope -- mount -t xfs -o dir_mode=0777,file_mode=0777,uid=1000,gid=1000,defaults /dev/disk/azure/scsi1/lun2 /var/lib/kubelet/plugins/kubernetes.io/azure-disk/mounts/m436970985
Output: Running scope as unit run-rb21966413ab449b3a242ae9b0fbc9398.scope.
mount: wrong fs type, bad option, bad superblock on /dev/sde,
       missing codepage or helper program, or other error
```

Вы можете устранить эту ошибку, выполнив одно из следующих действий.

* [Настройте контекст безопасности для pod](https://kubernetes.io/docs/tasks/configure-pod-container/security-context/), задав UID в runAsUser и GID в fsGroup. Например, следующий параметр задает запуск модуля pod как root и делает его доступным для любого файла:

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: security-context-demo
spec:
  securityContext:
    runAsUser: 0
    fsGroup: 0
```

  >[!NOTE]
  > Поскольку GID и UID подключаются по умолчанию как root или 0. Если GID или UID задан отличным от root, например 1000, Kubernetes будет использовать `chown` для изменения всех каталогов и файлов на этом диске. Эта операция может занять много времени и привести к очень медленному подключению диска.

* Используйте `chown` в initContainers, чтобы задать GID и UID. Пример:

```yaml
initContainers:
- name: volume-mount
  image: busybox
  command: ["sh", "-c", "chown -R 100:100 /data"]
  volumeMounts:
  - name: <your data volume>
    mountPath: /data
```

### <a name="azure-disk-detach-failure-leading-to-potential-race-condition-issue-and-invalid-data-disk-list"></a>Сбой отключения диска Azure, ведущий к потенциальной проблеме состояния гонки и недопустимому списку дисков данных.

При сбое отключения диска Azure будет выполнено до шести повторных попыток, чтобы отключить диск с помощью экспоненциального выключения. Он также будет удерживать блокировку на уровне узла в списке дисков данных примерно в течение 3 минут. Если список дисков обновить вручную в течение этого времени, то список дисков, сохраняемый блокировкой на уровне узла, станет устаревшим, что приведет к нестабильной работе узла.

Эта проблема исправлена в следующих версиях Kubernetes:

| Версия Kubernetes | Исправленная версия |
|--|:--:|
| 1.12 | 1.12.9 и более поздние |
| 1.13 | 1.13.6 и более поздние |
| 1,14 | 1.14.2 и более поздние |
| 1.15 и более поздние | Недоступно |

Если вы используете версию Kubernetes, которая не включает исправления для этой проблемы, и у узла имеется устаревший список дисков, вы можете исправить проблему, отключив все несуществующие диски от виртуальной машины в рамках групповой операции. **Отключение несуществующих дисков по отдельности может завершиться ошибкой.**

### <a name="large-number-of-azure-disks-causes-slow-attachdetach"></a>Большое количество дисков Azure приводит к снижению скорости их подключения или отключения.

Если количество операций подключения и отключения дисков Azure, нацеленных на виртуальную машину с одним узлом, превышает 10 или превышает 3 при выборе одного пула масштабируемых наборов виртуальных машин, операции могут быть медленнее, чем ожидалось, так как они выполняются последовательно. Эта проблема является известным ограничением, и в настоящее время у нее нет решений. [Пользовательский голосовой элемент для поддержки параллельного подключения и отключения за пределами числа.](https://feedback.azure.com/forums/216843-virtual-machines/suggestions/40444528-vmss-support-for-parallel-disk-attach-detach-for).

### <a name="azure-disk-detach-failure-leading-to-potential-node-vm-in-failed-state"></a>Сбой отключения диска Azure, приводящий к потенциальной виртуальной машине узла в состоянии сбоя

В некоторых случаях отключение диска Azure может завершиться частичным сбоем и оставить виртуальную машину узла в состоянии сбоя.

Эта проблема исправлена в следующих версиях Kubernetes:

| Версия Kubernetes | Исправленная версия |
|--|:--:|
| 1.12 | 1.12.10 и более поздние |
| 1.13 | 1.13.8 и более поздние |
| 1,14 | 1.14.4 и более поздние |
| 1.15 и более поздние | Недоступно |

Если вы применяете версию Kubernetes, которая не включает исправления для этой проблемы, а узел находится в состоянии сбоя, можно устранить неполадки, обновив состояние виртуальной машины вручную, используя один из следующих способов.

* Для кластера на основе группы доступности:
    ```azurecli
    az vm update -n <VM_NAME> -g <RESOURCE_GROUP_NAME>
    ```

* Для кластера на основе VMSS:
    ```azurecli
    az vmss update-instances -g <RESOURCE_GROUP_NAME> --name <VMSS_NAME> --instance-id <ID>
    ```

## <a name="azure-files-and-aks-troubleshooting"></a>Устранение неполадок Файлов Azure и AKS

### <a name="what-are-the-recommended-stable-versions-of-kubernetes-for-azure-files"></a>Какие стабильные версии Kubernetes рекомендуются для Файлов Azure?
 
| Версия Kubernetes | Рекомендуемая версия |
|--|:--:|
| 1.12 | 1.12.6 и более поздние |
| 1.13 | 1.13.4 и более поздние |
| 1,14 | 1.14.0 и более поздние |

### <a name="what-are-the-default-mountoptions-when-using-azure-files"></a>Каковы mountOptions по умолчанию при использовании Файлов Azure?

Рекомендуемые параметры:

| Версия Kubernetes | значение fileMode и dirMode|
|--|:--:|
| 1.12.0–1.12.1 | 0755 |
| 1.12.2 и более поздние | 0777 |

Параметры подключения можно указать в объекте класса хранения. В следующем примере задается значение *0777*.

```yaml
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: azurefile
provisioner: kubernetes.io/azure-file
mountOptions:
  - dir_mode=0777
  - file_mode=0777
  - uid=1000
  - gid=1000
  - mfsymlinks
  - nobrl
  - cache=none
parameters:
  skuName: Standard_LRS
```

Некоторые дополнительные полезные параметры *mountOptions*:

* *mfsymlinks* укажет подключению Файлов Azure (CIFS) поддерживать символические ссылки
* *nobrl* предотвратит отправку запросов на блокировку диапазона байтов на сервер. Этот параметр необходим для некоторых приложений, которые не используют обязательные блокировки диапазона байтов в стиле CIFS. Большинство серверов CIFS пока не поддерживают рекомендательные запросы на блокировку диапазона байтов. Если не использовать *nobrl*, приложения, которые не применяют обязательные блокировки диапазона байтов в стиле CIFS, могут вызвать сообщения об ошибках следующего вида:
    ```console
    Error: SQLITE_BUSY: database is locked
    ```

### <a name="error-could-not-change-permissions-when-using-azure-files"></a>Ошибка "не удалось изменить разрешения" при использовании Файлов Azure.

При запуске PostgreSQL в подключаемом модуле Файлов Azure может появиться сообщение об ошибке следующего вида:

```console
initdb: could not change permissions of directory "/var/lib/postgresql/data": Operation not permitted
fixing permissions on existing directory /var/lib/postgresql/data
```

Эта ошибка вызвана тем, что подключаемый модуль Файлов Azure использует протокол CIFS/SMB. При использовании протокола CIFS/SMB разрешения на доступ к файлам и каталогам не могут быть изменены после подключения.

Чтобы устранить эту проблему, используйте *subPath* вместе с подключаемым модулем диска Azure. 

> [!NOTE] 
> Для диска типа ext3/4 возникает потерянный и найденный каталог после форматирования диска.

### <a name="azure-files-has-high-latency-compared-to-azure-disk-when-handling-many-small-files"></a>Высокая задержка в Файлах Azure по сравнению с диском Azure при обработке большого количества небольших файлов

В некоторых случаях, например при обработке большого количества маленьких файлов, при использовании Файлов Azure может возникнуть высокая задержка, по сравнению с диском Azure.

### <a name="error-when-enabling-allow-access-allow-access-from-selected-network-setting-on-storage-account"></a>Ошибка при включении параметра "Разрешить доступ из выбранной сети" в учетной записи хранения

Если вы включаете *разрешить доступ из выбранной сети* в учетной записи хранения, которая используется для динамической подготовки в AKS, при создании общей папки AKS возникает ошибка:

```console
persistentvolume-controller (combined from similar events): Failed to provision volume with StorageClass "azurefile": failed to create share kubernetes-dynamic-pvc-xxx in account xxx: failed to create file share, err: storage: service returned error: StatusCode=403, ErrorCode=AuthorizationFailure, ErrorMessage=This request is not authorized to perform this operation.
```

Эта ошибка вызвана тем, что *persistentvolume-controller* Kubernetes не находится в сети, выбранной при настройке *разрешить доступ из выбранной сети*.

Вы можете устранить эту проблему, используя [статическую подготовку с помощью Файлов Azure](azure-files-volume.md).

### <a name="azure-files-fails-to-remount-in-windows-pod"></a>Не удается повторно подключить Файлы Azure в модуле Windows

Если удаляется модуль Windows с подключением к Файлам Azure и запланировано его повторное создание на том же узле, подключение завершится ошибкой. Это происходит из-за сбоя команды `New-SmbGlobalMapping`, так как служба Файлы Azure уже подключена к узлу.

Например, может появиться сообщение об ошибке следующего вида:

```console
E0118 08:15:52.041014    2112 nestedpendingoperations.go:267] Operation for "\"kubernetes.io/azure-file/42c0ea39-1af9-11e9-8941-000d3af95268-pvc-d7e1b5f9-1af3-11e9-8941-000d3af95268\" (\"42c0ea39-1af9-11e9-8941-000d3af95268\")" failed. No retries permitted until 2019-01-18 08:15:53.0410149 +0000 GMT m=+732.446642701 (durationBeforeRetry 1s). Error: "MountVolume.SetUp failed for volume \"pvc-d7e1b5f9-1af3-11e9-8941-000d3af95268\" (UniqueName: \"kubernetes.io/azure-file/42c0ea39-1af9-11e9-8941-000d3af95268-pvc-d7e1b5f9-1af3-11e9-8941-000d3af95268\") pod \"deployment-azurefile-697f98d559-6zrlf\" (UID: \"42c0ea39-1af9-11e9-8941-000d3af95268\") : azureMount: SmbGlobalMapping failed: exit status 1, only SMB mount is supported now, output: \"New-SmbGlobalMapping : Generic failure \\r\\nAt line:1 char:190\\r\\n+ ... ser, $PWord;New-SmbGlobalMapping -RemotePath $Env:smbremotepath -Cred ...\\r\\n+                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\r\\n    + CategoryInfo          : NotSpecified: (MSFT_SmbGlobalMapping:ROOT/Microsoft/...mbGlobalMapping) [New-SmbGlobalMa \\r\\n   pping], CimException\\r\\n    + FullyQualifiedErrorId : HRESULT 0x80041001,New-SmbGlobalMapping\\r\\n \\r\\n\""
```

Эта проблема исправлена в следующих версиях Kubernetes:

| Версия Kubernetes | Исправленная версия |
|--|:--:|
| 1.12 | 1.12.6 и более поздние |
| 1.13 | 1.13.4 и более поздние |
| 1.14 и более поздние | Недоступно |

### <a name="azure-files-mount-fails-because-of-storage-account-key-changed"></a>Сбой подключения к Файлам Azure из-за изменения ключа учетной записи хранения

Если ключ учетной записи хранения изменился, вы можете столкнуться со сбоями при подключении Файлов Azure.

Чтобы устранить ошибку, вручную обновите поле `azurestorageaccountkey` в секрете файла Azure с помощью ключа учетной записи хранения в кодировке Base64.

Чтобы закодировать ключ учетной записи хранения в Base64, можно использовать `base64`. Например:

```console
echo X+ALAAUgMhWHL7QmQ87E1kSfIqLKfgC03Guy7/xk9MyIg2w4Jzqeu60CVw2r/dm6v6E0DWHTnJUEJGVQAoPaBc== | base64
```

Чтобы обновить секретный файл Azure, используйте `kubectl edit secret`. Пример:

```console
kubectl edit secret azure-storage-account-{storage-account-name}-secret
```

Через несколько минут узел агента повторит попытку подключения Файла Azure к обновленному ключу хранилища.


### <a name="cluster-autoscaler-fails-to-scale-with-error-failed-to-fix-node-group-sizes"></a>Сбой автомасштабирования кластера с ошибкой "не удалось исправить размеры групп узлов"

Если автомасштабирование кластера в ту или иную сторону не выполняется и вы видите ошибку, подобную приведенной ниже, в [ журналах автомасштабирования кластера][view-master-logs].

```console
E1114 09:58:55.367731 1 static_autoscaler.go:239] Failed to fix node group sizes: failed to decrease aks-default-35246781-vmss: attempt to delete existing nodes
```

Эта ошибка возникла из-за состояния гонки автомасштабирования вышестоящего кластера. В этом случае автомасштабирование кластера заканчивается значением, отличным от того, которое фактически находится в кластере. Чтобы выйти из этого состояния, отключите и снова включите [автомасштабирование кластера][cluster-autoscaler].

### <a name="slow-disk-attachment-getazuredisklun-takes-10-to-15-minutes-and-you-receive-an-error"></a>Медленное подключение диска GetAzureDiskLun занимает от 10 до 15 минут, и появляется сообщение об ошибке

В версиях Kubernetes, **предшествовавших 1.15.0**, может появиться сообщение об ошибке, например **Ошибка WaitForAttach, не удается найти LUN для диска**.  Чтобы решить эту проблему, подождите примерно 15 минут и повторите попытку.

<!-- LINKS - internal -->
[view-master-logs]: view-master-logs.md
[cluster-autoscaler]: cluster-autoscaler.md
