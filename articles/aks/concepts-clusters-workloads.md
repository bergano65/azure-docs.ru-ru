---
title: Основные концепции Kubernetes — служба Azure Kubernetes (AKS)
description: Сведения о компонентах Kubernetes для базового кластера и рабочей нагрузки и из связях с функциями службы Azure Kubernetes (AKS)
services: container-service
ms.topic: conceptual
ms.date: 06/03/2019
ms.openlocfilehash: 17203123ceb0c196bd8f9011e2962f5022e54698
ms.sourcegitcommit: 693df7d78dfd5393a28bf1508e3e7487e2132293
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 10/28/2020
ms.locfileid: "92901288"
---
# <a name="kubernetes-core-concepts-for-azure-kubernetes-service-aks"></a>Ключевые концепции Kubernetes для службы Azure Kubernetes (AKS)

По мере того как разработка приложений перемещается в сторону подхода на основе контейнера, важно управлять ресурсами и контролировать их. Kubernetes сейчас является ведущей платформой надежного планирования рабочих нагрузок для отказоустойчивых приложений. Служба Azure Kubernetes (AKS) — это управляемая среда Kubernetes, упрощающая развертывание приложений на основе контейнеров и управление ими.

В этой статье описываются основные компоненты инфраструктуры Kubernetes, такие как *плоскость управления* , *узлы* и *Пулы узлов* . Кроме того, здесь дается представление о ресурсах рабочих нагрузок, таких как модули *pod* , *развертывания* и *наборы* , а также о возможности группировать ресурсы в *пространства имен* .

## <a name="what-is-kubernetes"></a>Что такое Kubernetes?

Платформа Kubernetes, которая сейчас очень быстро развивается, предназначена для управления контейнерными приложениями и связанными с ними компонентами сети и хранилища. Основное внимание в ней уделяется рабочим нагрузкам приложений, а не базовым компонентам инфраструктуры. В Kubernetes реализуется декларативный подход к развертываниям, подкрепленный продуманным набором API-интерфейсов для операций управления.

Вы можете создавать и запускать переносимые версии современных приложений на базе микрослужб, использующие возможности Kubernetes по оркестрации и управлению доступностью для поддерживаемых компонентов приложения. Kubernetes поддерживает приложения без отслеживания состояния и с отслеживанием состояния, что очень удобно при адаптации приложений на базе микрослужб.

Kubernetes как открытая платформа позволяет создавать приложения на любом языке программирования, для любой операционной системы, с применением любых библиотек и служб сообщений. С Kubernetes можно интегрировать любые средства непрерывной интеграции и непрерывной поставки (CI/CD) для планирования и развертывания выпусков.

Служба Azure Kubernetes (AKS) предоставляет управляемую службу Kubernetes, которая упрощает выполнение важнейших задач развертывания и управления, в том числе координацию обновлений. Плоскость управления AKS управляется платформой Azure, и вы платите только за узлы AKS, на которых выполняются ваши приложения. AKS построен на основе модуля Azure Kubernetes Service Engine с открытым исходным кодом ([AKS-Engine][aks-engine]).

## <a name="kubernetes-cluster-architecture"></a>Архитектура кластера Kubernetes

Кластер Kubernetes разделяется на два компонента:

- Узлы *плоскости управления* предоставляют базовые Kubernetes службы и оркестрации рабочих нагрузок приложений.
- *узлы* непосредственно выполняют рабочие нагрузки приложения.

![Kubernetes плоскость управления и компоненты узла](media/concepts-clusters-workloads/control-plane-and-nodes.png)

## <a name="control-plane"></a>Уровень управления

При создании кластера AKS автоматически создается и настраивается плоскость управления. Эта плоскость управления предоставляется как управляемый ресурс Azure, абстрактный от пользователя. На плоскость управления не взимается плата, а только узлы, входящие в кластер AKS. Плоскость управления и ее ресурсы находятся только в том регионе, в котором вы создали кластер.

Плоскость управления включает следующие основные компоненты Kubernetes:

- *kube-apiserver* — сервер API-интерфейсов, который предоставляет базовые API-интерфейсы Kubernetes. Этот компонент поддерживает взаимодействие со средствами управления, например с `kubectl` или панелью мониторинга Kubernetes.
- *etcd* — поддерживает состояния кластера Kubernetes и конфигурации. В Kubernetes поддерживается хранилище ключей *etcd* с высоким уровнем доступности.
- *kube-scheduler* — при создании или масштабировании приложения этот планировщик принимает решения о том, какие узлы могут выполнять рабочую нагрузку, и запускает их.
- *kube-controller-manager* — диспетчер контроллеров управляет работой нескольких небольших контроллеров, которые выполняют такие действия, как репликация модулей pod и обработка операций узлов.

AKS предоставляет плоскость управления с одним клиентом, с выделенным сервером API, планировщиком и т. д. Вы определяете число и размер узлов, а Платформа Azure настраивает безопасное взаимодействие между плоскостью управления и узлами. Взаимодействие с плоскостью управления осуществляется через API-интерфейсы Kubernetes, такие как `kubectl` или панель мониторинга Kubernetes.

Эта управляемая плоскость управления означает, что вам не нужно настраивать такие компоненты, как хранилище *etcd* высокой доступности, но это также означает, что доступ к плоскости управления напрямую невозможен. Обновление до Kubernetes контролируется с помощью Azure CLI или портал Azure, который обновляет плоскость управления, а затем узлы. Чтобы устранить возможные проблемы, можно просмотреть журналы плоскости управления с помощью журналов Azure Monitor.

Если необходимо настроить плоскость управления особым образом или требуется прямой доступ к ней, можно развернуть собственный кластер Kubernetes с помощью [AKS Engine][aks-engine].

Соответствующие рекомендации см. в статье рекомендации [по обеспечению безопасности и обновления кластера в AKS][operator-best-practices-cluster-security].

## <a name="nodes-and-node-pools"></a>Узлы и пулы узлов

Чтобы запускать приложения и вспомогательные службы, вам нужен *узел* Kubernetes. Кластер AKS содержит один или несколько узлов. Они представляют собой виртуальные машины Azure, которые выполняют компоненты узла Kubernetes и среду выполнения контейнера.

- `kubelet`— Это агент Kubernetes, который обрабатывает запросы оркестрации от плоскости управления и планирования выполнения запрошенных контейнеров.
- Поддержка виртуальных сетей обеспечивается прокси-сервером *kube-proxy* , который выполняется в каждом узле. Этот прокси-сервер перенаправляет сетевой трафик и управляет IP-адресами для служб и модулей pod.
- *Среда выполнения контейнера* — это компонент, поддерживающий выполнение контейнерных приложений и их взаимодействие с дополнительными ресурсами, такими как виртуальная сеть и хранилище. В AKS в качестве среды выполнения контейнера используется значок Кита.

![Виртуальная машина Azure и вспомогательные ресурсы для узла Kubernetes](media/concepts-clusters-workloads/aks-node-resource-interactions.png)

Размер виртуальной Машины Azure для узлов определяет количество ядер ЦП, объем памяти, а также тип и размер хранилища (высокопроизводительные твердотельные накопители или обычные жесткие диски), которые будут доступны. Если вы ожидаете, что приложениям потребуется большой объем ресурсов ЦП и (или) памяти и (или) высокая производительность хранилища, учтите это при выборе размера узла. Можно также масштабировать количество узлов в кластере AKS для удовлетворения спроса.

В AKS образ виртуальной машины для узлов в кластере в настоящее время основан на Ubuntu Linux или Windows Server 2019. При создании кластера AKS или масштабировании количества узлов Платформа Azure создает запрошенное количество виртуальных машин и настраивает их. Ручная настройка не выполняется. Узлы агентов выставляются как стандартные виртуальные машины, поэтому все скидки на используемом вами размере виртуальной машины (включая [резервирование Azure][reservation-discounts]) применяются автоматически.

Если вы хотите использовать другую операционную систему узла, контейнерную среду выполнения или пользовательские пакеты, следует развернуть собственный кластер Kubernetes на основе [aks-engine][aks-engine]. Компоненты и параметры конфигурации становятся доступными для вышестоящего обработчика `aks-engine` раньше, чем начинается официальная поддержка в кластерах AKS. Например, если вы хотите использовать среду выполнения контейнеров, отличную от значок Кита, можно использовать `aks-engine` для настройки и развертывания кластера Kubernetes, который соответствует текущим потребностям.

### <a name="resource-reservations"></a>Резервирование ресурсов

Ресурсы узла используются AKS для обеспечения работы узла в составе кластера. Такое использование может создать расхождение между общим ресурсом узла и ресурсами, аллокатабле при использовании в AKS. Эти сведения важно учитывать при задании запросов и ограничений для пользователей, развернутых в Pod.

Чтобы найти ресурсы аллокатабле узла, выполните команду:
```kubectl
kubectl describe node [NODE_NAME]

```

Для поддержания производительности и функциональности узла ресурсы зарезервированы на каждом узле AKS. По мере роста размера узла в ресурсах резервирование ресурсов растет из-за большего количества пользователей, развернутых в модулях, которым требуется управление.

>[!NOTE]
> Использование надстроек AKS, таких как Container Insights (OMS), приведет к использованию дополнительных ресурсов узла.

- Зарезервированный **ЦП зависит** от типа узла и конфигурации кластера, что может привести к меньшему аллокатабле ЦП из-за запуска дополнительных функций.

| Ядра ЦП на узле | 1    | 2    | 4    | 8    | 16 | 32|64|
|---|---|---|---|---|---|---|---|
|KUBE — зарезервировано (миллиардах)|60|100|140|180|260|420|740|

- **Память** — объем памяти, используемый AKS, включает сумму двух значений.

1. Управляющая программа kubelet устанавливается на всех узлах агента Kubernetes для управления созданием и завершением контейнеров. По умолчанию в AKS Эта управляющая программа имеет следующее правило вытеснения: *Memory. available<750Mi* , что означает, что узел всегда должен иметь по крайней мере 750 аллокатабле MI.  Если узел находится ниже порогового значения доступной памяти, kubelet завершит один из работающих модулей, чтобы освободить память на размещающем компьютере и защитить ее. Это действие активируется, когда доступная память уменьшается сверх порогового значения 750Mi.

2. Второе значение — это регрессионная частота резервирования памяти для правильной работы управляющей программы kubelet (KUBE-reserved).
    - 25% от первых 4 ГБ памяти
    - 20% следующих 4 ГБ памяти (до 8 ГБ)
    - 10% от следующих 8 ГБ памяти (до 16 ГБ)
    - 6% следующих 112 ГБ памяти (до 128 ГБ)
    - 2% любой памяти выше 128 ГБ

Приведенные выше правила распределения памяти и ЦП используются для поддержания работоспособности узлов агентов, включая некоторые системные модули размещения, критически важные для работоспособности кластера. Эти правила распределения также приводят к тому, что узел будет сообщать о меньшем объеме памяти аллокатабле и ЦП, чем обычно, если он не был частью кластера Kubernetes. Указанные выше резервирования ресурсов невозможно изменить.

Например, если узел предлагает 7 ГБ, он сообщит 34% памяти, не аллокатабле, включая пороговое значение 750Mi жесткого вытеснения.

`0.75 + (0.25*4) + (0.20*3) = 0.75GB + 1GB + 0.6GB = 2.35GB / 7GB = 33.57% reserved`

Помимо резервирования для Kubernetes, базовая ОС узла также резервирует ресурсы ЦП и памяти для поддержки функций ОС.

Соответствующие рекомендации см. в разделе рекомендации [по основным функциям планировщика в AKS][operator-best-practices-scheduler].

### <a name="node-pools"></a>Пулы узлов

Узлы с одинаковой конфигурацией группируются в *пулы узлов* . Кластер Kubernetes содержит один или несколько пулов узлов. При создании кластера AKS вы указываете начальное количество и размер узлов, которые составляют *пул узлов по умолчанию* . В пуле узлов по умолчанию в AKS содержатся базовые виртуальные машины, на которых выполняются узлы агентов.

> [!NOTE]
> Чтобы обеспечить надежную работу кластера, создайте не менее 2 (двух) узлов в пуле узлов по умолчанию.

Когда вы масштабируете или обновляете кластер AKS, выбранные действия применяются именно к пулу узлов по умолчанию. Можно также масштабировать или обновлять конкретный пул узлов. При операциях обновления все запущенные контейнеры поочередно переносятся в другие узлы в том же пуле узлов, пока все узлы не будут успешно обновлены.

Дополнительные сведения об использовании нескольких пулов узлов в AKS см. в статье [Создание пулов нескольких узлов для кластера в AKS и управление ими][use-multiple-node-pools].

### <a name="node-selectors"></a>Селекторы узлов

В кластере AKS, содержащем несколько пулов узлов, может потребоваться сообщить планировщику Kubernetes, какой пул узлов использовать для данного ресурса. Например, контроллеры входящих данных не должны выполняться на узлах Windows Server. С помощью селекторов узлов можно определить различные параметры, такие как ОС узла, чтобы контролировать, где следует планировать модуль.

В следующем примере экземпляр NGINX планируется на узле Linux с помощью средства выбора узла *"Beta.kubernetes.IO/OS": Linux* :

```yaml
kind: Pod
apiVersion: v1
metadata:
  name: nginx
spec:
  containers:
    - name: myfrontend
      image: mcr.microsoft.com/oss/nginx/nginx:1.15.12-alpine
  nodeSelector:
    "beta.kubernetes.io/os": linux
```

Дополнительные сведения о том, как управлять расположением модулей Pod, см. в разделе рекомендации [по использованию расширенных функций планировщика в AKS][operator-best-practices-advanced-scheduler].

## <a name="pods"></a>Модули pod

Для запуска экземпляров приложения Kubernetes использует *модули pod* . Каждый pod соответствует одному экземпляру приложения. Обычно с каждым контейнером сопоставляется ровно один pod, но в некоторых сложных сценариях pod может содержать несколько контейнеров. Такие pod с несколькими контейнерами назначаются одному узлу и позволяют контейнерам совместно использовать связанные с ними ресурсы.

При создании Pod можно определить *запросы ресурсов* для запроса определенного объема ресурсов ЦП или памяти. В этом случае планировщик Kubernetes будет стараться распределить pod в узел, содержащий достаточное (запрошенное) количество ресурсов. Вы также можете указать максимальное ограничение на используемые ресурсы, чтобы pod не потреблял слишком много вычислительных ресурсов базового узла. Мы рекомендуем включать ограничения ресурсов для всех pod, чтобы планировщик Kubernetes лучше понимал, какие ресурсы потребуются для работы и какие можно использовать.

Дополнительные сведения см. в [обзоре модулей pod Kubernetes ][kubernetes-pods] и [документации по жизненному циклу pod Kubernetes][kubernetes-pod-lifecycle].

Pod представляет собой логический ресурс, а контейнеры выполняют фактические рабочие нагрузки. Модули pod — это временные одноразовые ресурсы. Отдельное планирование pod лишает вас некоторых возможностей Kubernetes, повышающих уровень доступности и избыточность. Вместо этого модули Pod развертываются и управляются *контроллерами* Kubernetes, такими как контроллер развертывания.

## <a name="deployments-and-yaml-manifests"></a>Развертывания и манифесты YAML

*Развертывание* обозначает один или несколько идентичных модулей pod под управлением контроллера развертывания Kubernetes. Развертывание определяет количество создаваемых *реплик* (pod), а планировщик Kubernetes следит за тем, чтобы при возникновении проблем с pod или узлами своевременно создавались дополнительные модули pod в работоспособных узлах.

Вы можете обновить развертывание, изменяя конфигурацию pod, используемого образа контейнера или хранилища данных. Контроллер развертывания будет освобождать и останавливать некоторое число реплик, а также создавать новые реплики с новым определением развертывания. Этот процесс продолжается, пока не будут обновлены все реплики в развертывании.

Для большинства приложений без отслеживания состояний следует использовать именно такую модель развертывания в AKS, а не распределять отдельные модули pod. Kubernetes может отслеживать работоспособность и состояние развертывания, поддерживая выполнение в кластере необходимого числа реплик. Если запланировать отдельные модули, то модули не перезапускаются при возникновении проблемы и не перепланируются на работоспособные узлы, если на их текущем узле возникла проблема.

Если приложению важно иметь минимальный доступный набор экземпляров, поддерживающий свободу в принятии решений, процесс обновления не должен нарушать такую возможность. *Бюджеты неработоспособности pod* позволяют определить, сколько реплик в развертывании допустимо одновременно отключать на период обновления или изменения узлов. Например, если в развертывании имеется *пять реплик (5)* , можно определить прерывание Pod *4* , чтобы только одна реплика была удалена или перепланирована одновременно. Как и с ограничениями ресурсов для pod, мы рекомендуем всегда указывать бюджет неработоспособности pod для приложений, для которых требуется постоянное присутствие минимального числа реплик.

Развертывания обычно создаются и управляются с помощью `kubectl create` или `kubectl apply`. Чтобы создать развертывание, определите файл манифеста в формате YAML. В следующем примере создается базовое развертывание веб-сервера NGINX. В развертывании указано *три (3)* реплики, которые будут созданы, и требуется, чтобы порт *80* был открыт в контейнере. Также определены требуемые и максимальные объемы ресурсов ЦП и памяти.

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: mcr.microsoft.com/oss/nginx/nginx:1.15.2-alpine
        ports:
        - containerPort: 80
        resources:
          requests:
            cpu: 250m
            memory: 64Mi
          limits:
            cpu: 500m
            memory: 256Mi
```

Чтобы создать более сложные приложения, можно включить в манифест в формате YAML дополнительные службы, например подсистемы балансировки нагрузки.

Дополнительные сведения см. в [документации по развертываниям Kubernetes][kubernetes-deployments].

### <a name="package-management-with-helm"></a>Управление пакетами с помощью Helm

Распространенный подход к управлению приложениями в Kubernetes — применение [Helm][helm]. Вы можете создавать новые или использовать существующие общедоступные *диаграммы* Helm, которые содержат упакованную версию кода приложения и YAML-манифесты Kubernetes для развертывания ресурсов. Эти диаграммы Helm можно хранить локально или в удаленном репозитории, например в [репозитории диаграмм Helm в Реестре контейнеров Azure][acr-helm].

Чтобы использовать Helm, установите клиент Helm на компьютере или используйте клиент Helm в [Azure Cloud Shell][azure-cloud-shell]. С помощью клиента вы можете найти или создать диаграммы Helm, а затем установить их в кластере Kubernetes. Дополнительные сведения см. [в статье Установка существующих приложений с помощью Helm в AKS][aks-helm].

## <a name="statefulsets-and-daemonsets"></a>StatefulSet и DaemonSet

Контроллер развертывания использует планировщик Kubernetes для выполнения указанного количества реплик в любом доступном узле с достаточными ресурсами. Такой подход к использованию развертываний часто обоснован для приложений без отслеживания состояния, но непригоден для приложений, которым нужно поддержание постоянных имен или хранилищ. Для приложений, реплика которых должна существовать в каждом узле или в определенном наборе узлов в кластере, контроллер развертывания не отслеживает распределение реплик между узлами.

В работе с такими приложениями вам помогут два ресурса Kubernetes:

- наборы *StatefulSet* , которые поддерживают состояние приложений за пределами жизненного цикла отдельных модулей pod, например для хранилища;
- наборы *Daemonset* , которые обеспечивают запуск экземпляров на каждом узле с самых ранних этапов начальной загрузки Kubernetes.

### <a name="statefulsets"></a>Наборы StatefulSet

Современные приложения часто работают без отслеживания состояния, но если отслеживание состояния будет обязательным, например для приложений с компонентами баз данных, вы можете применить наборы *StatefulSet* . Наборы StatefulSet действуют так же, как развертывание одного или нескольких идентичных модулей pod и управление ими. Для реплик, включенных в StatefulSet, соблюдаются мягкие и последовательные процессы развертывания, масштабирования, обновления и прерывания. С помощью StatefulSet (при перепланировании реплик) соглашения об именовании, сетевых имен и хранилища сохраняются.

Вам достаточно определить приложение в формате YAML с помощью `kind: StatefulSet`, и контроллер StatefulSet возьмет на себя развертывание требуемых реплик, а также управление ими. Данные сохраняются в постоянном хранилище, предоставленном в Управляемых дисках Azure или в службе файлов Azure. При использовании StatefulSet базовое постоянное хранилище сохраняется даже после удаления StatefulSet.

Дополнительные сведения см. в [документации по StatefulSet в Kubernetes][kubernetes-statefulsets].

Включенные в StatefulSet реплики назначаются и выполняются в любом доступном узле кластера AKS. Если вам нужно гарантировать, что на каждом узле выполняется по меньшей мере один модуль pod из набора, правильнее использовать наборы DaemonSet.

### <a name="daemonsets"></a>Наборы DaemonSet

Для некоторых задач сбора журналов и (или) мониторинга, возможно, потребуется выполнять определенный модуль pod во всех узлах или в определенном наборе узлов. Объект *DaemonSet* также предназначен для развертывания одного или нескольких идентичных модулей pod, но, в отличие от предыдущего, контроллер DaemonSet гарантирует выполнение экземпляра pod на каждом из указанных узлов.

Контроллер DaemonSet может распределять модули pod в узлы в самом начале процесса загрузки кластера, еще до запуска стандартного планировщика Kubernetes. Эта возможность гарантирует, что модули pod из набора DaemonSet будут запущены раньше, чем модули pod из основного развертывания или набора StatefulSet.

DaemonSet, как и StatefulSet, включается в определение YAML с помощью `kind: DaemonSet`.

Дополнительные сведения см. в [документации по DaemonSet в Kubernetes][kubernetes-daemonset].

> [!NOTE]
> Если используется [надстройка виртуальных узлов](virtual-nodes-cli.md#enable-virtual-nodes-addon), daemonset не будет создавать модули Pod на виртуальном узле.

## <a name="namespaces"></a>Пространства имен

Ресурсы Kubernetes, такие как модули pod и развертывания, логически группируются в *пространства имен* . Такая группировка позволяет логически разделить кластер AKS и ограничить права на создание, просмотр ресурсов и управление ими. Например, вы можете создать отдельные пространства имен для разных бизнес-подразделений. Пользователи смогут взаимодействовать только с ресурсами из назначенных им пространств имен.

![Пространства имен Kubernetes для логического разделения ресурсов и приложений](media/concepts-clusters-workloads/namespaces.png)

Когда вы создаете кластер AKS, вам доступны следующие пространства имен:

- *default* (по умолчанию) — в этом пространстве имен по умолчанию создаются модули pod и развертывания, для которых не указано пространство имен. В небольших средах вполне допустимо развертывать все приложения в пространстве имен по умолчанию, не создавая дополнительные логические разделы. Если при любом взаимодействии с API Kubernetes, например через `kubectl get pods`, не указано конкретное пространство имен, всегда используется пространство имен по умолчанию.
- *kube-system* — в этом пространстве имен содержатся основные ресурсы, такие как DNS, прокси-сервер и другие сетевые компоненты, а также панели мониторинга Kubernetes. Обычно вам не нужно развертывать приложения в этом пространстве имен.
- *kube-public*  — это пространство имен обычно не используется. Оно позволяет сделать ресурс видимым в пределах всего кластера и доступным для просмотра всем пользователям.

Дополнительные сведения см. в [документации по пространствам имен в Kubernetes][kubernetes-namespaces].

## <a name="next-steps"></a>Дальнейшие действия

Из этой статьи вы узнали об основных компонентах Kubernetes и о том, как они применяются к кластерах AKS. Дополнительные сведения о ключевых понятиях Kubernetes и AKS приведены в следующих статьях:

- [Доступ и идентификация в Kubernetes и AKS][aks-concepts-identity]
- [Безопасность Kubernetes и AKS][aks-concepts-security]
- [Виртуальные сети Kubernetes и AKS][aks-concepts-network]
- [Хранилище Kubernetes и AKS][aks-concepts-storage]
- [Масштабирование Kubernetes и AKS][aks-concepts-scale]

<!-- EXTERNAL LINKS -->
[aks-engine]: https://github.com/Azure/aks-engine
[kubernetes-pods]: https://kubernetes.io/docs/concepts/workloads/pods/pod-overview/
[kubernetes-pod-lifecycle]: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/
[kubernetes-deployments]: https://kubernetes.io/docs/concepts/workloads/controllers/deployment/
[kubernetes-statefulsets]: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/
[kubernetes-daemonset]: https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/
[kubernetes-namespaces]: https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/
[helm]: https://helm.sh/
[azure-cloud-shell]: https://shell.azure.com

<!-- INTERNAL LINKS -->
[aks-concepts-identity]: concepts-identity.md
[aks-concepts-security]: concepts-security.md
[aks-concepts-scale]: concepts-scale.md
[aks-concepts-storage]: concepts-storage.md
[aks-concepts-network]: concepts-network.md
[acr-helm]: ../container-registry/container-registry-helm-repos.md
[aks-helm]: kubernetes-helm.md
[operator-best-practices-cluster-security]: operator-best-practices-cluster-security.md
[operator-best-practices-scheduler]: operator-best-practices-scheduler.md
[use-multiple-node-pools]: use-multiple-node-pools.md
[operator-best-practices-advanced-scheduler]: operator-best-practices-advanced-scheduler.md
[reservation-discounts]:../cost-management-billing/reservations/save-compute-costs-reservations.md
