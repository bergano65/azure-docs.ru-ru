---
title: 'Хранилище: Миграция локальных Apache Hadoop в Azure HDInsight'
description: Ознакомьтесь с рекомендациями по использованию хранилища в рамках миграции локальных кластеров Hadoop в Azure HDInsight.
author: hrasheed-msft
ms.author: hrasheed
ms.reviewer: ashishth
ms.service: hdinsight
ms.topic: how-to
ms.custom: hdinsightactive
ms.date: 12/10/2019
ms.openlocfilehash: cd27babee4b78d22bbd49ab53c1ed2fe5a54a0da
ms.sourcegitcommit: 829d951d5c90442a38012daaf77e86046018e5b9
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 10/08/2020
ms.locfileid: "91856693"
---
# <a name="migrate-on-premises-apache-hadoop-clusters-to-azure-hdinsight"></a>Миграция локальных Apache Hadoop кластеров в Azure HDInsight

В этой статье представлены рекомендации по хранению данных в системах Azure HDInsight. Это часть цикла, где приведены лучшие методики, применимые при перемещении локальных систем Apache Hadoop в Azure HDInsight.

## <a name="choose-right-storage-system-for-hdinsight-clusters"></a>Выбор правильной системы хранения для кластеров HDInsight

Структуру каталогов локальной Apache Hadoop файловой системы (HDFS) можно создать повторно в хранилище BLOB-объектов Azure или Azure Data Lake Storage. Затем можно безопасно и без потери пользовательских данных удалять используемые для расчетов кластеры HDInsight. Обе службы могут использоваться и как файловая система по умолчанию, и как дополнительная файловая система для кластера HDInsight. Кластер HDInsight и учетная запись хранения должны размещаться в одном регионе.

### <a name="azure-storage"></a>Хранилище Azure

Кластеры HDInsight могут использовать контейнер больших двоичных объектов в службе хранилища Azure как файловую систему по умолчанию или как дополнительную файловую систему.С кластерами HDInsight можно использовать учетную запись хранения уровня "Стандартный". Уровень Premier не поддерживается. Стандартный контейнер больших двоичных объектов хранит сведения о кластере, включая журналы заданий.Совместное использование одного контейнера больших двоичных объектов в качестве файловой системы по умолчанию для нескольких кластеров не поддерживается.

Определенные на этапе создания учетные записи хранения и соответствующие ключи хранятся в файле `%HADOOP_HOME%/conf/core-site.xml` на узлах кластера. Их также можно получить в разделе Custom core site (Пользовательский основной сайт) в конфигурации HDFS в интерфейсе Ambari. Ключ учетной записи хранения зашифрован по умолчанию. Перед тем как передать ключи управляющим программам Hadoop, используется собственный сценарий расшифровки. Задания, включая Hive, MapReduce, потоковую передачу Hadoop и Pig, могут переносить описание учетных записей хранения и метаданные вместе с ними.

Хранилище Azure можно географически реплицировать. Хотя это обеспечивает возможность географического восстановления и избыточность данных, переход в расположение геореплицированных данных при отработке отказа заметно сказывается на производительности, что может привести к дополнительным затратам. Поэтому мы рекомендуем взвешенно подходить к выбору георепликации и выбирать ее только в том случае, если ценность данных окупит дополнительные затраты.

Для доступа к данным, хранящимся в службе хранилища Azure, может использоваться один из следующих форматов:

|Формат доступа к данным |Описание |
|---|---|
|`wasb:///`|Хранилище по умолчанию без шифрования обмена данными.|
|`wasbs:///`|Хранилище по умолчанию с шифрованием обмена данными.|
|`wasb://<container-name>@<account-name>.blob.core.windows.net/`|При взаимодействии с учетной записью хранения, кроме используемой по умолчанию. |

[Целевые показатели масштабируемости для учетных записей хранения](../../storage/common/scalability-targets-standard-account.md) уровня "Стандартный" содержат текущие ограничения учетных записей хранения Azure. Если предельно достижимых показателей масштабируемости для одной учетной записи хранения недостаточно для работы приложения, то при разработке такого приложения можно настроить его на использование нескольких таких учетных записей и распределить объекты данных между ними.

[Аналитика службы хранилища Azure](../../storage/storage-analytics.md)   предоставляет метрики для всех служб хранилища и портал Azure можно настроить для сбора метрик для визуализации с помощью диаграмм. Можно создать оповещения, чтобы получать уведомления о достижении пороговых значений для метрик ресурсов хранилища.

Служба хранилища Azure обеспечивает [обратимое удаление объектов BLOB](../../storage/blobs/storage-blob-soft-delete.md) , чтобы помочь восстановить данные при случайном изменении или удалении приложения или другого пользователя учетной записи хранения.

Можно создавать [моментальные снимки больших двоичных объектов](https://docs.microsoft.com/rest/api/storageservices/creating-a-snapshot-of-a-blob). Моментальный снимок — это версия большого двоичного объекта только для чтения, сделанная в определенный момент времени, которая обеспечивает способ резервного копирования этого объекта. После создания моментального снимка его можно читать, копировать или удалять, но нельзя изменять.

> [!Note]
> Для более старых версий локальных дистрибутивов Hadoop, у которых нет сертификата wasbs, этот сертификат необходимо импортировать в доверенное хранилище Java.

Для импорта сертификатов в доверенное хранилище Java могут использоваться следующие методы.

Скачайте сертификат TLS или SSL большого двоичного объекта Azure в файл

```bash
echo -n | openssl s_client -connect <storage-account>.blob.core.windows.net:443 | sed -ne '/-BEGIN CERTIFICATE-/,/-END CERTIFICATE-/p' > Azure_Storage.cer
```

Импортируйте указанный выше файл в доверенное хранилище Java на всех узлах:

```bash
keytool -import -trustcacerts -keystore /path/to/jre/lib/security/cacerts -storepass changeit -noprompt -alias blobtrust -file Azure_Storage.cer
```

Убедитесь, что добавленный сертификат находится в доверенном хранилище:

```bash
keytool -list -v -keystore /path/to/jre/lib/security/cacerts
```

Дополнительные сведения см. в следующих статьях:

- [Использование службы хранилища Azure с кластерами Azure HDInsight](../hdinsight-hadoop-use-blob-storage.md)
- [Целевые показатели масштабируемости для учетных записей хранения уровня "Стандартный"](../../storage/common/scalability-targets-standard-account.md)
- [Целевые показатели масштабируемости и производительности для хранилища BLOB-объектов](../../storage/blobs/scalability-targets.md)
- [Производительность хранилища Microsoft Azure и контрольный список масштабируемости](../../storage/common/storage-performance-checklist.md)
- [Наблюдение, диагностика и устранение неисправностей хранилища Microsoft Azure](../../storage/common/storage-monitoring-diagnosing-troubleshooting.md)
- [Мониторинг учетной записи хранения на портале Azure](../../storage/common/storage-monitor-storage-account.md)

### <a name="azure-data-lake-storage-gen1"></a>Хранилище Azure Data Lake Storage 1-го поколения

Azure Data Lake Storage 1-го поколения реализует модель управления доступом в стиле HDFS и POSIX. Она обеспечивает интеграцию первого класса с Azure AD для детального контроля доступа. Нет ограничений на размер данных, которые можно хранить, или на способность запускать аналитику с массовым параллелизмом.

Дополнительные сведения см. в следующих статьях:

- [Создание кластеров HDInsight с Data Lake Storage 1-го поколения с помощью портал Azure](../../data-lake-store/data-lake-store-hdinsight-hadoop-use-portal.md)
- [Использование Data Lake Storage 1-го поколения с кластерами Azure HDInsight](../hdinsight-hadoop-use-data-lake-storage-gen1.md)

### <a name="azure-data-lake-storage-gen2"></a>Azure Data Lake Storage 2-го поколения

Azure Data Lake Storage 2-го поколения является последним предложением хранилища. Он объединяет основные возможности из первого поколения Azure Data Lake Storage 1-го поколения с конечной точкой файловой системы, совместимой с Hadoop, непосредственно интегрированной в хранилище BLOB-объектов Azure. Это усовершенствование сочетает в себе преимущества масштабирования и экономии хранения объектов с высокой надежностью и производительностью, которые обычно связаны только с локальными файловыми системами.

Azure Data Lake Storage Gen 2 построена на основе [хранилища BLOB-объектов Azure](../../storage/blobs/storage-blobs-introduction.md) и позволяет взаимодействовать с данными с помощью парадигмы файловой системы и хранилища объектов. Это хранилище предоставляет функции  [Azure Data Lake Storage 1-го поколения](../../data-lake-store/index.yml), например семантика файловой системы, защита на уровне файлов и масштабирование, а также экономичность, многоуровневость, возможности высокой доступности и аварийного восстановления, большая экосистема средств и пакетов SDK  [хранилища BLOB-объектов Azure](../../storage/blobs/storage-blobs-introduction.md). В Data Lake Storage Gen2 сохранились все качества хранилища объектов, а также появились возможности работы с файловой системой, что позволило оптимизировать рабочие нагрузки аналитики.

Основной компонент Data Lake Storage 2-го поколения — это добавление [иерархического пространства имен](../../storage/data-lake-storage/namespace.md)   в службу хранилища BLOB-объектов, которое организует объекты и файлы в иерархию каталогов для выполнения доступа к данным.Иерархическая структура позволяет выполнять такие задачи, как переименование или удаление каталога, в виде единичных атомарных операций с метаданными в каталоге. Больше не нужно перечислять или обрабатывать все объекты с общим префиксом имени каталога.

Раньше облачная аналитика влияла на производительность, возможности управления и безопасность. Далее приведены основные функции ADLS 2-го поколения.

- **Доступ, совместимый с Hadoop.** Хранилище Azure Data Lake Storage 2-го поколения позволяет получать доступ к данным и управлять ими так же, как и в  [распределенной файловой системе Hadoop (HDFS)](https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html). Новый [драйвер абфс](../../storage/data-lake-storage/abfs-driver.md)   доступен во всех Apache Hadoop средах, включенных в [Azure HDInsight](../index.yml). Этот драйвер позволяет получать доступ к данным в ADLS 2-го поколения.

- **Надмножество разрешений POSIX**. Модель безопасности Data Lake 2-го поколения полностью поддерживает разрешения ACL и POSIX, а также некоторую дополнительную детализацию, относящуюся к Data Lake Storage 2-го поколения. Параметры могут быть настроены через средства администрирования или с помощью платформ, таких как Hive и Spark.

- **Экономичность.** Data Lake Storage Gen2 отличается низкой стоимостью приобретения емкости хранилища и выполнения транзакций. За счет таких встроенных функций, как  [Жизненный цикл хранилища BLOB-объектов Azure](../../storage/common/storage-lifecycle-management-concepts.md), в ходе жизненного цикла данных тарифные ставки максимально снижаются.

- **Поддержка средств, платформ и приложений хранилища BLOB-объектов.** Хранилище Data Lake Storage Gen2 поддерживает большое количество средств, платформ и приложений хранилища BLOB-объектов.

- **Оптимизированный драйвер**. Драйвер файловой системы больших двоичных объектов Azure (ABFS) [оптимизирован специально](../../storage/data-lake-storage/abfs-driver.md) для анализа больших данных. Соответствующие интерфейсы REST API подключены через конечную точку dfs — dfs.core.windows.net.

Для доступа к данным, хранящимся в ADLS 2-го поколения, может использоваться один из следующих форматов:
- `abfs:///`: доступ к хранилищу Data Lake Storage, используемому по умолчанию для кластера.
- `abfs://file_system@account_name.dfs.core.windows.net`: используется при взаимодействии с Data Lake Storage, отличным от используемого по умолчанию.

Дополнительные сведения см. в следующих статьях:

- [Общие сведения об Azure Data Lake Storage 2-го поколения](../../storage/data-lake-storage/introduction.md)
- [Драйвер файловой системы больших двоичных объектов Azure (ABFS): выделенный драйвер хранилища Azure для Hadoop](../../storage/data-lake-storage/abfs-driver.md)
- [Использование Azure Data Lake Storage Gen2 с кластерами Azure HDInsight](../hdinsight-hadoop-use-data-lake-storage-gen2.md)

## <a name="secure-azure-storage-keys-within-on-premises-hadoop-cluster-configuration"></a>Обеспечение безопасности ключей хранилища Azure в локальной конфигурации кластера Hadoop

Ключи хранилища Azure, которые добавляются в файлы конфигурации Hadoop, устанавливают подключение между локальными HDFS и хранилищем BLOB-объектов Azure. Эти ключи можно защитить, зашифровав их с помощью инфраструктуры поставщика учетных данных Hadoop. Зашифрованные ключи можно надежно хранить и безопасно получать к ним доступ.

**Чтобы подготовить учетные данные:**

```bash
hadoop credential create fs.azure.account.key.account.blob.core.windows.net -value <storage key> -provider jceks://hdfs@headnode.xx.internal.cloudapp.net/path/to/jceks/file
```

**Чтобы добавить указанный выше путь поставщика в файл core-site.xml или в конфигурацию Ambari под настраиваемым основным сайтом:**

```xml
<property>
    <name>hadoop.security.credential.provider.path</name>
        <value>
        jceks://hdfs@headnode.xx.internal.cloudapp.net/path/to/jceks
        </value>
    <description>Path to interrogate for protected credentials.</description>
</property>
```

> [!Note]
> Свойство пути поставщика также можно добавить в командную строку distcp вместо сохранения ключа на уровне кластера в файле core-site.xml следующим образом:

```bash
hadoop distcp -D hadoop.security.credential.provider.path=jceks://hdfs@headnode.xx.internal.cloudapp.net/path/to/jceks /user/user1/ wasb:<//yourcontainer@youraccount.blob.core.windows.net/>user1
```

## <a name="restrict-azure-storage-data-access-using-sas"></a>Ограничение доступа к данным службы хранилища Azure с помощью SAS

HDInsight по умолчанию имеет полный доступ к данным в учетных записях хранения Azure, связанных с кластером. Подписанные URL-адреса (SAS) в контейнере больших двоичных объектов могут использоваться для ограничения доступа к данным, например предоставления пользователям доступа только для чтения.

### <a name="using-the-sas-token-created-with-python"></a>Использование маркера SAS, созданного с помощью Python

1. Откройте файл [SASToken.py](https://github.com/Azure-Samples/hdinsight-dotnet-python-azure-storage-shared-access-signature/blob/master/Python/SASToken.py) и измените следующие значения:

    |Свойство маркера|Описание|
    |---|---|
    |policy_name|Имя, которое будет использоваться для создаваемой хранимой политики.|
    |storage_account_name|Имя учетной записи хранения.|
    |storage_account_key|Ключ для учетной записи хранения.|
    |storage_container_name|Контейнер в учетной записи хранения, доступ к которому необходимо ограничить.|
    |example_file_path|Путь к файлу, который будет отправляться в контейнер.|

2. Файл SASToken.py поставляется с разрешениями `ContainerPermissions.READ + ContainerPermissions.LIST` и может быть скорректирован с учетом варианта использования.

3. Выполните сценарий следующим образом: `python SASToken.py`

4. После выполнения сценария отобразится маркер SAS следующего вида: `sr=c&si=policyname&sig=dOAi8CXuz5Fm15EjRUu5dHlOzYNtcK3Afp1xqxniEps%3D&sv=2014-02-14`

5. Чтобы ограничить доступ к контейнеру с помощью подписанного URL-адреса, добавьте пользовательскую запись в свойстве Add (Добавить) расширенной пользовательской конфигурации основного сайта Ambari HDFS для кластера.

6. В полях **Key** (Ключ) и **Value** (Значение) укажите следующие значения.

    **Ключ**: `fs.azure.sas.YOURCONTAINER.YOURACCOUNT.blob.core.windows.net` **значение**: ключ SAS, возвращенный приложением Python из шага 4 выше.

7. Нажмите кнопку **Add** (Добавить), чтобы сохранить этот ключ и значение, а затем кнопку **Save** (Сохранить), чтобы сохранить изменения в конфигурации. При появлении запроса добавьте описание внесенного изменения (например, "Добавление доступа к хранилищу SAS") и нажмите кнопку **Сохранить**.

8. В пользовательском веб-интерфейсе Ambari выберите HDFS в списке слева, а затем щелкните **Restart All Affected** (Перезапустить все затронутые) в раскрывающемся списке Service Actions (Действия службы) справа. Когда появится запрос, выберите **Conform Restart All** (Подтвердить перезапуск всех).

9. Повторите эту процедуру для MapReduce2 и YARN.

Существует три важных вещи, которые следует помнить при использовании маркеров SAS в Azure:

1. Если маркеры SAS создаются с разрешениями READ + LIST, пользователи, которые обращаются к контейнеру больших двоичных объектов, используя этот маркер SAS, не смогут записывать и удалять данные. Пользователи, которые обращаются к контейнеру больших двоичных объектов, используя этот маркер SAS, и пытаются выполнить операцию записи или удаления, получат сообщение типа `"This request is not authorized to perform this operation"`.

2. Если маркеры SAS создаются с разрешениями `READ + LIST + WRITE` (для ограничения только `DELETE`), команды, подобные `hadoop fs -put`, сначала записывают в файл `\_COPYING\_`, а затем пытаются переименовать файл. Эта операция HDFS сопоставляется с `copy+delete` для WASB. Так как `DELETE` разрешение не было предоставлено, "размещение" приведет к сбою. Операция `\_COPYING\_` — это функция Hadoop, предназначенная для обеспечения некоторого контроля параллелизма. В настоящее время невозможно ограничить операцию удаления, не влияя на операции записи.

3. К сожалению, поставщик учетных данных Hadoop и поставщик ключей расшифровки (Шеллдекриптионкэйпровидер) в настоящее время не работают с маркерами SAS, поэтому в настоящее время их невозможно защитить от видимости.

Дополнительные сведения см. в статье [Использование подписанных URL-адресов хранилища Azure для ограничения доступа к данным в HDInsight](../hdinsight-storage-sharedaccesssignature-permissions.md).

## <a name="use-data-encryption-and-replication"></a>Использование шифрования и репликации данных

Все данные, записываемые в службу хранилища Azure, автоматически шифруются с помощью  [шифрования службы хранилища (SSE)](../../storage/common/storage-service-encryption.md). Данные в учетной записи хранения Azure всегда реплицируются для обеспечения высокой доступности.При создании учетной записи хранения можно выбрать один из следующих вариантов репликации:

- [Локально избыточное хранилище (LRS)](../../storage/common/storage-redundancy-lrs.md)
- [хранилище, избыточное между зонами (ZRS);](../../storage/common/storage-redundancy-zrs.md)
- [Геоизбыточное хранилище (GRS)](../../storage/common/storage-redundancy-grs.md)
- [Геоизбыточное хранилище с доступом на чтение (RA-GRS)](../../storage/common/storage-redundancy.md)

Служба хранилища Azure предоставляет локально избыточное хранилище (LRS), но вы также должны скопировать важные данные в другую учетную запись хранения Azure в другом регионе с частотой, которая соответствует потребностям плана аварийного восстановления.Существует несколько способов копирования данных, включая [ADLCopy](../../data-lake-store/data-lake-store-copy-data-azure-storage-blob.md), [DistCp](https://hadoop.apache.org/docs/current/hadoop-distcp/DistCp.html), [Azure PowerShell](../../data-lake-store/data-lake-store-get-started-powershell.md)или [фабрику данных Azure](../../data-factory/connector-azure-data-lake-store.md).Также рекомендуется применять политики доступа для учетной записи хранения Azure, чтобы предотвратить случайное удаление.

Дополнительные сведения см. в следующих статьях:

- [Репликация службы хранилища Azure](../../storage/common/storage-redundancy.md)
- [Аварийное руководство для Azure Data Lake Storage 1-го поколения (ADLS)](../../data-lake-store/data-lake-store-disaster-recovery-guidance.md)

## <a name="attach-additional-azure-storage-accounts-to-cluster"></a>Подключение дополнительных учетных записей хранения Azure к кластеру

В процессе создания HDInsight в качестве файловой системы по умолчанию выбирается учетная запись хранения Azure, Azure Data Lake Storage 1-го поколения или Azure Data Lake Storage 2-го поколения. Помимо этой учетной записи хранения по умолчанию в процессе создания или после создания кластера можно добавить дополнительные учетные записи хранения из той же или других подписок Azure.

Дополнительную учетную запись хранения можно добавить одним из следующих способов:
- Добавить имя и ключ учетной записи хранения для основного сайта в расширенной пользовательской конфигурации Ambari HDFS и перезапустить службы.
- Использовать [действие сценария](../hdinsight-hadoop-add-storage.md), передав имя и ключ учетной записи хранения.

> [!Note]
> В допустимых случаях ограничения в хранилище Azure можно увеличить с помощью запроса в  [службу поддержки Azure](https://azure.microsoft.com/support/faq/).

Дополнительные сведения см. в статье [Добавление дополнительных учетных записей хранения в HDInsight](../hdinsight-hadoop-add-storage.md).

## <a name="next-steps"></a>Дальнейшие действия

Ознакомьтесь со следующей статьей в этой серии: рекомендации по [переносу данных из локальной среды в Azure HDInsight Hadoop миграции](apache-hadoop-on-premises-migration-best-practices-data-migration.md).
