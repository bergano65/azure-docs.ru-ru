---
title: Руководство по настройке размера кластера HDInsight Interactive Query (LLAP)
description: Руководство по настройке размера LLAP
ms.service: hdinsight
ms.topic: troubleshooting
author: aniket-ms
ms.author: aadnaik
ms.reviewer: HDI HiveLLAP Team
ms.date: 05/05/2020
ms.openlocfilehash: 626b061cc237f7238d47863a3e1ed88961d2f742
ms.sourcegitcommit: 66b0caafd915544f1c658c131eaf4695daba74c8
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 12/18/2020
ms.locfileid: "97680537"
---
# <a name="azure-hdinsight-interactive-query-cluster-hive-llap-sizing-guide"></a>Руководство по настройке размера кластера Azure HDInsight Interactive Query (Hive LLAP)

В этом документе описывается настройка размера кластера Interactive Query HDInsight (Hive LLAP) для типичной рабочей нагрузки с целью достижения разумной производительности. Обратите внимание, что в этом документе приведены общие рекомендации и для конкретных рабочих нагрузок может потребоваться специфичная тонкая настройка.

### <a name="azure-default-vm-types-for-hdinsight-interactive-query-clusterllap"></a>**Типы виртуальных машин Azure по умолчанию для кластера HDInsight Interactive Query (LLAP)**

| Тип узла      | Экземпляр | Размер     |
| :---        |    :----:   | :---     |
| Head      | D13 v2       | 8 виртуальных ЦП, 56 ГБ ОЗУ, диск SSD 400 ГБ   |
| Рабочий узел   | **D14 v2**        | **16 виртуальных ЦП, 112 ГБ ОЗУ, диск SSD 800 ГБ**       |
| ZooKeeper   | A4 v2        | 4 виртуальных ЦП, 8 ГБ ОЗУ, диск SSD 40 ГБ       |

**_Примечание. все рекомендуемые значения конфигурации основаны на узле рабочей роли типа D14 v2_*.  

### <a name="_configuration"></a>_ *Конфигурация:**    
| Ключ конфигурации      | Рекомендуемое значение  | Описание |
| :---        |    :----:   | :---     |
| yarn.nodemanager.resource.memory-mb | 102400 (MБ) | Общий объем памяти в МБ, выделенный для всех контейнеров YARN на узле. | 
| yarn.scheduler.maximum-allocation-mb | 102400 (MБ) | Максимальное выделение памяти в МБ для каждого запроса контейнера в Resource Manager. Запросы на память выше этого значения не будут выполнены. |
| yarn.scheduler.maximum-allocation-vcores | 12 |Максимальное количество ядер ЦП для каждого запроса контейнера в Resource Manager. Запросы на получение больших значений не будут выполнены. |
| yarn.nodemanager.resource.cpu-vcores | 12 | Количество ядер ЦП на каждый диспетчер узлов, которые могут быть выделены для контейнеров. |
| yarn.scheduler.capacity.root.llap.capacity | 85 (%) | Распределение емкости YARN в очереди LLAP.  |
| tez.am.resource.memory.mb | 4096 (МБ) | Объем памяти в МБ, используемый Tez AppMaster. |
| hive.server2.tez.sessions.per.default.queue | <number_of_worker_nodes> |Число сеансов для каждой очереди, заданной в hive.server2.tez.default.queues. Это число соответствует числу координаторов запросов (мастер-приложений Tez). |
| hive.tez.container.size | 4096 (МБ) | Заданный размер контейнера Tez в МБ. |
| hive.llap.daemon.num.executors | 19 | Число исполнителей на одну управляющую программу LLAP. | 
| hive.llap.io.threadpool.size | 19 | Размер пула потоков для исполнителей. |
| hive.llap.daemon.yarn.container.mb | 81920 (МБ) | Общий объем памяти в МБ, используемый отдельными управляющими программами LLAP (памяти на одну управляющую программу).
| hive.llap.io.memory.size | 242688 (МБ) | Размер кэша в МБ на одну управляющую программу LLAP при условии, что кэш SSD включен. |
| hive.auto.convert.join.noconditionaltask.size | 2048 (МБ) | Объем памяти в МБ для соединения с сопоставлением. |

### <a name="llap-architecturecomponents"></a>**Архитектура и компоненты LLAP:**  

!["LLAP Architecture/Components"](./media/hive-llap-sizing-guide/LLAP_architecture_sizing_guide.png "Архитектура и компоненты LLAP")

### <a name="llap-daemon-size-estimations"></a>**Оценка размеров управляющей программы LLAP** 

#### <a name="1-determining-total-yarn-memory-allocation-for-all-containers-on-a-node"></a>**1. Определение общего выделения памяти YARN для всех контейнеров на узле**    
Конфигурация: **_Yarn. NodeManager. resource. Memory-МБ_* _  

Это значение указывает максимальный общий объем памяти в МБ, которая может использоваться контейнерами YARN на каждом узле. Указанное значение должно быть меньше, чем общий объем физической памяти на этом узле.   
Общий объем памяти для всех контейнеров YARN на узле = (общий объем физической памяти — память для ОС + другие службы)  
Присвойте этому параметру значение около 90 % от всей доступной оперативной памяти.  
Для D14 v2 рекомендуется использовать значение _ * 102 400 МБ * *. 

#### <a name="2-determining-maximum-amount-of-memory-per-yarn-container-request"></a>**2. Определение максимального объема памяти для запроса контейнера YARN**  
Конфигурация: **_Yarn. Scheduler. Maximum-Allocation-МБ_* _

Это значение указывает максимальное выделение памяти в МБ для каждого запроса контейнера в Resource Manager. Запросы на память выше указанного значения не будут выполнены. Диспетчер ресурсов может предоставлять память контейнерам с приращениями _yarn. Scheduler. Minimum-Memory-МБ * и не может превышать размер, указанный в *Yarn. Scheduler. Maximum-Allocation-МБ*. Указанное значение не должно превышать общую память для всех контейнеров на узле, заданную ключом *yarn.nodemanager.resource.memory-mb*.    
Для рабочих узлов D14 v2 рекомендуется значение **102400 МБ**

#### <a name="3-determining-maximum-amount-of-vcores-per-yarn-container-request"></a>**3. Определение максимального количества виртуальных ядер для запроса контейнера YARN**  
Конфигурация: **_Yarn. Scheduler. Maximum-Allocation-виртуальных ядер_* _  

Это значение конфигурации указывает максимальное количество ядер виртуального ЦП для каждого запроса контейнера в Resource Manager. Запрос большего количества виртуальных ядер, чем требуется, не выполняется. Это является глобальным свойством планировщика YARN. Для контейнера управляющей программы LLAP это значение можно задать равным 75 % от общего числа доступных виртуальных ядер. Оставшиеся 25 % должны быть зарезервированы для диспетчера узлов, диспетчера данных и других служб, работающих на рабочих узлах.  
Всего на виртуальных машинах D14 v2 существует 16 виртуальных ядер, и 75 % из этих 16 можно использовать для контейнера управляющей программы LLAP.  
Для D14 v2 рекомендуется использовать значение _ * 12 * *.  

#### <a name="4-number-of-concurrent-queries"></a>**4. Число параллельных запросов**  
Конфигурация: **_Hive. server2. Tez. Sessions. per. Default. Queue_* _

Это значение конфигурации определяет количество сеансов Tez, которые должны быть запущены параллельно. Такие сеансы Tez будут запущены для каждой очереди, заданной параметром "hive.server2.tez.default.queues". Данное значение соответствует числу мастер-приложений Tez (координаторов запросов). Рекомендуется, чтобы это значение совпадало с количеством рабочих узлов. Число мастер-приложений Tez может быть больше, чем число узлов управляющей программы LLAP. Основная задача мастер-приложений Tez заключается в координации выполнения запроса и назначении фрагментов плана запроса соответствующим управляющим программам LLAP для выполнения. Для достижения более высокой пропускной способности этот параметр должен быть кратен числу узлов управляющей программы LLAP.  

Кластер HDInsight по умолчанию имеет четыре управляющие программы LLAP, выполняющиеся на четырех рабочих узлах, поэтому рекомендуемое значение — _ * 4 * *.  

**Ползунок пользовательского интерфейса Ambari для переменной конфигурации Hive `hive.server2.tez.sessions.per.default.queue` :**

![' LLAP максимальное число одновременных запросов '](./media/hive-llap-sizing-guide/LLAP_sizing_guide_max_concurrent_queries.png "LLAP максимальное число одновременных запросов")

#### <a name="5-tez-container-and-tez-application-master-size"></a>**5. Размер контейнера Tez и мастер-приложения Tez**    
Конфигурация: **_Tez. am. resource. Memory. МБ, Hive. Tez. Container. size_* _  

_tez. am. resource. Memory. МБ * — определяет основной размер приложения Tez.  
Рекомендуется использовать значение **4096 МБ**.
   
*hive.tez.container.size* определяет объем памяти, выделяемый контейнеру Tez. Это значение должно быть задано в диапазоне от минимального (*yarn.scheduler.minimum-allocation-mb*) до максимального (*yarn.scheduler.maximum-allocation-mb*) размера контейнера YARN. Исполнители управляющей программы LLAP используют это значение для ограничения использования памяти каждым исполнителем.  
Рекомендуется использовать значение **4096 МБ**.  

#### <a name="6-llap-queue-capacity-allocation"></a>**6. Выделение емкости для очереди LLAP**   
Конфигурация: **_Yarn. Scheduler. Capacity. root. llap. Capacity_* _  

Это значение указывает процент емкости, выделенной для очереди LLAP. Выделенная емкость может отличаться для разных рабочих нагрузок в зависимости от настройки очередей YARN. Если рабочая нагрузка — это только операции чтения, задайте для нее значение 90 % емкости. Однако если Рабочая нагрузка объединяет операции обновления/удаления или слияния с помощью управляемых таблиц, рекомендуется предоставить 85% емкости для очереди llap. Оставшаяся 15% емкости может использоваться другими задачами, такими как сжатие и т. д., чтобы выделить контейнеры из очереди по умолчанию. Таким образом ресурсы YARN не будут расходоваться на задачи в очереди по умолчанию.    

Для рабочих узлов D14v2 рекомендуемым значением для очереди llap является _ * 85 * *.     
(Для рабочих нагрузок только чтения, его при необходимости можно увеличить до 90.)  

#### <a name="7-llap-daemon-container-size"></a>**7. Размер контейнера управляющей программы LLAP**    
Конфигурация: **_Hive. llap. Демон. Yarn. Container. МБ_* _  
   
Управляющая программа LLAP запускается как контейнер YARN на каждом рабочем узле. Общий объем памяти для контейнера управляющей программы LLAP зависит от следующих факторов:    
_ Конфигурации размера контейнера YARN (YARN. Scheduler. Minimum-Allocation-МБ, YARN. Scheduler. Maximum-Allocation-МБ, YARN. NodeManager. resource. Memory-МБ)
*  числа мастер-приложений Tez на узле;
*  общего объема памяти, настроенного для всех контейнеров на узле, и емкости очереди LLAP.  

Память, необходимая для мастер-приложений Tez (Tez AM), может быть рассчитана следующим образом.  
Tez AM выступает в качестве координатора запросов, и количество Tez AMs должно быть настроено на основе нескольких параллельных запросов для обслуживания. Теоретически мы можем рассмотреть один Tez на каждый рабочий узел. Однако возможно, что на рабочем узле можно увидеть более одного Tez. Для целей вычисления предполагается равномерное распределение Tez AMs по всем узлам управляющей программы LLAP и рабочим узлам.
Для каждого мастер-приложения Tez рекомендуется выделить 4 ГБ памяти.  

Число Tez AMS = value, заданное параметром Hive config ***Hive. server2. Tez. Sessions. per. Default. Queue** _.  
Число узлов управляющей программы LLAP, заданных переменной пер _*_num_llap_nodes_for_llap_daemons_*_ в пользовательском интерфейсе Ambari.  
Tez — размер контейнера = значение, заданное в Tez config _*_Tez. am. resource. Memory. МБ_*_.  

Tez память на узел = _ *(** ceil **(** число Tez AMs **/** количество узлов управляющей программы LLAP **)** **x** Tez AM-размер контейнера **)**  
Для D14 v2 конфигурация по умолчанию имеет четыре мастер-приложения Tez и четыре узла управляющей программы LLAP.  
Память мастер-приложения на узел = (округление до большего (4/4) x 4 ГБ) = 4 ГБ

Общий объем доступной памяти для очереди LLAP на рабочий узел можно вычислить следующим образом:  
Это значение зависит от общего объема памяти, доступного для всех контейнеров YARN на узле (*YARN. NodeManager. resource. Memory-МБ*), и доли емкости, настроенной для очереди llap (*YARN. Scheduler. Capacity. root. llap. Capacity*).  
Общий объем памяти для очереди LLAP на рабочем узле = Общий объем доступной памяти для всех контейнеров YARN на узле x Процент емкости для очереди LLAP.  
Для D14 v2 это значение равно (100 ГБ x 0,85) = 85 ГБ.

Размер контейнера управляющей программы LLAP вычисляется следующим образом.

**Размер контейнера управляющей программы LLAP = (общий объем памяти для очереди LLAP в workernode) — (в Tez-память на узел) — (размер главного контейнера службы)**  
В кластере, порожденном на одном из рабочих узлов, существует только один главный хозяин службы (главный узел приложения для службы LLAP). Для целей вычисления мы рассмотрим одну главную службу на каждый рабочий узел.  
Для рабочего узла D14 v2 HDI 4,0 — рекомендуемое значение (85 ГБ-4 ГБ-1 ГБ)) = **80 ГБ**   
(Для HDI 3,6 рекомендуемое значение — **79 ГБ** , так как необходимо зарезервировать дополнительные ~ 2 ГБ для ползунка.)  

#### <a name="8-determining-number-of-executors-per-llap-daemon"></a>**8. Определение числа исполнителей на одну управляющую программу LLAP**  
Конфигурация: **_hive.llap.daemon.num.exeкуторс_* _, _*_Hive. llap. IO. ThreadPool. size_*_

_*_hive.llap.daemon.num.exeкуторс_*_:   
Этот ключ конфигурации позволяет управлять количеством исполнителей, которые могут выполнять задачи параллельно, для каждой управляющей программы LLAP. Это значение зависит от числа виртуальных ядер, объема памяти, используемого исполнителем, и общего объема памяти, доступного для контейнера управляющей программы LLAP.    Количество исполнителей может быть переподписано на 120% доступных виртуальных ядер на каждый рабочий узел. Однако его следует скорректировать, если он не соответствует требованиям к памяти в зависимости от объема памяти, необходимой для каждого исполнителя и размера контейнера управляющей программы LLAP.

Каждый исполнитель эквивалентен контейнеру Tez и может потреблять 4 ГБ памяти (размер контейнера Tez). Все исполнители в управляющей программе LLAP совместно используют одну и ту же память кучи. Предполагая, что не все исполнители выполняют операции с интенсивным использованием памяти одновременно, можно рассмотреть 75% размера контейнера Tez (4 ГБ) для каждого исполнителя. Таким образом можно увеличить количество исполнителей, предоставив каждому исполнителю меньше памяти (например, 3 ГБ) для повышения параллелизма. Однако рекомендуется настроить этот параметр для целевой рабочей нагрузки.

На виртуальных машинах D14 v2 имеется 16 виртуальных ядер.
Для D14 v2 рекомендуемым значением для числа исполнителей является (16 виртуальных ядер x 120%) ~ = _ *19** на каждом рабочем узле, где число 3 ГБ на исполнителя.

**_Hive. llap. IO. ThreadPool. size_*_: это значение указывает размер пула потоков для исполнителей. Так как исполнители зафиксированы в соответствии с указаниями, они будут совпадать с количеством исполнителей на управляющую программу LLAP. Для D14 v2 рекомендуется использовать значение _* 19**.

#### <a name="9-determining-llap-daemon-cache-size"></a>**9. Определение размера кэша управляющей программы LLAP**  
Конфигурация: **_куст. llap. IO. память. size_* _

Память контейнера управляющей программы LLAP состоит из следующих компонентов. _ Головное пространство
*  память кучи, используемая исполнителями (Xmx);
*  кэш в памяти на каждую управляющую программу (емкость вне кучи; неприменимо, если включен кэш SSD);
*  пространство для метаданных кэша в памяти (применяется, только если включен кэш SSD).

**Размер запаса**. Это значение определяет часть памяти вне кучи, используемой для служебных данных на виртуальной машине Java (метапространства, стека потоков, структуры данных gc и т. д.). Как правило, на эти служебные данные приходится около 6 % от размера кучи (Xmx). Для большей уверенности это значение можно принять равным 6 % от общего объема памяти управляющей программы LLAP.  
Для D14 v2 рекомендуемым значением является ceil (80 ГБ x 0,06) ~ = **4 ГБ**.  

**Размер кучи (Xmx)** . Это объем памяти кучи, доступной для всех исполнителей.
Общий размер кучи = число исполнителей x 3 ГБ  
Для D14 v2 это значение равно 19 x 3 ГБ = **57 ГБ** .  

`Ambari environment variable for LLAP heap size:`

!["Размер кучи LLAP"](./media/hive-llap-sizing-guide/LLAP_sizing_guide_llap_heap_size.png "Размер кучи LLAP")

Если кэш SSD отключен, кэш в памяти будет иметь объем памяти, оставшийся после извлечения размера контейнера управляющей программы LLAP и размера кучи.

При включенном кэше SSD размер кэша вычисляется иначе.
Кэширование SSD включено, если значение *hive.llap.io.allocator.mmap* равно true.
Если кэш SSD включен, часть памяти будет использоваться для хранения его метаданных. Метаданные хранятся в памяти и должны быть в размере ~ 8% от размера кэша SSD.   
Размер метаданных кэша SSD в памяти = размер контейнера управляющей программы LLAP — (головное комната + размер кучи)  
Для D14 v2 с HDI 4,0, размер метаданных кэша SSD в памяти = 80 ГБ — (4 ГБ + 57 ГБ) = **19 ГБ**  
Для D14 v2 с HDI 3,6, размер метаданных кэша SSD в памяти = 79 ГБ — (4 ГБ + 57 ГБ) = **18 ГБ**

Исходя из размера доступной памяти для хранения метаданных кэша SSD, можно вычислить поддерживаемый размер кэша SSD.  
Размер метаданных в памяти для кэша SSD = LLAP размер контейнера управляющей программы-(головное комната + размер кучи) = 19 ГБ     
Размер кэша SSD = размер метаданных в памяти для кэша SSD (19 ГБ)/0,08 (8 процентов)  

Для D14 v2 и HDI 4,0 рекомендуемый размер кэша SSD составляет 19 ГБ/0,08 ~ = **237 ГБ**  
Для D14 v2 и HDI 3,6 Рекомендуемый размер кэша SSD составляет 18 ГБ/0,08 ~ = **225 ГБ**

#### <a name="10-adjusting-map-join-memory"></a>**10. Настройка памяти для соединения с сопоставлением**   
Конфигурация: **_Hive. авто. Convert. Join. noconditionaltask. size_* _

Чтобы этот параметр вступил в силу, убедитесь, что у вас есть _hive. Auto. Convert. Join. noconditionaltask *.
Эта конфигурация определяет пороговое значение для выбора Мапжоин с помощью оптимизатора Hive, который считает превышение лимита подписки памяти от других исполнителей, чтобы освободить пространство для хэш-таблиц в памяти, чтобы разрешить дополнительные преобразования соединений карт. Учитывая 3 ГБ на каждый исполнитель, этот размер может быть превышен до 3 ГБ, но для буферов сортировки, буферов и т. д. в других операциях можно также использовать некоторую память кучи.   
Таким образом, для D14 v2 с объемом памяти в 3 ГБ на исполнителя рекомендуется установить это значение равным **2048 МБ**.  

(Примечание. для этого значения могут потребоваться корректировки, подходящие для вашей рабочей нагрузки. При слишком низком значении функцию автопреобразования использовать невозможно. И установка слишком высокого значения может привести к возникновению исключений нехватки памяти или приостановки GC, что может привести к неблагоприятной производительности.)  

#### <a name="11-number-of-llap-daemons"></a>**11. количество управляющих программ LLAP**
Переменные среды Ambari: **_num_llap_nodes, num_llap_nodes_for_llap_daemons_* _  

_ *num_llap_nodes** — указывает количество узлов, используемых службой Hive llap, включая узлы под управлением управляющей программы Llap, главного сервера Llap и главного приложения Tez (Tez AM).  

![' Число узлов для службы LLAP '](./media/hive-llap-sizing-guide/LLAP_sizing_guide_num_llap_nodes.png "Число узлов для службы LLAP")  

**num_llap_nodes_for_llap_daemons** указанное количество узлов, используемое только для управляющих программ llap. Размеры контейнера управляющей программы LLAP задаются равными максимальному размеру узла, поэтому в каждом узле будет одна управляющая программа LLAP.

![' Число узлов для управляющих LLAP '](./media/hive-llap-sizing-guide/LLAP_sizing_guide_num_llap_nodes_for_llap_daemons.png "Число узлов для управляющих программ LLAP")

Рекомендуется, чтобы оба значения совпадали с количеством рабочих узлов в кластере интерактивных запросов.

### <a name="considerations-for-workload-management"></a>**Рекомендации по управлению рабочей нагрузкой**  
Если вы хотите включить управление рабочей нагрузкой для LLAP, убедитесь, что зарезервировано достаточное количество ресурсов для управления рабочей нагрузкой, как ожидалось. Управление рабочей нагрузкой требует настройки настраиваемой очереди YARN, которая является дополнением к `llap` очереди. Убедитесь, что общая емкость ресурса кластера делится между очередью управления llap очередью и рабочей нагрузкой в соответствии с требованиями к рабочей нагрузке.
При активации плана ресурсов Управление рабочей нагрузкой порождает Tez главные приложения (Tez AMs).
Обратите внимание на следующее:

* Tez AMs порождено путем активации использования ресурсов планом ресурсов из очереди управления рабочей нагрузкой, как указано в `hive.server2.tez.interactive.queue` .  
* Число Tez AMs будет зависеть от значения, `QUERY_PARALLELISM` указанного в плане ресурсов.  
* Когда Управление рабочей нагрузкой активно, TEZ AMs в очереди llap не будет использоваться. Для координации запросов используются только Tez AMs из очереди управления рабочей нагрузкой. Tez AMs в `llap` очереди используются при отключении управления рабочей нагрузкой.
 
Например: общая емкость кластера = 100 ГБ памяти, разделенная между LLAP, управлением рабочей нагрузкой и очередями по умолчанию следующим образом:
 - емкость очереди llap = 70 ГБ
 - Емкость очереди управления рабочей нагрузкой = 20 ГБ
 - Емкость очереди по умолчанию = 10 ГБ

При наличии 20 ГБ в очереди управления рабочей нагрузкой план ресурсов может указать `QUERY_PARALLELISM` значение 5, что означает, что Управление рабочей нагрузкой может запускать пять Tez AMs с размером контейнера 4 ГБ каждый. Если `QUERY_PARALLELISM` значение превышает емкость, вы можете увидеть некоторые Tez AMs в состоянии зависания `ACCEPTED` . Hiveserver2 Interactive не может отправить фрагменты запросов в AMs Tez, которые не находятся в `RUNNING` состоянии.


#### <a name="next-steps"></a>**Следующие шаги**
Если проблему не удалось устранить путем настройки этих значений, воспользуйтесь следующими ресурсами.

* Получите ответы специалистов Azure на [сайте поддержки сообщества пользователей Azure](https://azure.microsoft.com/support/community/).

* Подключитесь к [@AzureSupport](https://twitter.com/azuresupport) — официальной учетной записи Microsoft Azure. Она помогает оптимизировать работу пользователей благодаря возможности доступа к ресурсам сообщества Azure (ответы на вопросы, поддержка и консультации специалистов).

* Если вам нужна дополнительная помощь, отправьте запрос в службу поддержки на [портале Azure](https://portal.azure.com/?#blade/Microsoft_Azure_Support/HelpAndSupportBlade/). Выберите **Поддержка** в строке меню или откройте центр **Справка и поддержка**. Дополнительные сведения см. в статье [Создание запроса на поддержку Azure](../../azure-portal/supportability/how-to-create-azure-support-request.md). Доступ к средствам управления подписками и поддержка выставления счетов уже включены в вашу подписку Microsoft Azure, а техническая поддержка предоставляется в рамках одного из [планов Службы поддержки Azure](https://azure.microsoft.com/support/plans/).  

* ##### <a name="other-references"></a>**Прочие ссылки**
  * [Настройка других свойств LLAP](https://docs.cloudera.com/HDPDocuments/HDP3/HDP-3.1.5/performance-tuning/content/hive_setup_llap.html)  
  * [Настройка размера кучи сервера Hive](https://docs.cloudera.com/HDPDocuments/HDP3/HDP-3.1.5/performance-tuning/content/hive_hiveserver_heap_sizing.html)  
  * [Определение размера памяти для соединения с сопоставлением для LLAP](https://community.cloudera.com/t5/Community-Articles/Map-Join-Memory-Sizing-For-LLAP/ta-p/247462)  
  * [Свойства подсистемы выполнения Tez](https://docs.cloudera.com/HDPDocuments/HDP3/HDP-3.1.5/performance-tuning/content/hive_tez_engine_properties.html)  
  * [Подробное описание возможностей Hive LLAP](https://community.cloudera.com/t5/Community-Articles/Hive-LLAP-deep-dive/ta-p/248893)