---
title: Настройка параметров Spark для Azure HDInsight
description: Просмотр и Настройка параметров Apache Spark для кластера Azure HDInsight
author: hrasheed-msft
ms.author: hrasheed
ms.reviewer: jasonh
ms.service: hdinsight
ms.topic: conceptual
ms.custom: hdinsightactive
ms.date: 04/24/2020
ms.openlocfilehash: cd16d898408bff46cee13b4df63cd3386d0581b1
ms.sourcegitcommit: 1ed0230c48656d0e5c72a502bfb4f53b8a774ef1
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 04/24/2020
ms.locfileid: "82137847"
---
# <a name="configure-apache-spark-settings"></a>Настройка параметров Apache Spark

Кластер HDInsight Spark включает в себя установку библиотеки Apache Spark.  Каждый кластер HDInsight содержит параметры конфигурации по умолчанию для всех установленных служб, в том числе для Spark.  Ключевым аспектом управления кластером HDInsight Apache Hadoop является мониторинг рабочей нагрузки, включая задания Spark. Для лучшего выполнения заданий Spark при определении логической конфигурации кластера следует учитывать конфигурацию физического кластера.

По умолчанию кластер HDInsight Apache Spark содержит следующие узлы: три узла Apache ZooKeeper, два головных узла и один или несколько рабочих узлов:

![Архитектура Spark HDInsight](./media/apache-spark-settings/spark-hdinsight-arch.png)

Количество виртуальных машин и размеры виртуальных машин для узлов в кластере HDInsight могут повлиять на конфигурацию Spark. Нестандартные значения конфигурации HDInsight часто требуют нестандартных значений конфигурации Spark. При создании кластера HDInsight Spark вы можете отобразить Предлагаемые размеры виртуальных машин для каждого из компонентов. Сейчас [размерами виртуальных машин Azure с Linux, оптимизированными для памяти,](../../virtual-machines/linux/sizes-memory.md) являются экземпляры D12 версии 2 или выше.

## <a name="apache-spark-versions"></a>Версии Apache Spark

Используйте наиболее подходящую версию Spark для кластера.  Служба HDInsight включает в себя несколько версий Spark и HDInsight.  Каждая версия Spark содержит набор стандартных параметров кластера.  

Ниже приведены разные версии Spark. Вы можете выбрать одну из них для создания кластера. Для просмотра полного списка [компонентов и версий HDInsight](https://docs.microsoft.com/azure/hdinsight/hdinsight-component-versioning).

> [!NOTE]  
> Версия по умолчанию Apache Spark в службе HDInsight может измениться без предварительного уведомления. Если используется зависимость версии, корпорация Майкрософт рекомендует указать конкретную версию при создании кластеров с использованием пакета SDK для .NET, Azure PowerShell и классического Azure CLI.

В Apache Spark системная конфигурация находится в трех расположениях.

* Свойства Spark управляют большинством параметров приложения. Их можно установить с помощью объекта `SparkConf` или свойств системы Java.
* Переменные среды можно использовать для установки параметров (например, IP-адреса) отдельного компьютера на каждом узле с помощью скрипта `conf/spark-env.sh`.
* Ведение журнала можно настроить с помощью `log4j.properties`.

Кластер Spark любой версии содержит параметры конфигурации по умолчанию.  Вы можете изменить их, используя настраиваемый файл конфигурации Spark.  Ниже приведен пример такого файла.

```
spark.hadoop.io.compression.codecs org.apache.hadoop.io.compress.GzipCodec
spark.hadoop.mapreduce.input.fileinputformat.split.minsize 1099511627776
spark.hadoop.parquet.block.size 1099511627776
spark.sql.files.maxPartitionBytes 1099511627776
spark.sql.files.openCostInBytes 1099511627776
```

В примере выше переопределяется несколько значений по умолчанию для пяти параметров конфигурации Spark.  Эти значения являются кодеком сжатия, Apache Hadoop минимальным размером разбиения MapReduce и размерами блоков Parquet. Кроме того, раздел «меру SQL» и размер открытого файла имеют значения по умолчанию.  Эти изменения конфигурации выбираются потому, что связанные данные и задания (в этом примере геномных данные) имеют определенные характеристики. Эти характеристики будут лучше использовать эти настраиваемые параметры конфигурации.

---

## <a name="view-cluster-configuration-settings"></a>Просмотр параметров конфигурации кластера

Проверьте текущие параметры конфигурации кластера HDInsight, прежде чем выполнять оптимизацию производительности в кластере. Запустите панель мониторинга HDInsight на портале Azure, щелкнув ссылку **Панель мониторинга** в области кластера Spark. Войдите с помощью имени пользователя и пароля администратора кластера.

Откроется веб-интерфейс Apache Ambari с информационной панелью метрик использования ресурсов кластера ключей.  На панели мониторинга Ambari отображается конфигурация Apache Spark и другие установленные службы. Панель мониторинга содержит вкладку **Журнал конфигурации** , в которой можно просмотреть сведения об установленных службах, включая Spark.

Чтобы просмотреть значения конфигурации для Apache Spark, выберите **Config History** (История конфигураций), а затем — **Spark2**.  Перейдите на вкладку **Configs** (Конфигурации), а затем в списке служб щелкните ссылку `Spark` (или `Spark2` в зависимости от вашей версии).  Отобразится список значений конфигурации для вашего кластера:

![Конфигурации Spark](./media/apache-spark-settings/spark-configurations.png)

Чтобы просмотреть и изменить отдельные значения конфигурации Spark, выберите любую ссылку с "Spark" в заголовке.  Конфигурации для Spark содержат как пользовательские, так и расширенные значения конфигурации в следующих категориях:

* Custom Spark2-defaults;
* Custom Spark2-metrics-properties;
* Advanced Spark2-defaults;
* Advanced Spark2-env;
* Advanced spark2-hive-site-override;

При создании набора значений конфигурации, не заданных по умолчанию, отображается журнал обновлений.  В истории конфигурации можно увидеть, какая нестандартная конфигурация имеет оптимальную производительность.

> [!NOTE]  
> Чтобы увидеть (но не изменять) общие параметры конфигурации кластера Spark, выберите вкладку **Environment** (Среда) в интерфейсе верхнего уровня **Spark Job UI** (Пользовательский интерфейс задания Spark).

## <a name="configuring-spark-executors"></a>Настройка исполнителей Spark

На следующей схеме показаны ключевые объекты Spark. К ним относятся программа драйвера и связанный с ней контекст Spark, а также диспетчер кластера и его рабочие узлы *n*.  Каждый рабочий узел включает в себя исполнитель, кэш и экземпляры задач *n*.

![Объекты кластера](./media/apache-spark-settings/hdi-spark-architecture.png)

Задания Spark используют рабочие ресурсы, в частности память, поэтому для исполнителей рабочего узла необходимо настроить значения конфигурации Spark.

Существует три ключевых параметра — `spark.executor.instances`, `spark.executor.cores` и `spark.executor.memory`. Их часто изменяют, чтобы настроить конфигурации Spark в соответствии с требованиями приложения. Исполнитель — это процесс, запущенный для приложения Spark. Он выполняется на рабочем узле и отвечает за выполнение задач этого приложения. Число рабочих узлов и размер рабочего узла определяет количество исполнителей и размеры исполнителя. Эти значения хранятся в `spark-defaults.conf` на головных узлах кластера.  Вы можете изменить эти значения в работающем кластере, выбрав **пользовательские параметры Spark-Default** в веб-интерфейсе Ambari.  После внесения изменений пользовательский интерфейс запрашивает **перезапуск** всех затронутых служб.

> [!NOTE]  
> Эти три параметра конфигурации можно настроить на уровне кластера (для всех приложений, работающих в кластере) или для каждого отдельного приложения.

Другим источником сведений о ресурсах, используемых исполнителями Spark, является Пользовательский интерфейс приложения Spark.  В пользовательском интерфейсе **исполнители** отображают сводные и подробные представления конфигурации и потребляемые ресурсы.  Определение необходимости изменения значений исполнителей для всего кластера или определенного набора выполнений заданий.

![Исполнители Spark](./media/apache-spark-settings/apache-spark-executors.png)

Также можно использовать REST API Ambari для программной проверки параметров конфигурации кластера HDInsight и Spark.  Дополнительные сведения см. в [справочнике по API Apache Ambari на сайте GitHub](https://github.com/apache/ambari/blob/trunk/ambari-server/docs/api/v1/index.md).

В зависимости от рабочей нагрузки Spark может выясниться, что нестандартная конфигурация Spark оптимизирует выполнение задания.  Проверяйте тест производительности с помощью примеров рабочих нагрузок для проверки любых конфигураций кластера, отличных от заданной по умолчанию.  Можно настроить такие общие параметры:

|Параметр |Описание|
|---|---|
|--Num-исполнители|Задает количество исполнителей.|
|--исполнитель-ядра|Задает количество ядер для каждого исполнителя. Мы советуем выбирать исполнителей среднего размера, так как некоторые процессы используют доступную память.|
|--исполнитель-память|Управляет объемом памяти (размером кучи) каждого исполнителя в Apache Hadoop YARN, и вам придется оставить некоторый объем памяти для выполнения дополнительной нагрузки.|

Ниже приведен пример двух рабочих узлов с разными значениями конфигурации.

![Конфигурации двух узлов](./media/apache-spark-settings/executor-configuration.png)

Ниже приведен список ключевых параметров памяти исполнителя Spark.

|Параметр |Описание|
|---|---|
|spark.executor.memory.|Определяет общий объем памяти, доступной для исполнителя.|
|Spark. Storage. Меморифрактион| (по умолчанию ~ 60 %) определяет объем памяти, доступный для хранения устойчивых распределенных наборов данных (RDD).|
|Spark. в случайном порядке. Меморифрактион| (по умолчанию ~ 20 %) определяет объем памяти, зарезервированный для перемешивания.|
|Spark. Storage. Унроллфрактион и Spark. Storage. Сафетифрактион|(Итого ~ 30% от общей памяти). Эти значения используются внутри Spark и не должны изменяться.|

YARN управляет максимальным объемом памяти, используемой контейнерами на всех узлах Spark. На следующей схеме показаны отношения между объектами конфигурации YARN и Spark для каждого узла.

![Управление памятью Spark в YARN](./media/apache-spark-settings/hdi-yarn-spark-memory.png)

## <a name="change-parameters-for-an-application-running-in-jupyter-notebook"></a>Изменение параметров для приложения, запущенного в записной книжке Jupyter

Кластеры Spark в HDInsight включают в себя несколько компонентов по умолчанию. Каждый из этих компонентов содержит значения конфигурации по умолчанию, которые можно переопределить при необходимости.

|Компонент |Описание|
|---|---|
|Ядро Spark|В Spark Core, Spark SQL, API потоковой передачи Spark, GraphX и Apache Spark MLlib.|
|Anaconda|Диспетчер пакетов Python.|
|Apache Livy|REST API Apache Spark, используемый для отправки удаленных заданий в кластер HDInsight Spark.|
|Записные книжки Jupyter и Apache Zeppelin|Интерактивный пользовательский интерфейс на основе браузера для взаимодействия с кластером Spark.|
|Драйвер ODBC|Подключает кластеры Spark в HDInsight к средствам бизнес-аналитики (BI), таким как Microsoft Power BI и Tableau.|

Чтобы изменить конфигурацию для приложений, запущенных в записной книжке Jupyter, можно использовать команду `%%configure`. Эти изменения конфигурации будут применены к заданиям Spark, запущенным из экземпляра записной книжки. Внесите такие изменения в начале приложения, прежде чем запускать первую ячейку кода. Измененная конфигурация будет применена к сеансу Livy, когда он будет создан.

> [!NOTE]  
> Чтобы изменить конфигурацию на более позднем этапе выполнения приложения, используйте параметр `-f` (принудительное выполнение). Но при этом будут потеряны все результаты, полученные в приложении.

В следующем коде показано, как изменить конфигурацию для приложения, работающего в записной книжке Jupyter.

```
%%configure
{"executorMemory": "3072M", "executorCores": 4, "numExecutors":10}
```

## <a name="conclusion"></a>Заключение

Отслеживайте основные параметры конфигурации, чтобы гарантировать выполнение заданий Spark предсказуемым и производительным образом. Эти параметры помогают определить лучшую конфигурацию кластера Spark для конкретных рабочих нагрузок.  Вам также потребуется отслеживать выполнение длительных и или ресурсоемких заданий Spark.  Наиболее распространенные проблемы связаны с недостатком памяти от неправильных конфигураций, таких как неправильное изменение размера исполнителей. Кроме того, длительные операции и задачи, в результате которых выполняются операции Декарт.

## <a name="next-steps"></a>Следующие шаги

* [Компоненты и версии Apache Hadoop, доступные в HDInsight](../hdinsight-component-versioning.md)
* [Управление ресурсами для кластера Apache Spark в HDInsight](apache-spark-resource-manager.md)
* [Конфигурация Apache Spark](https://spark.apache.org/docs/latest/configuration.html)
* [Выполнение Apache Spark в Apache Hadoop YARN](https://spark.apache.org/docs/latest/running-on-yarn.html)
