---
title: Apache Spark в Azure HDInsight
description: В этой статье представлены общие сведения о Spark в HDInsight и различные сценарии, в которых вы можете использовать кластер Spark в HDInsight.
services: hdinsight
author: hrasheed-msft
ms.reviewer: jasonh
ms.service: hdinsight
ms.custom: hdinsightactive,mvc
ms.topic: overview
ms.date: 01/28/2019
ms.author: hrasheed
ms.openlocfilehash: 264faaf0168d1604668e3358b1d99bc169d7e2f0
ms.sourcegitcommit: 1afd2e835dd507259cf7bb798b1b130adbb21840
ms.translationtype: HT
ms.contentlocale: ru-RU
ms.lasthandoff: 02/28/2019
ms.locfileid: "56985178"
---
# <a name="what-is-apache-spark-in-azure-hdinsight"></a>Apache Spark в Azure HDInsight

Apache Spark — это платформа параллельной обработки, которая поддерживает обработку в памяти, чтобы повысить производительность приложений для анализа больших данных. Apache Spark в Azure HDInsight — это реализация Apache Spark в облаке, предоставляемая корпорацией Майкрософт. HDInsight упрощает создание и настройку кластера Spark в Azure. Кластеры Spark в HDInsight совместимы со службой хранилища Azure и Azure Data Lake Storage. Поэтому эти кластеры можно использовать для обработки данных, хранящихся в Azure. Дополнительные сведения о компонентах и версиях см. в статье [Что представляют собой компоненты и версии Apache Hadoop, доступные в HDInsight](../hdinsight-component-versioning.md).

![Spark: единая платформа](./media/apache-spark-overview/hdinsight-spark-overview.png)

## <a name="what-is-apache-spark"></a>Что такое Apache Spark?

Spark предоставляет примитивы для кластерных вычислений в памяти. Задание Spark может загрузить данные, поместить их в кэш в памяти и запрашивать их неоднократно. Вычисления в памяти выполняются намного быстрее, чем в приложениях, использующих диски, таких как приложение Hadoop, которое предоставляет доступ к данным через распределенную файловую систему Hadoop (HDFS). Spark также интегрируется в язык программирования Scala, что дает возможность управлять распределенными наборами данных, такими как локальные коллекции. Нет необходимости структурировать обмен данными как операции сопоставления и редукции.

![Сравнительная характеристика традиционной модели MapReduce и Spark](./media/apache-spark-overview/mapreduce-vs-spark.png)

Кластеры Spark в HDInsight предлагают полностью управляемую службу Spark. Ниже приведены преимущества создания кластера Spark в HDInsight.

| Функция | ОПИСАНИЕ |
| --- | --- |
| Простота создания |Создание кластера Spark в HDInsight с помощью портала Azure, Azure PowerShell или пакета SDK для HDInsight .NET занимает всего несколько минут. См. инструкции по [началу работы с кластером Apache Spark в HDInsight](apache-spark-jupyter-spark-sql-use-portal.md). |
| Простота использования |Кластер Spark в HDInsight включает записные книжки Jupyter и Apache Zeppelin. Их можно использовать для интерактивной обработки и визуализации данных.|
| Интерфейсы API REST |Кластеры Spark в HDInsight включают [Apache Livy](https://github.com/cloudera/hue/tree/master/apps/spark/java#welcome-to-livy-the-rest-spark-server), сервер заданий Spark на основе REST API, который позволяет удаленно отправлять и отслеживать задания. См. руководство по [удаленной отправке заданий Spark в кластер Azure HDInsight с помощью Apache Spark REST API](apache-spark-livy-rest-interface.md).|
| Поддержка Azure Data Lake Storage | Кластеры Spark в HDInsight могут использовать Azure Data Lake Storage как основное или дополнительное хранилище. Дополнительные сведения о Data Lake Storage см. в [обзоре Azure Data Lake Storage](../../data-lake-store/data-lake-store-overview.md). |
| Интеграция со службами Azure |Кластер Spark в HDInsight поставляется с соединителем для Центров событий Azure. Вы можете создавать приложения потоковой передачи с помощью Центров событий (в дополнение к системе [Apache Kafka](https://kafka.apache.org/), которая уже входит в состав Spark). |
| Поддержка ML Server | Поддержка ML Server в HDInsight предоставляется в рамках типа кластера **Служб машинного обучения**. В кластере Служб машинного обучения можно настроить выполнение распределенных вычислений в среде R со скоростью, заявленной для кластера Spark. Дополнительные сведения см. в статье [Начало работы с кластером R Server в Azure HDInsight](../r-server/r-server-get-started.md). |
| Интеграция со сторонними IDE | HDInsight предоставляет несколько подключаемых модулей IDE, которые можно использовать для создания приложений и их отправки в кластер HDInsight Spark. См. дополнительные сведения об [использовании Azure Toolkit for IntelliJ (IDEA)](apache-spark-intellij-tool-plugin.md), [использовании HDInsight для VSCode](../hdinsight-for-vscode.md) и [использовании Azure Toolkit for Eclipse](apache-spark-eclipse-tool-plugin.md).|
| Параллельные запросы |Кластеры Spark в HDInsight поддерживают параллельные запросы. Благодаря этому несколько запросов от одного пользователя или несколько запросов от разных пользователей и из различных приложений могут использовать одни и те же ресурсы кластера. |
| Кэширование на накопители SSD |Можно выбрать кэширование данных в памяти или на накопители SSD, подключенные к узлам кластера. Кэширование в память обеспечивает наилучшую производительность запросов, однако может оказаться ресурсоемким. Кэширование на накопители SSD предоставляет возможность повысить производительность запросов без необходимости создания кластера такого размера, который необходим для размещения всего набора данных в памяти. |
| Интеграция со средствами бизнес-аналитики |В состав кластеров Spark для HDInsight входят соединители для инструментов бизнес-аналитики, таких как [Power BI](https://www.powerbi.com/) для анализа данных. |
| Предварительно загруженные библиотеки Anaconda |Кластеры Spark в HDInsight поставляются с предустановленными библиотеками Anaconda. [Anaconda](https://docs.continuum.io/anaconda/) содержит порядка 200 библиотек для машинного обучения, анализа данных, визуализации и т. д. |
| Масштабируемость | В HDInsight можно изменить количество узлов кластера. Кроме того, кластеры Spark можно удалить без потери данных, так как все данные хранятся в службе хранилища Azure или Data Lake Storage. |
| Соглашение об уровне обслуживания |Для кластеров Spark в HDInsight предоставляется круглосуточная и ежедневная поддержка и соглашения об уровне обслуживания, гарантирующие время бесперебойной работы на уровне 99,9 %. |

Кластеры Apache Spark в HDInsight включают следующие компоненты, доступные в кластерах по умолчанию.

* [Ядро Spark](https://spark.apache.org/docs/latest/). Включает ядро Spark, Spark SQL, потоковые API-интерфейсы Spark, GraphX и MLlib.
* [Anaconda](https://docs.continuum.io/anaconda/)
* [Apache Livy](https://github.com/cloudera/hue/tree/master/apps/spark/java#welcome-to-livy-the-rest-spark-server)
* [Записная книжка Jupyter](https://jupyter.org)
* [Записная книжка Apache Zeppelin](http://zeppelin-project.org/)

Кроме того, кластеры Spark в HDInsight включают [драйвер ODBC](https://go.microsoft.com/fwlink/?LinkId=616229) для подключения к кластерам Spark в HDInsight из таких инструментов бизнес-аналитики, как Microsoft Power BI.

## <a name="spark-cluster-architecture"></a>Архитектура кластера Spark

![Архитектура HDInsight Spark](./media/apache-spark-overview/spark-architecture.png)

Чтобы разобраться с компонентами Spark, нужно понять принцип работы Spark в кластерах HDInsight.

Приложения Spark выполняются как независимые наборы процессов в кластере, координируемые объектом SparkContext в основной программе (называемой программой драйвера).

SparkContext может подключаться к нескольким типам диспетчеров кластеров, которые распределяют ресурсы между приложениями. К этим диспетчерам кластеров относятся [Apache Mesos](https://mesos.apache.org/), [Apache Hadoop YARN](https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html) и Spark. В HDInsight Spark выполняется с использованием диспетчера кластеров YARN. После подключения Spark получает исполнителей на рабочих узлах кластера. Исполнители представляют собой процессы,которые выполняют вычисления и хранят данные для приложения. Затем Spark отправляет исполнителям код приложения (определенный в JAR- или Python-файлах, переданных в SparkContext). Наконец, SparkContext отправляет исполнителям задачи для выполнения.

SparkContext выполняет основную функцию пользователя и осуществляет различные параллельные операции на рабочих узлах. Затем SparkContext собирает результаты операций. Рабочие узлы считывают данные из распределенной файловой системы Hadoop и записывают их в нее. Кроме того, рабочие узлы помещают преобразованные данные в кэш в памяти как устойчивые распределенные наборы данных (RDD).

SparkContext подключается к главному узлу Spark и отвечает за преобразование приложения в ориентированный граф (DAG) для отдельных задач, которые выполняются в рамках процесса исполнителя в рабочих узлах. Каждое приложение получает отдельные процессы исполнителя, которые остаются активными во время выполнения приложения и обрабатывают задачи в нескольких потоках.

## <a name="spark-in-hdinsight-use-cases"></a>Варианты использования Spark в HDInsight

Ниже представлены сценарии для использования кластеров Spark в HDInsight.

- Интерактивный анализ данных и бизнес-аналитика

    Apache Spark в HDInsight хранит данные в службе хранилища Azure или Azure Data Lake Storage. Бизнес-эксперты и лица, ответственные за принятие решений, могут анализировать и создавать отчеты на основе этих данных, а также создавать интерактивные отчеты из проанализированных данных с помощью средств Microsoft Power BI. Аналитики могут использовать неструктурированные или полуструктурированные данные в хранилище кластеров, определить схему для данных с помощью записных книжек, а затем создать модели данных с помощью средств Microsoft Power BI. Кластеры Spark в HDInsight также поддерживают ряд инструментов бизнес-аналитики сторонних разработчиков, таких как Tableau. Поэтому Spark является лучшим вариантом для аналитиков, бизнес-экспертов и лиц, ответственных за принятие решений.

    [Руководство. Визуализация данных Spark с помощью Power BI](apache-spark-use-bi-tools.md)
- Машинное обучение Spark

    В состав Apache Spark входит [MLlib](https://spark.apache.org/mllib/), библиотека машинного обучения, созданная на основе Spark, которую вы можете использовать из кластера Spark в HDInsight. Кластер Spark в HDInsight также включает библиотеку Anaconda, распространяемую Python и содержащую различные пакеты для машинного обучения. Все это дополнено встроенной поддержкой записных книжек Jupyter и Zeppelin — в итоге вы получаете в свое распоряжение среду для создания приложений машинного обучения.

    [Руководство Прогнозирование температуры в зданиях с помощью данных системы кондиционирования](apache-spark-ipython-notebook-machine-learning.md)  
    [Руководство Прогнозирование результата проверки продуктов](apache-spark-machine-learning-mllib-ipython.md).    
- Потоковая передача и анализ данных в режиме реального времени в Spark

    Кластеры Spark в HDInsight обладают широкой поддержкой для создания решений для аналитики в режиме реального времени. Поскольку в состав Spark уже входят соединители для приема данных из различных источников, таких как Flume, Kafka, Twitter, ZeroMQ или сокеты TCP, Spark в HDInsight позволяет реализовать первоклассную поддержку для приема данных из Центров событий Azure. Центры событий — это наиболее часто используемые службы очередей в Azure. Встроенная поддержка Центров событий делает кластеры Spark в HDInsight идеальной платформой для создания конвейеров аналитики в режиме реального времени.

## <a name="where-do-i-start"></a>С чего начать?

Дополнительные сведения об Apache Spark в HDInsight см. в следующих руководствах:

- [Краткое руководство. Создание кластера Apache Spark в HDInsight и выполнению интерактивных запросов с помощью Jupyter](./apache-spark-jupyter-spark-sql-use-portal.md)
- [Руководство Запуск заданий Apache Spark с помощью Jupyter](./apache-spark-load-data-run-query.md)
- [Руководство Анализ данных с помощью средств бизнес-аналитики](./apache-spark-use-bi-tools.md)
- [Руководство Использование машинного обучения с Apache Spark](./apache-spark-ipython-notebook-machine-learning.md)
- [Руководство Создание приложения Scala Maven с помощью IntelliJ](./apache-spark-create-standalone-application.md)

## <a name="next-steps"></a>Дальнейшие действия

В этой обзорной статье вы получили некоторые основные сведения об Apache Spark в Azure HDInsight. Из следующей статьи вы узнаете, как создать кластер HDInsight Spark и выполнить некоторые запросы Spark SQL.

- [Создание кластеров под управлением Linux в HDInsight с помощью портала Azure](./apache-spark-jupyter-spark-sql-use-portal.md)
