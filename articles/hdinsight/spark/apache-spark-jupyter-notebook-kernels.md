---
title: Ядра для записной книжки Jupyter в кластерах Spark в Azure HDInsight
description: Сведения о ядрах PySpark, PySpark3 и Spark для записной книжки Jupyter, доступной с кластерами Spark в Azure HDInsight.
keywords: записная книжка jupyter в spark, jupyter в spark
author: hrasheed-msft
ms.author: hrasheed
ms.reviewer: jasonh
ms.service: hdinsight
ms.custom: hdinsightactive,hdiseo17may2017
ms.topic: conceptual
ms.date: 05/27/2019
ms.openlocfilehash: 44089ea4b997e06cb7654fc6665a1a9a59ae2658
ms.sourcegitcommit: c22327552d62f88aeaa321189f9b9a631525027c
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 11/04/2019
ms.locfileid: "73494114"
---
# <a name="kernels-for-jupyter-notebook-on-apache-spark-clusters-in-azure-hdinsight"></a>Ядра для записной книжки Jupyter в кластерах Apache Spark в Azure HDInsight

Кластеры HDInsight Spark предоставляют ядра, которые можно использовать с записной книжкой Jupyter в [Apache Spark](https://spark.apache.org/) для тестирования приложений. Ядра — это программа, которая выполняет и интерпретирует ваш код. Вот эти ядра:

- **PySpark** — для приложений, написанных на языке Python2.
- **PySpark3** — для приложений, написанных на языке Python3.
- **Spark** — для приложений, написанных на языке Scala.

В этой статье вы узнаете, как использовать эти ядра, а также преимущества их использования.

## <a name="prerequisites"></a>Технические условия

Кластер Apache Spark в HDInsight. Инструкции см. в статье [Начало работы. Создание кластера Apache Spark в HDInsight на платформе Linux и выполнение интерактивных запросов с помощью SQL Spark](apache-spark-jupyter-spark-sql.md).

## <a name="create-a-jupyter-notebook-on-spark-hdinsight"></a>Создание записной книжки Jupyter в Spark HDInsight

1. В [портал Azure](https://portal.azure.com/)выберите свой кластер Spark.  Инструкции см. в разделе [Отображение кластеров](../hdinsight-administer-use-portal-linux.md#showClusters). Откроется представление **Обзор** .

2. В представлении " **Обзор** " в области **панели мониторинга кластера** выберите **Записная книжка Jupyter**. При появлении запроса введите учетные данные администратора для кластера.

    ![Записная книжка Jupyter на Apache Spark](./media/apache-spark-jupyter-notebook-kernels/hdinsight-spark-open-jupyter-interactive-spark-sql-query.png "Записная книжка Jupyter в Spark")
  
   > [!NOTE]  
   > Вы также можете получить доступ к записной книжке Jupyter в кластере Spark, открыв следующий URL-адрес в браузере. Замените **CLUSTERNAME** именем кластера:
   >
   > `https://CLUSTERNAME.azurehdinsight.net/jupyter`

3. Выберите **создать**, а затем выберите **Pyspark**, **PySpark3**или **Spark** , чтобы создать записную книжку. Для приложений Scala используйте ядро Spark, для приложений Python2 — ядро PySpark, а для приложений Python3 — ядро PySpark3.

    ![Ядра для записной книжки Jupyter в Spark](./media/apache-spark-jupyter-notebook-kernels/kernel-jupyter-notebook-on-spark.png "Ядра для записной книжки Jupyter в Spark")

4. Объект Notebook должен открыться с помощью выбранного ядра.

## <a name="benefits-of-using-the-kernels"></a>Преимущества использования ядер

Ниже приведены некоторые преимущества использования новых ядер для записной книжки Jupyter в кластерах Spark HDInsight.

- **Предустановленные контексты**. При использовании **PySpark**, **PySpark3**или ядер **Spark** не нужно явно задавать контексты Spark и Hive перед началом работы с приложениями. Они доступны по умолчанию. а именно:

  - **sc** для контекста Spark;
  - **sqlContext** для контекста Hive.

    Это значит, что для настройки этих контекстов вам не придется выполнять операторы следующего вида:

         sc = SparkContext('yarn-client')
         sqlContext = HiveContext(sc)

    Вместо этого вы сможете сразу использовать в своем приложении предустановленные контексты.

- **Волшебные команды.** Ядро PySpark предоставляет несколько "магических команд". Это специальные команды, которые можно вызывать с помощью `%%` (например, `%%MAGIC` `<args>`). Волшебная команда должна быть первым словом в ячейке кода и может состоять из нескольких строк содержимого. Волшебное слово должно быть первым словом в ячейке. Любые другие слова перед магической командой, даже комментарии, приведут к ошибке.     Дополнительные сведения о волшебных командах см. [здесь](https://ipython.readthedocs.org/en/stable/interactive/magics.html).

    В следующей таблице перечислены различные магические команды, доступные для ядер.

   | Волшебная команда | Пример | Description (Описание) |
   | --- | --- | --- |
   | help |`%%help` |Формирует таблицу из всех доступных волшебных слов с примерами и описанием. |
   | info |`%%info` |Выводит сведения о сеансе для текущей конечной точки Livy. |
   | Настройка |`%%configure -f`<br>`{"executorMemory": "1000M"`,<br>`"executorCores": 4`} |Настраивает параметры для создания сеанса. Флаг force (-f) является обязательным, если сеанс уже был создан, иначе сеанс будет удален и создан заново. Список допустимых параметров приведен в разделе, посвященном [тексту запроса сеансов POST Livy](https://github.com/cloudera/livy#request-body) . Параметры должны передаваться в виде строки JSON, следующей после волшебной команды, как показано в столбце примера. |
   | sql |`%%sql -o <variable name>`<br> `SHOW TABLES` |Выполняет запрос Hive к sqlContext. Если передан параметр `-o` , результат запроса сохраняется в контексте Python %%local в качестве таблицы данных [Pandas](https://pandas.pydata.org/) . |
   | local |`%%local`<br>`a=1` |Весь код в последующих строках выполняется локально. Код должен быть допустимым python2 кодом даже независимо от используемого ядра. Таким образом, даже если вы выбрали ядра **PySpark3** или **Spark** при создании записной книжки, при использовании волшебной `%%local` в ячейке эта ячейка должна содержать только допустимый код python2. |
   | журналы |`%%logs` |Выводит журналы для текущего сеанса Livy. |
   | удалить |`%%delete -f -s <session number>` |Удаляет указанный сеанс для текущей конечной точки Livy. Нельзя удалить сеанс, инициированный для самого ядра. |
   | cleanup |`%%cleanup -f` |Удаляет все сеансы для текущей конечной точки Livy, включая сеанс этой записной книжки. Флаг -f является обязательным. |

   > [!NOTE]  
   > Помимо магических команд, добавленных ядром PySpark, можно также использовать [встроенные магические команды](https://ipython.org/ipython-doc/3/interactive/magics.html#cell-magics) IPython, в том числе `%%sh`. Можно использовать магическую команду `%%sh` для выполнения сценариев и блоков кода на головном узле кластера.

- **Автоматическая визуализация**. Ядро Pyspark автоматически визуализирует выходные данные запросов Hive и SQL. Вы можете выбрать различные типы средства визуализации, включая таблицы, круговые диаграммы, графики, диаграммы с областями и линейчатые диаграммы.

## <a name="parameters-supported-with-the-sql-magic"></a>Параметры, поддерживаемые волшебной командой %%sql

Магическая команда `%%sql` поддерживает различные параметры, позволяющие управлять результатом выполнения запросов. Возможные результаты показаны в следующей таблице.

| Параметр | Пример | Description (Описание) |
| --- | --- | --- |
| -o |`-o <VARIABLE NAME>` |При использовании этого параметра результат запроса сохраняется в контексте Python %%local в качестве таблицы данных [Pandas](https://pandas.pydata.org/) . Именем переменной таблицы данных служит указанное вами имя переменной. |
| -q |`-q` |Позволяет отключить визуализации для ячейки. Если вы не хотите выполнять автовизуализацию содержимого ячейки и просто хотите записать ее как кадр данных, используйте `-q -o <VARIABLE>`. Если вы хотите отключить визуализацию, не записывая результаты (например, для выполнения запроса SQL, такого как инструкция `CREATE TABLE`), то используйте параметр `-q` без аргумента `-o`. |
| -m |`-m <METHOD>` |Параметр **METHOD** имеет значение **take** или **sample** (по умолчанию используется значение **take**). Если используется метод **take**, то ядро выбирает элементы из верхней части результирующего набора данных, который определяется параметром MAXROWS (описывается далее в этой таблице). Если используется метод **sample**, то ядро выбирает элементы из набора данных случайным образом в соответствии с параметром `-r`, описанным далее в этой таблице. |
| -r |`-r <FRACTION>` |Здесь **FRACTION** — это число с плавающей запятой от 0,0 до 1,0. Если для SQL-запроса используется метод выборки `sample`, то ядро выбирает заданную долю элементов из результирующего набора случайным образом. Например, при выполнении SQL-запроса с аргументами `-m sample -r 0.01` из результирующего набора данных случайным образом отбирается 1 % строк. |
| -n |`-n <MAXROWS>` |**MAXROWS** должно быть выражено целым числом. Число выходных строк для параметра **MAXROWS** ограничивается ядром. Если **MAXROWS** является отрицательным числом, например **-1**, то число строк в результирующем наборе не ограничено. |

**Пример**

    %%sql -q -m sample -r 0.1 -n 500 -o query2
    SELECT * FROM hivesampletable

Приведенная выше инструкция делает следующее:

- Выбирает все записи из таблицы **hivesampletable**.
- Так как мы используем-q, он отключает автовизуализацию.
- Случайным образом выбирает 10 % строк из таблицы hivesampletable и ограничивает размер результирующего набора 500 строками, так как включает параметр `-m sample -r 0.1 -n 500` .
- И наконец, сохраняет выходные данные в таблицу данных `-o query2`query2 **, так как включает параметр** .

## <a name="considerations-while-using-the-new-kernels"></a>Рекомендации по использованию новых ядер

Какое бы ядро вы ни использовали, работающие объекты Notebook потребляют ресурсы кластера.  С этими ядрами, поскольку контексты являются предустановленными, просто выход из записных книжек не приводит к закрытию контекста и, следовательно, ресурсы кластера продолжают использоваться. Рекомендуется использовать параметр **Закрыть и остановить** в меню **файл** записной книжки, когда вы закончите использовать записную книжку, которая завершает контекст и закрывает записную книжку.

## <a name="where-are-the-notebooks-stored"></a>Где хранятся записные книжки?

Если кластер HDInsight использует службу хранилища Azure в качестве учетной записи хранения по умолчанию, записные книжки Jupyter сохраняются в папке **/HdiNotebooks** в учетной записи хранения.  Доступ к объектам Notebook, текстовым файлам и папкам, создаваемым в Jupyter, можно получить через учетную запись хранения.  Например, если Jupyter используется для создания папки **myfolder** и объекта Notebook **myfolder/mynotebook.ipynb**, то доступ к этому объекту можно получить в расположении `/HdiNotebooks/myfolder/mynotebook.ipynb` в учетной записи хранения.  Верно и обратное: если вы передаете объект Notebook непосредственно в свою учетную запись хранения в `/HdiNotebooks/mynotebook1.ipynb`, то этот объект также отображается в Jupyter.  Объекты Notebook хранятся в учетной записи хранения даже после удаления кластера.

> [!NOTE]  
> Для кластеров HDInsight, использующих Azure Data Lake Storage в качестве хранилища по умолчанию, записные книжки не сохраняются в связанном хранилище.

Записные книжки сохраняются в учетной записи хранения, совместимой с [Apache Hadoop HDFS](https://hadoop.apache.org/docs/r1.2.1/hdfs_design.html). Таким образом, подключаясь к кластеру по протоколу SSH, вы можете использовать команды управления файлами, как показано в следующем фрагменте:

    hdfs dfs -ls /HdiNotebooks                            # List everything at the root directory – everything in this directory is visible to Jupyter from the home page
    hdfs dfs –copyToLocal /HdiNotebooks                   # Download the contents of the HdiNotebooks folder
    hdfs dfs –copyFromLocal example.ipynb /HdiNotebooks   # Upload a notebook example.ipynb to the root folder so it’s visible from Jupyter

Вне зависимости от того, использует кластер службу хранилища Azure или Azure Data Lake Storage в качестве учетной записи хранения по умолчанию, записные книжки также сохраняются на головном узле кластера в `/var/lib/jupyter`.

## <a name="supported-browser"></a>Поддерживаемый браузер

Записные книжки Jupyter, выполняемые в кластерах HDInsight Spark, поддерживаются только браузером Google Chrome.

## <a name="feedback"></a>Отзыв

Новые ядра находятся в стадии развития и будут улучшаться со временем. Кроме того, это может означать, что по мере развития этих ядер API могут измениться. Мы будем признательны вам за любые отзывы о работе с новыми ядрами. Ваши комментарии помогут нам оформить финальную версию этих ядер. Комментарии и отзывы можно оставить в разделе " **Отзывы** " в нижней части этой статьи.

## <a name="seealso"></a>Дополнительные материалы

- [Обзор: Apache Spark в Azure HDInsight](apache-spark-overview.md)

### <a name="scenarios"></a>Сценарии

- [Использование Apache Spark со средствами бизнес-аналитики. Выполнение интерактивного анализа данных с использованием Spark в HDInsight с помощью средств бизнес-аналитики](apache-spark-use-bi-tools.md)
- [Apache Spark и Машинное обучение. Анализ температуры в здании на основе данных системы кондиционирования с помощью Spark в HDInsight](apache-spark-ipython-notebook-machine-learning.md)
- [Apache Spark и Машинное обучение. Прогнозирование результатов проверки пищевых продуктов с помощью Spark в HDInsight](apache-spark-machine-learning-mllib-ipython.md)
- [Анализ журнала веб-сайта с использованием Apache Spark в HDInsight](apache-spark-custom-library-website-log-analysis.md)

### <a name="create-and-run-applications"></a>Создание и запуск приложений

- [Создание автономного приложения с использованием Scala](apache-spark-create-standalone-application.md)
- [Удаленный запуск заданий с помощью Apache Livy в кластере Apache Spark](apache-spark-livy-rest-interface.md)

### <a name="tools-and-extensions"></a>Средства и расширения

- [Использование подключаемого модуля средств HDInsight для IntelliJ IDEA для создания и отправки приложений Spark Scala](apache-spark-intellij-tool-plugin.md)
- [Удаленная отладка приложений Apache Spark в HDInsight через VPN с помощью Azure Toolkit for IntelliJ](apache-spark-intellij-tool-plugin-debug-jobs-remotely.md)
- [Использование записных книжек Zeppelin с кластером Apache Spark в Azure HDInsight](apache-spark-zeppelin-notebook.md)
- [Использование внешних пакетов с записными книжками Jupyter](apache-spark-jupyter-notebook-use-external-packages.md)
- [Установка записной книжки Jupyter на компьютере и ее подключение к кластеру Apache Spark в Azure HDInsight (предварительная версия)](apache-spark-jupyter-notebook-install-locally.md)

### <a name="manage-resources"></a>Управление ресурсами

- [Управление ресурсами кластера Apache Spark в Azure HDInsight](apache-spark-resource-manager.md)
- [Отслеживание и отладка заданий в кластере Apache Spark в HDInsight на платформе Linux](apache-spark-job-debugging.md)
